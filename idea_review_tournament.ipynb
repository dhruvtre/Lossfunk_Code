{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5XG0_tjB0tE6",
        "z-xezZHCtnw9",
        "YKSwzk2JuTLf",
        "Nk1aaFCUkBVj",
        "gtNzacElcoW5",
        "j20E18UruNsZ",
        "8jTF18E9pAkX",
        "0hpRskQKroBM",
        "sFEt41V5r4wV"
      ],
      "authorship_tag": "ABX9TyP9XYEfUmY/7tp0O1EiWVg1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Input Processing Module"
      ],
      "metadata": {
        "id": "5XG0_tjB0tE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import csv"
      ],
      "metadata": {
        "id": "rId7Bygs2jXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_ideas(file_path):\n",
        "    \"\"\"\n",
        "    Load idea records from a JSONL or CSV file.\n",
        "\n",
        "    Returns:\n",
        "      List[dict] each with keys:\n",
        "        - id: str (simple incremental, e.g. idea_001, idea_002, â€¦)\n",
        "        - text: str (idea text)\n",
        "        - original_data: dict (all fields from the source)\n",
        "    \"\"\"\n",
        "    ideas = []\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "    counter = 1\n",
        "\n",
        "    if ext == '.jsonl':\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for lineno, line in enumerate(f, start=1):\n",
        "                try:\n",
        "                    item = json.loads(line)\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\"Skipping malformed JSONL line {lineno}\")\n",
        "                    continue\n",
        "\n",
        "                text = item.get('text', '').strip()\n",
        "                if not text:\n",
        "                    print(f\"Skipping empty text at JSONL line {lineno}\")\n",
        "                    continue\n",
        "\n",
        "                idea_id = f\"idea_{counter:03d}\"\n",
        "                counter += 1\n",
        "                ideas.append({\n",
        "                    'id': idea_id,\n",
        "                    'text': text,\n",
        "                    'original_data': item\n",
        "                })\n",
        "\n",
        "    elif ext == '.csv':\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            for rowno, row in enumerate(reader, start=2):  # header is row 1\n",
        "                # prefer an 'idea_text' column if present\n",
        "                text = row.get('idea_text') or row.get('text') or row.get('description') or ''\n",
        "                text = text.strip()\n",
        "                if not text:\n",
        "                    print(f\"Skipping empty text at CSV row {rowno}\")\n",
        "                    continue\n",
        "\n",
        "                idea_id = f\"idea_{counter:03d}\"\n",
        "                counter += 1\n",
        "                ideas.append({\n",
        "                    'id': idea_id,\n",
        "                    'text': text,\n",
        "                    'original_data': row\n",
        "                })\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file format: {ext}\")\n",
        "\n",
        "    return ideas\n"
      ],
      "metadata": {
        "id": "Vm7N9U-uuxiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Prompt 1 - Taken from Chain of Ideas Paper\n",
        "* Common Criteria: Significance, Clarity\n",
        "\n",
        "---\n",
        "\n",
        "* Unique Criteria: Novelty /Originality in LLM Reviewer/\n",
        "Feasibility, Effectiveness\n"
      ],
      "metadata": {
        "id": "z-xezZHCtnw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_coi_prompt(idea0: str, idea1: str, topic: str) -> str:\n",
        "    \"\"\"Generates the prompt for evaluating two research ideas.\"\"\"\n",
        "    evaluation_prompt = f'''\n",
        "    You are a judge in a competition. You have to decide which idea is better.\n",
        "\n",
        "    The idea0 is: {idea0}\n",
        "    The idea1 is: {idea1}\n",
        "\n",
        "    The topic is: {topic}\n",
        "\n",
        "    Which idea do you think is better? Please write a short paragraph to explain your choice.\n",
        "\n",
        "    Here are your evaluation criteria:\n",
        "    Novelty: Are the problems or approaches new? Is this a novel combination of familiar techniques? Is it clear how this work differs from previous contributions? Is related work adequately referenced?\n",
        "    Significance: Are the idea important? Are other people (practitioners or researchers) likely to use these ideas or build on them? Does the idea address a difficult problem in a better way than previous research? Does it provide a unique theoretical or pragmatic approach?\n",
        "    Feasibility: Can the idea be realized with existing technology or methods? Are there any technical difficulties or bottlenecks? Is the idea clear and logical? Is there any obvious error or unreasonable part in the idea, and can the experiment be designed normally according to this idea.\n",
        "    Clarity: Is the paper clearly written? Is it well-organized? Does it adequately inform the reader?\n",
        "    Effectiveness: How likely the proposed idea is going to work well (e.g., better than existing baselines).\n",
        "\n",
        "    Note:\n",
        "    Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. DO NOT allow the LENGTH of the responses to influence your evaluation, choose the one that is straight-to-the-point instead of unnecessarily verbose. Be as objective as possible. (very important!!!)\n",
        "\n",
        "    If you think idea0 is better than idea1, you should output 0. If you think idea1 is better than idea0, you should output 1. If you think idea0 and idea1 are equally good, you should output 2.\n",
        "\n",
        "    Your output should be strictly in following format:\n",
        "    Your thinking process:\n",
        "\n",
        "    Your choice:\n",
        "    <novelty>{{ Your choice for novelty }}</novelty>\n",
        "    <significance>{{ Your choice for significance }}</significance>\n",
        "    <feasibility>{{ Your choice for feasibility }}</feasibility>\n",
        "    <clarity>{{ Your choice for clarity }}</clarity>\n",
        "    <effectiveness>{{ Your choice for effectiveness }}</effectiveness>\n",
        "    '''\n",
        "    return evaluation_prompt\n"
      ],
      "metadata": {
        "id": "wo2o9AjO_r48"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_coi_response(response_content: str) -> dict:\n",
        "    \"\"\"\n",
        "    Parse a Chain-of-Ideas style evaluation response.\n",
        "\n",
        "    Args:\n",
        "        response_content: The LLM's response text containing thinking process and axis scores\n",
        "\n",
        "    Returns:\n",
        "        dict with keys:\n",
        "        - 'thinking_process': str\n",
        "        - 'axis_scores': dict mapping axis names to scores (0, 1, or 2)\n",
        "        - 'overall_winner': str ('idea0', 'idea1', or 'tie')\n",
        "        - 'success': bool indicating if parsing succeeded\n",
        "    \"\"\"\n",
        "    result = {\n",
        "        'thinking_process': '',\n",
        "        'axis_scores': {},\n",
        "        'overall_winner': 'tie',  # Default to tie\n",
        "        'success': False\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Extract thinking process (everything before the first XML tag)\n",
        "        first_tag_match = re.search(r'<(novelty|significance|feasibility|clarity|effectiveness)>',\n",
        "                                    response_content)\n",
        "        if first_tag_match:\n",
        "            result['thinking_process'] = response_content[:first_tag_match.start()].strip()\n",
        "        else:\n",
        "            # If no tags found, treat entire response as thinking process\n",
        "            result['thinking_process'] = response_content.strip()\n",
        "\n",
        "        # Extract axis scores\n",
        "        axes = ['novelty', 'significance', 'feasibility', 'clarity', 'effectiveness']\n",
        "        for axis in axes:\n",
        "            pattern = f'<{axis}>\\\\s*(\\\\d)\\\\s*</{axis}>'\n",
        "            match = re.search(pattern, response_content)\n",
        "            if match:\n",
        "                score = int(match.group(1))\n",
        "                # Validate score is 0, 1, or 2\n",
        "                if score in [0, 1, 2]:\n",
        "                    result['axis_scores'][axis] = score\n",
        "                else:\n",
        "                    # Invalid score, treat as parsing failure for this axis\n",
        "                    result['axis_scores'][axis] = 2  # Default to tie\n",
        "            else:\n",
        "                # Missing axis, treat as tie\n",
        "                result['axis_scores'][axis] = 2\n",
        "\n",
        "        # Calculate overall winner using point system\n",
        "        if len(result['axis_scores']) == 5:  # All axes parsed\n",
        "            idea0_points = 0\n",
        "            idea1_points = 0\n",
        "\n",
        "            for axis, score in result['axis_scores'].items():\n",
        "                if score == 0:\n",
        "                    idea0_points += 1\n",
        "                elif score == 1:\n",
        "                    idea1_points += 1\n",
        "                else:  # score == 2 (tie)\n",
        "                    idea0_points += 0.5\n",
        "                    idea1_points += 0.5\n",
        "\n",
        "            # Determine overall winner with string labels\n",
        "            if idea0_points > idea1_points:\n",
        "                result['overall_winner'] = 'idea0'\n",
        "            elif idea1_points > idea0_points:\n",
        "                result['overall_winner'] = 'idea1'\n",
        "            else:\n",
        "                result['overall_winner'] = 'tie'\n",
        "\n",
        "            result['success'] = True\n",
        "\n",
        "            # Add point totals for transparency\n",
        "            result['point_totals'] = {\n",
        "                'idea0': idea0_points,\n",
        "                'idea1': idea1_points\n",
        "            }\n",
        "\n",
        "    except Exception as e:\n",
        "        # If any parsing error occurs, return failure\n",
        "        result['error'] = str(e)\n",
        "        result['success'] = False\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "_AtcFuR4hlXM"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Prompt 2 - From Google Co-Scientist\n",
        "* Can use custom idea attributes to run tournament, removed reviews of hypothesis and addtional note / research goal."
      ],
      "metadata": {
        "id": "YKSwzk2JuTLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_gsc_simple_prompt(idea0, idea1, idea_attributes, primary_area):\n",
        "  prompt = f'''\n",
        "  You are an expert evaluator tasked with comparing two hypotheses.\n",
        "  Evaluate the two provided hypotheses (hypothesis 1 and hypothesis 2) and determine which one is superior based on the specified evaluation criteria.\n",
        "  Provide a concise rationale for your selection, concluding with the phrase \"better idea: <1 or 2>\".\n",
        "\n",
        "  Primary Area: {primary_area}\n",
        "\n",
        "  Evaluation criteria:\n",
        "  {idea_attributes}\n",
        "\n",
        "  Hypothesis 1:\n",
        "  {idea0}\n",
        "  Hypothesis 2:\n",
        "  {idea1}\n",
        "\n",
        "  Reasoning and conclusion (end with \"better hypothesis: <1 or 2>\"):\n",
        "  '''\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "PZBVsQxyuaWC"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_gcs_response(response_content: str) -> dict:\n",
        "    \"\"\"\n",
        "    Parse a Google Co-Scientist style evaluation response.\n",
        "\n",
        "    Args:\n",
        "        response_content: The LLM's response text ending with \"better hypothesis: <1 or 2>\"\n",
        "\n",
        "    Returns:\n",
        "        dict with keys:\n",
        "        - 'reasoning': str (the rationale text)\n",
        "        - 'overall_winner': str ('idea0', 'idea1', or 'tie')\n",
        "        - 'success': bool indicating if parsing succeeded\n",
        "    \"\"\"\n",
        "    result = {\n",
        "        'reasoning': '',\n",
        "        'overall_winner': 'tie',  # Default to tie\n",
        "        'success': False\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Look for the pattern \"better hypothesis: <number>\" at the end\n",
        "        # Case-insensitive and flexible with spacing\n",
        "        pattern = r'better\\s+hypothesis:\\s*<?(\\d+)>?\\s*$'\n",
        "        match = re.search(pattern, response_content, re.IGNORECASE | re.MULTILINE)\n",
        "\n",
        "        if match:\n",
        "            winner_num = int(match.group(1))\n",
        "\n",
        "            # Extract reasoning (everything before the conclusion)\n",
        "            result['reasoning'] = response_content[:match.start()].strip()\n",
        "\n",
        "            # Map winner number to our convention\n",
        "            # Note: GCS uses 1/2, we use idea0/idea1\n",
        "            if winner_num == 1:\n",
        "                result['overall_winner'] = 'idea0'  # First idea in our pairing\n",
        "            elif winner_num == 2:\n",
        "                result['overall_winner'] = 'idea1'  # Second idea in our pairing\n",
        "            else:\n",
        "                # Invalid number, treat as tie\n",
        "                result['overall_winner'] = 'tie'\n",
        "\n",
        "            result['success'] = True\n",
        "        else:\n",
        "            # Pattern not found, treat entire response as reasoning\n",
        "            result['reasoning'] = response_content.strip()\n",
        "            result['success'] = False\n",
        "            result['error'] = \"Could not find 'better hypothesis: <1 or 2>' pattern\"\n",
        "\n",
        "    except Exception as e:\n",
        "        result['error'] = str(e)\n",
        "        result['success'] = False\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "YhMV4QJMjOa0"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Registry"
      ],
      "metadata": {
        "id": "Nk1aaFCUkBVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_REGISTRY = {\n",
        "    \"coi_5axis\": {\n",
        "        \"generator\": generate_coi_prompt,\n",
        "        \"parser\": parse_coi_response,\n",
        "        \"params\": [\"idea0_text\", \"idea1_text\", \"topic\"]\n",
        "    },\n",
        "    \"gcs_simple\": {\n",
        "        \"generator\": generate_gsc_simple_prompt,\n",
        "        \"parser\": parse_gcs_response,\n",
        "        \"params\": [\"idea0_text\", \"idea1_text\", \"idea_attributes\", \"primary_area\"]\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "GTeTlK-HkDEp"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AI Functions"
      ],
      "metadata": {
        "id": "gtNzacElcoW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting OpenRouter API Key\n",
        "from google.colab import userdata\n",
        "openrouter_api_key = userdata.get('OpenRouter_Key')\n",
        "\n",
        "import requests\n",
        "import json\n",
        "from typing import Optional\n",
        "import re"
      ],
      "metadata": {
        "id": "cWSruYawgSbk"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def send_ai_request(user_message, system_prompt=None, model=\"google/gemini-2.5-pro-preview-03-25\", file=None, file_data=None, file_name=None, temperature=1):\n",
        "    \"\"\"Send a request to the OpenRouter API and return the response\"\"\"\n",
        "    # Construction message\n",
        "    messages_array = []\n",
        "    if system_prompt:\n",
        "      system_prompt_message = {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": system_prompt\n",
        "      }\n",
        "      messages_array.append(system_prompt_message)\n",
        "\n",
        "    user_message_prompt_message = [{\n",
        "        \"type\": \"text\",\n",
        "        \"text\": user_message\n",
        "    }]\n",
        "    if file:\n",
        "        user_message_prompt_message.append({\n",
        "            \"type\": \"file\",\n",
        "            \"file\": {\n",
        "            \"filename\": file_name,\n",
        "            \"file_data\": file_data\n",
        "            }\n",
        "        }\n",
        "        )\n",
        "    user_message_prompt_message = str(user_message_prompt_message)\n",
        "\n",
        "    user_message = {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_message_prompt_message\n",
        "    }\n",
        "\n",
        "    messages_array.append(user_message)\n",
        "\n",
        "    response = requests.post(\n",
        "        url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
        "        headers={\n",
        "            \"Authorization\": f\"Bearer {openrouter_api_key}\"\n",
        "        },\n",
        "        data=json.dumps({\n",
        "            \"model\": model,\n",
        "            \"messages\": messages_array,\n",
        "            \"temperature\": temperature,\n",
        "            # \"max_tokens\": 5000,\n",
        "            \"transforms\" : [\"middle-out\"]\n",
        "        })\n",
        "    )\n",
        "    return response.json()"
      ],
      "metadata": {
        "id": "i7H_gEcAir8C"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_ai_response(response, reasoning=False):\n",
        "    \"\"\"Extract the content and usage metrics from API response\"\"\"\n",
        "    try:\n",
        "        content = response['choices'][0]['message']['content']\n",
        "        usage = response['usage']\n",
        "        if reasoning:\n",
        "            reasoning_text = response['choices'][0]['message']['reasoning']\n",
        "            return {\n",
        "              'content': content,\n",
        "              'usage': usage,\n",
        "              'reasoning': reasoning_text,\n",
        "              'success': True\n",
        "          }\n",
        "        elif reasoning is False:\n",
        "            return {\n",
        "              'content': content,\n",
        "              'usage': usage,\n",
        "              'success': True\n",
        "          }\n",
        "    except (KeyError, IndexError) as e:\n",
        "        return {\n",
        "            'content': None,\n",
        "            'usage': None,\n",
        "            'success': False,\n",
        "            'reasoning': None,\n",
        "            'error': str(e),\n",
        "            'response': response\n",
        "        }"
      ],
      "metadata": {
        "id": "bHxIFFu1iv-7"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_json_between_markers(llm_output: str) -> dict | None:\n",
        "    # Regular expression pattern to find JSON content between ```json and ```\n",
        "    json_pattern = r\"```json(.*?)```\"\n",
        "    matches = re.findall(json_pattern, llm_output, re.DOTALL)\n",
        "\n",
        "    if not matches:\n",
        "        # Fallback: Try to find any JSON-like content in the output\n",
        "        json_pattern = r\"\\{.*?\\}\"\n",
        "        matches = re.findall(json_pattern, llm_output, re.DOTALL)\n",
        "\n",
        "    for json_string in matches:\n",
        "        json_string = json_string.strip()\n",
        "        try:\n",
        "            parsed_json = json.loads(json_string)\n",
        "            return parsed_json\n",
        "        except json.JSONDecodeError:\n",
        "            # Attempt to fix common JSON issues\n",
        "            try:\n",
        "                # Remove invalid control characters\n",
        "                json_string_clean = re.sub(r\"[\\x00-\\x1F\\x7F]\", \"\", json_string)\n",
        "                parsed_json = json.loads(json_string_clean)\n",
        "                return parsed_json\n",
        "            except json.JSONDecodeError:\n",
        "                continue  # Try next match\n",
        "\n",
        "    return None  # No valid JSON found"
      ],
      "metadata": {
        "id": "JcNEpmyNt2u6"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single Match Runner"
      ],
      "metadata": {
        "id": "j20E18UruNsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_match(idea_a, idea_b, variant, model, prompt_kwargs, bidirectional=False):\n",
        "    \"\"\"\n",
        "    Execute pairwise comparison between two ideas.\n",
        "\n",
        "    Args:\n",
        "        idea_a: dict with keys 'id', 'text', 'original_data'\n",
        "        idea_b: dict with keys 'id', 'text', 'original_data'\n",
        "        variant: str, key from PROMPT_REGISTRY ('coi_5axis' or 'gcs_simple')\n",
        "        model: str, model identifier for LLM\n",
        "        prompt_kwargs: dict with variant-specific parameters (e.g., topic, idea_attributes)\n",
        "        bidirectional: bool, whether to run comparison in both directions\n",
        "\n",
        "    Returns:\n",
        "        dict with 'match_results' list, 'variant', and 'model'\n",
        "    \"\"\"\n",
        "    # Validate variant\n",
        "    if variant not in PROMPT_REGISTRY:\n",
        "        return {\n",
        "            \"match_results\": [],\n",
        "            \"variant\": variant,\n",
        "            \"model\": model,\n",
        "            \"error\": f\"Unknown variant: {variant}\"\n",
        "        }\n",
        "\n",
        "    print(f\"\\n[DEBUG] Starting match: {idea_a['id']} vs {idea_b['id']}\")\n",
        "    print(f\"[DEBUG] Variant: {variant}, Model: {model}\")\n",
        "    print(f\"[DEBUG] Prompt kwargs: {prompt_kwargs}\")\n",
        "\n",
        "    registry_entry = PROMPT_REGISTRY[variant]\n",
        "    generator = registry_entry[\"generator\"]\n",
        "    parser = registry_entry[\"parser\"]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Define the evaluation directions\n",
        "    if bidirectional:\n",
        "        directions = [\n",
        "            (\"forward\", idea_a, idea_b),\n",
        "            (\"reverse\", idea_b, idea_a)\n",
        "        ]\n",
        "    else:\n",
        "        directions = [(\"forward\", idea_a, idea_b)]\n",
        "\n",
        "\n",
        "\n",
        "    # Run evaluation(s)\n",
        "    for direction, first_idea, second_idea in directions:\n",
        "        try:\n",
        "            # Generate prompt with idea texts and variant-specific params\n",
        "            prompt = generator(first_idea['text'], second_idea['text'], **prompt_kwargs)\n",
        "\n",
        "            print(f\"\\n[DEBUG] Direction: {direction}\")\n",
        "            print(f\"[DEBUG] Generated prompt length: {len(prompt)}\")\n",
        "            print(f\"[DEBUG] First 200 chars of prompt: {prompt[:200]}...\")\n",
        "\n",
        "            # Call LLM\n",
        "            raw_response = send_ai_request(prompt, model=model, temperature=1)\n",
        "            parsed_response = parse_ai_response(raw_response)\n",
        "\n",
        "            # Parse the response\n",
        "            if parsed_response['success']:\n",
        "                print(f\"[DEBUG] LLM call success: {parsed_response['success']}\")\n",
        "                print(f\"[DEBUG] Response length: {len(parsed_response['content'])}\")\n",
        "                print(f\"[DEBUG] First 200 chars: {parsed_response['content'][:200]}...\")\n",
        "                result = parser(parsed_response['content'])\n",
        "\n",
        "                print(f\"[DEBUG] Parser success: {result.get('success', 'No success field')}\")\n",
        "                print(f\"[DEBUG] Overall winner: {result.get('overall_winner', 'No winner field')}\")\n",
        "                if not result.get('success', True):\n",
        "                  print(f\"[DEBUG] Parser error: {result.get('error', 'No error details')}\")\n",
        "\n",
        "                # Add metadata\n",
        "                result['direction'] = direction\n",
        "                result['idea0_id'] = first_idea['id']\n",
        "                result['idea1_id'] = second_idea['id']\n",
        "                result['idea0_text'] = first_idea['text']\n",
        "                result['idea1_text'] = second_idea['text']\n",
        "\n",
        "                # Add winning text for convenience\n",
        "                if result.get('overall_winner') == 'idea0':\n",
        "                    result['winner_text'] = first_idea['text']\n",
        "                    result['winner_id'] = first_idea['id']\n",
        "                elif result.get('overall_winner') == 'idea1':\n",
        "                    result['winner_text'] = second_idea['text']\n",
        "                    result['winner_id'] = second_idea['id']\n",
        "                else:  # tie\n",
        "                    result['winner_text'] = None\n",
        "                    result['winner_id'] = None\n",
        "\n",
        "                # Include LLM metadata\n",
        "                result['llm_usage'] = parsed_response.get('usage', {})\n",
        "\n",
        "            else:\n",
        "                # LLM call failed, create failure result\n",
        "                print(f\"[DEBUG] LLM Error: {parsed_response.get('error', 'Unknown error')}\")\n",
        "                result = {\n",
        "                    'direction': direction,\n",
        "                    'idea0_id': first_idea['id'],\n",
        "                    'idea1_id': second_idea['id'],\n",
        "                    'idea0_text': first_idea['text'],\n",
        "                    'idea1_text': second_idea['text'],\n",
        "                    'overall_winner': 'tie',\n",
        "                    'winner_text': None,\n",
        "                    'winner_id': None,\n",
        "                    'success': False,\n",
        "                    'error': parsed_response.get('error', 'LLM call failed')\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            # Unexpected error, create failure result\n",
        "            result = {\n",
        "                'direction': direction,\n",
        "                'idea0_id': first_idea['id'],\n",
        "                'idea1_id': second_idea['id'],\n",
        "                'idea0_text': first_idea['text'],\n",
        "                'idea1_text': second_idea['text'],\n",
        "                'overall_winner': 'tie',\n",
        "                'winner_text': None,\n",
        "                'winner_id': None,\n",
        "                'success': False,\n",
        "                'error': f\"Unexpected error: {str(e)}\"\n",
        "            }\n",
        "\n",
        "        results.append(result)\n",
        "\n",
        "    return {\n",
        "        \"match_results\": results,\n",
        "        \"variant\": variant,\n",
        "        \"model\": model\n",
        "    }"
      ],
      "metadata": {
        "id": "TZYzpkDVo-T2"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tournament Runner"
      ],
      "metadata": {
        "id": "8jTF18E9pAkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "WWbSMEscpC1c"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_tournament(ideas, variant, model, prompt_kwargs, bidirectional=False):\n",
        "    \"\"\"\n",
        "    Run a complete round-robin tournament comparing all ideas.\n",
        "\n",
        "    Parameters:\n",
        "    - ideas: list of dicts with 'id', 'text', 'original_data'\n",
        "    - variant: 'coi_5axis' or 'gcs_simple'\n",
        "    - model: LLM model string\n",
        "    - prompt_kwargs: dict with variant-specific params (topic, idea_attributes, etc.)\n",
        "    - bidirectional: whether to evaluate both A vs B and B vs A\n",
        "\n",
        "    Returns: dict with all tournament data\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize tracking structures\n",
        "    win_counts = {idea['id']: 0 for idea in ideas}\n",
        "    all_matches = []\n",
        "    total_comparisons = 0\n",
        "    failed_comparisons = 0\n",
        "\n",
        "    # Print progress header\n",
        "    print(f\"Starting tournament with {len(ideas)} ideas\")\n",
        "    total_matches = len(ideas) * (len(ideas)-1) // 2\n",
        "    print(f\"Total matches to run: {total_matches}\")\n",
        "    if bidirectional:\n",
        "        print(f\"Total LLM calls: {total_matches * 2}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Round-robin pairing\n",
        "    for i in range(len(ideas)):\n",
        "        for j in range(i+1, len(ideas)):  # Only pair each once\n",
        "            # Progress indicator\n",
        "            print(f\"Match {total_comparisons + 1}/{total_matches}: {ideas[i]['id']} vs {ideas[j]['id']}\")\n",
        "\n",
        "            # Execute the match\n",
        "            match_result = execute_match(\n",
        "                ideas[i],\n",
        "                ideas[j],\n",
        "                variant,\n",
        "                model,\n",
        "                prompt_kwargs,\n",
        "                bidirectional\n",
        "            )\n",
        "\n",
        "            # Update win counts based on results\n",
        "            for result in match_result['match_results']:\n",
        "                if result.get('success', True):  # Default to True for backward compatibility\n",
        "                    print(\"Match results include success.\")\n",
        "                    if result['overall_winner'] == 'idea0':\n",
        "                        win_counts[result['idea0_id']] += 1\n",
        "                    elif result['overall_winner'] == 'idea1':\n",
        "                        win_counts[result['idea1_id']] += 1\n",
        "                    else:  # tie\n",
        "                        win_counts[result['idea0_id']] += 0.5\n",
        "                        win_counts[result['idea1_id']] += 0.5\n",
        "                else:\n",
        "                    # Failed comparison counts as tie\n",
        "                    win_counts[result['idea0_id']] += 0.5\n",
        "                    win_counts[result['idea1_id']] += 0.5\n",
        "                    failed_comparisons += 1\n",
        "\n",
        "            # Store the complete match result\n",
        "            all_matches.append(match_result)\n",
        "            total_comparisons += 1\n",
        "\n",
        "    # Create final rankings (list of tuples)\n",
        "    rankings = sorted(win_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Add idea text to rankings for convenience\n",
        "    rankings_with_text = []\n",
        "    idea_text_map = {idea['id']: idea['text'] for idea in ideas}\n",
        "\n",
        "    for rank, (idea_id, wins) in enumerate(rankings, 1):\n",
        "        idea_text = idea_text_map[idea_id]\n",
        "        text_preview = idea_text[:100] + '...' if len(idea_text) > 100 else idea_text\n",
        "\n",
        "        rankings_with_text.append({\n",
        "            'rank': rank,\n",
        "            'idea_id': idea_id,\n",
        "            'wins': wins,\n",
        "            'win_rate': wins / (len(ideas) - 1) if bidirectional else wins / ((len(ideas) - 1) / 2),\n",
        "            'idea_text': text_preview\n",
        "        })\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Tournament complete! Failed comparisons: {failed_comparisons}\")\n",
        "\n",
        "    # Compile final output\n",
        "    return {\n",
        "        'rankings': rankings_with_text,\n",
        "        'win_counts': win_counts,\n",
        "        'matches': all_matches,\n",
        "        'metadata': {\n",
        "            'variant': variant,\n",
        "            'model': model,\n",
        "            'prompt_kwargs': prompt_kwargs,\n",
        "            'bidirectional': bidirectional,\n",
        "            'total_ideas': len(ideas),\n",
        "            'total_comparisons': total_comparisons,\n",
        "            'failed_comparisons': failed_comparisons,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "    }"
      ],
      "metadata": {
        "id": "NW63Ig-brZjy"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving Output"
      ],
      "metadata": {
        "id": "0hpRskQKroBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_tournament_results(tournament_data, output_path):\n",
        "    \"\"\"\n",
        "    Save tournament results to JSONL file.\n",
        "\n",
        "    File structure:\n",
        "    - Line 1: Tournament summary with rankings and metadata\n",
        "    - Lines 2+: Individual match results\n",
        "    \"\"\"\n",
        "\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        # First line: Tournament summary\n",
        "        summary = {\n",
        "            'type': 'tournament_summary',\n",
        "            'rankings': tournament_data['rankings'],\n",
        "            'metadata': tournament_data['metadata']\n",
        "        }\n",
        "        f.write(json.dumps(summary) + '\\n')\n",
        "\n",
        "        # Subsequent lines: Individual matches with all details\n",
        "        for match_idx, match in enumerate(tournament_data['matches']):\n",
        "            match_record = {\n",
        "                'type': 'match_result',\n",
        "                'match_index': match_idx,\n",
        "                'variant': match['variant'],\n",
        "                'model': match['model'],\n",
        "                'results': match['match_results']  # This includes all parse details\n",
        "            }\n",
        "            print(f\"[DEBUG] Match {match_idx} has {len(match['match_results'])} results\")\n",
        "            f.write(json.dumps(match_record) + '\\n')\n",
        "\n",
        "    print(f\"\\nTournament results saved to {output_path}\")\n",
        "    print(f\"Total lines written: {len(tournament_data['matches']) + 1}\")\n",
        "\n",
        "    # Print top 3 rankings\n",
        "    print(\"\\nTop 3 ideas:\")\n",
        "    for item in tournament_data['rankings'][:3]:\n",
        "        print(f\"{item['rank']}. {item['idea_id']} (wins: {item['wins']}, rate: {item['win_rate']:.2%})\")\n",
        "        print(f\"   {item['idea_text']}\")"
      ],
      "metadata": {
        "id": "_l35WAHNrpqk"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main"
      ],
      "metadata": {
        "id": "sFEt41V5r4wV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the tournament-based idea evaluation.\n",
        "    \"\"\"\n",
        "    print(\"=== Tournament-Based Idea Reviewer ===\\n\")\n",
        "\n",
        "    # Step 1: Load ideas\n",
        "    file_path = input(\"Enter path to ideas file (JSONL or CSV): \").strip()\n",
        "    try:\n",
        "        ideas = load_ideas(file_path)\n",
        "        print(f\"âœ“ Loaded {len(ideas)} ideas successfully\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"âœ— Error loading ideas: {e}\")\n",
        "        return\n",
        "\n",
        "    # Step 2: Select evaluation variant\n",
        "    print(\"Available evaluation variants:\")\n",
        "    for key in PROMPT_REGISTRY.keys():\n",
        "        print(f\"  - {key}\")\n",
        "    variant = input(\"Select variant: \").strip()\n",
        "\n",
        "    if variant not in PROMPT_REGISTRY:\n",
        "        print(f\"âœ— Invalid variant: {variant}\")\n",
        "        return\n",
        "\n",
        "    # Step 3: Collect variant-specific parameters\n",
        "    print(f\"\\nParameters needed for {variant}:\")\n",
        "    prompt_kwargs = {}\n",
        "\n",
        "    if variant == \"coi_5axis\":\n",
        "        topic = input(\"Enter topic/primary area: \").strip()\n",
        "        prompt_kwargs['topic'] = topic\n",
        "\n",
        "    elif variant == \"gcs_simple\":\n",
        "        primary_area = input(\"Enter primary area: \").strip()\n",
        "        idea_attributes = input(\"Enter evaluation criteria (e.g., 'novelty, feasibility, impact'): \").strip()\n",
        "        prompt_kwargs['primary_area'] = primary_area\n",
        "        prompt_kwargs['idea_attributes'] = idea_attributes\n",
        "\n",
        "    # Step 4: Model selection\n",
        "    print(\"\\nCommon models:\")\n",
        "    print(\"  - google/gemini-2.0-flash-exp\")\n",
        "    print(\"  - openai/gpt-4o\")\n",
        "    print(\"  - anthropic/claude-3-5-sonnet\")\n",
        "    print(\"  - openai/o3\")\n",
        "    print(\"  - anthropic/claude-3.7-sonnet\")\n",
        "    model = input(\"Enter model name: \").strip()\n",
        "\n",
        "    # Step 5: Bidirectional option\n",
        "    bidirectional_input = input(\"\\nUse bidirectional evaluation? (y/n, default: n): \").strip().lower()\n",
        "    bidirectional = bidirectional_input != 'n'\n",
        "\n",
        "    # Step 6: Confirm before starting\n",
        "    print(\"\\n=== Tournament Configuration ===\")\n",
        "    print(f\"Ideas: {len(ideas)}\")\n",
        "    print(f\"Variant: {variant}\")\n",
        "    print(f\"Model: {model}\")\n",
        "    print(f\"Bidirectional: {bidirectional}\")\n",
        "    print(f\"Parameters: {prompt_kwargs}\")\n",
        "    print(f\"Estimated LLM calls: {len(ideas) * (len(ideas)-1) // 2 * (2 if bidirectional else 1)}\")\n",
        "\n",
        "    confirm = input(\"\\nProceed with tournament? (y/n): \").strip().lower()\n",
        "    if confirm != 'y':\n",
        "        print(\"Tournament cancelled.\")\n",
        "        return\n",
        "\n",
        "    # Step 7: Run tournament\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    try:\n",
        "        tournament_results = run_tournament(\n",
        "            ideas=ideas,\n",
        "            variant=variant,\n",
        "            model=model,\n",
        "            prompt_kwargs=prompt_kwargs,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâœ— Error during tournament: {e}\")\n",
        "        return\n",
        "\n",
        "    # Step 8: Save results\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_path = f\"tournament_results_{variant}_{timestamp}.jsonl\"\n",
        "\n",
        "    try:\n",
        "        save_tournament_results(tournament_results, output_path)\n",
        "        print(f\"\\nâœ“ Tournament completed successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâœ— Error saving results: {e}\")\n",
        "        return\n",
        "\n",
        "    # Step 9: Offer to display full rankings\n",
        "    show_all = input(\"\\nShow all rankings? (y/n): \").strip().lower()\n",
        "    if show_all == 'y':\n",
        "        print(\"\\n=== Complete Rankings ===\")\n",
        "        for item in tournament_results['rankings']:\n",
        "            print(f\"{item['rank']:2d}. {item['idea_id']} - Wins: {item['wins']:.1f} ({item['win_rate']:.1%})\")\n",
        "            print(f\"    {item['idea_text']}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFzIUbWLr5ew",
        "outputId": "30477f22-daf3-476c-e0b0-5e6d5349afc1"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Tournament-Based Idea Reviewer ===\n",
            "\n",
            "Enter path to ideas file (JSONL or CSV): /content/sample_ideas.jsonl\n",
            "âœ“ Loaded 4 ideas successfully\n",
            "\n",
            "Available evaluation variants:\n",
            "  - coi_5axis\n",
            "  - gcs_simple\n",
            "Select variant: coi_5axis\n",
            "\n",
            "Parameters needed for coi_5axis:\n",
            "Enter topic/primary area: Artificial Intelligence\n",
            "\n",
            "Common models:\n",
            "  - google/gemini-2.0-flash-exp\n",
            "  - openai/gpt-4o\n",
            "  - anthropic/claude-3-5-sonnet\n",
            "  - openai/o3\n",
            "  - anthropic/claude-3.7-sonnet\n",
            "Enter model name: anthropic/claude-3.7-sonnet\n",
            "\n",
            "Use bidirectional evaluation? (y/n, default: n): y\n",
            "\n",
            "=== Tournament Configuration ===\n",
            "Ideas: 4\n",
            "Variant: coi_5axis\n",
            "Model: anthropic/claude-3.7-sonnet\n",
            "Bidirectional: True\n",
            "Parameters: {'topic': 'Artificial Intelligence'}\n",
            "Estimated LLM calls: 12\n",
            "\n",
            "Proceed with tournament? (y/n): y\n",
            "\n",
            "==================================================\n",
            "Starting tournament with 4 ideas\n",
            "Total matches to run: 6\n",
            "Total LLM calls: 12\n",
            "--------------------------------------------------\n",
            "Match 1/6: idea_001 vs idea_002\n",
            "\n",
            "[DEBUG] Starting match: idea_001 vs idea_002\n",
            "[DEBUG] Variant: coi_5axis, Model: anthropic/claude-3.7-sonnet\n",
            "[DEBUG] Prompt kwargs: {'topic': 'Artificial Intelligence'}\n",
            "\n",
            "[DEBUG] Direction: forward\n",
            "[DEBUG] Generated prompt length: 2885\n",
            "[DEBUG] First 200 chars of prompt: \n",
            "    You are a judge in a competition. You have to decide which idea is better.\n",
            "\n",
            "    The idea0 is: Using diffusion models to generate synthetic training data for rare event detection in autonomous veh...\n",
            "[DEBUG] LLM call success: True\n",
            "[DEBUG] Response length: 2218\n",
            "[DEBUG] First 200 chars: Your thinking process:\n",
            "\n",
            "I need to evaluate two AI-related ideas on five criteria: novelty, significance, feasibility, clarity, and effectiveness.\n",
            "\n",
            "Idea 0: Using diffusion models to generate synthetic ...\n",
            "[DEBUG] Parser success: True\n",
            "[DEBUG] Overall winner: idea1\n",
            "\n",
            "[DEBUG] Direction: reverse\n",
            "[DEBUG] Generated prompt length: 2885\n",
            "[DEBUG] First 200 chars of prompt: \n",
            "    You are a judge in a competition. You have to decide which idea is better.\n",
            "\n",
            "    The idea0 is: Implementing a federated learning framework with differential privacy for collaborative medical diagn...\n",
            "[DEBUG] LLM call success: True\n",
            "[DEBUG] Response length: 1848\n",
            "[DEBUG] First 200 chars: Your thinking process:\n",
            "\n",
            "To evaluate these two AI-focused ideas, I'll analyze each criterion systematically:\n",
            "\n",
            "Novelty: \n",
            "- Idea0 (Federated learning with differential privacy): While federated learning ...\n",
            "[DEBUG] Parser success: True\n",
            "[DEBUG] Overall winner: tie\n",
            "Match results include success.\n",
            "Match results include success.\n",
            "Match 2/6: idea_001 vs idea_003\n",
            "\n",
            "[DEBUG] Starting match: idea_001 vs idea_003\n",
            "[DEBUG] Variant: coi_5axis, Model: anthropic/claude-3.7-sonnet\n",
            "[DEBUG] Prompt kwargs: {'topic': 'Artificial Intelligence'}\n",
            "\n",
            "[DEBUG] Direction: forward\n",
            "[DEBUG] Generated prompt length: 2855\n",
            "[DEBUG] First 200 chars of prompt: \n",
            "    You are a judge in a competition. You have to decide which idea is better.\n",
            "\n",
            "    The idea0 is: Using diffusion models to generate synthetic training data for rare event detection in autonomous veh...\n",
            "[DEBUG] LLM call success: True\n",
            "[DEBUG] Response length: 1500\n",
            "[DEBUG] First 200 chars: Your thinking process:\n",
            "\n",
            "For novelty, idea0 applies diffusion models to generate synthetic data for autonomous vehicles, which is a practical application of existing technology. Idea1 proposes a meta-l...\n",
            "[DEBUG] Parser success: True\n",
            "[DEBUG] Overall winner: tie\n",
            "\n",
            "[DEBUG] Direction: reverse\n",
            "[DEBUG] Generated prompt length: 2855\n",
            "[DEBUG] First 200 chars of prompt: \n",
            "    You are a judge in a competition. You have to decide which idea is better.\n",
            "\n",
            "    The idea0 is: Developing a meta-learning algorithm that can quickly adapt to new robotics tasks by learning a share...\n",
            "[DEBUG] LLM call success: True\n",
            "[DEBUG] Response length: 1688\n",
            "[DEBUG] First 200 chars: Your thinking process:\n",
            "\n",
            "I need to evaluate two AI ideas based on novelty, significance, feasibility, clarity, and effectiveness.\n",
            "\n",
            "Idea 0: A meta-learning algorithm using graph neural networks to enabl...\n",
            "[DEBUG] Parser success: True\n",
            "[DEBUG] Overall winner: idea1\n",
            "Match results include success.\n",
            "Match results include success.\n",
            "Match 3/6: idea_001 vs idea_004\n",
            "\n",
            "[DEBUG] Starting match: idea_001 vs idea_004\n",
            "[DEBUG] Variant: coi_5axis, Model: anthropic/claude-3.7-sonnet\n",
            "[DEBUG] Prompt kwargs: {'topic': 'Artificial Intelligence'}\n",
            "\n",
            "[DEBUG] Direction: forward\n",
            "[DEBUG] Generated prompt length: 2862\n",
            "[DEBUG] First 200 chars of prompt: \n",
            "    You are a judge in a competition. You have to decide which idea is better.\n",
            "\n",
            "    The idea0 is: Using diffusion models to generate synthetic training data for rare event detection in autonomous veh...\n",
            "[DEBUG] LLM call success: True\n",
            "[DEBUG] Response length: 1839\n",
            "[DEBUG] First 200 chars: Your thinking process:\n",
            "\n",
            "I need to evaluate two AI ideas on novelty, significance, feasibility, clarity and effectiveness.\n",
            "\n",
            "Idea 0 (Synthetic Data for Autonomous Vehicles):\n",
            "- Novelty: Applies diffusion...\n",
            "[DEBUG] Parser success: True\n",
            "[DEBUG] Overall winner: idea0\n",
            "\n",
            "[DEBUG] Direction: reverse\n",
            "[DEBUG] Generated prompt length: 2862\n",
            "[DEBUG] First 200 chars of prompt: \n",
            "    You are a judge in a competition. You have to decide which idea is better.\n",
            "\n",
            "    The idea0 is: Creating an LLM-powered code review system that not only identifies bugs but also suggests architectu...\n",
            "[DEBUG] LLM call success: True\n",
            "[DEBUG] Response length: 2111\n",
            "[DEBUG] First 200 chars: Your thinking process:\n",
            "\n",
            "I'll compare both ideas across the evaluation criteria:\n",
            "\n",
            "Novelty:\n",
            "- Idea 0 (LLM code review): Combines existing LLM capabilities with retrieval-augmented generation for code im...\n",
            "[DEBUG] Parser success: True\n",
            "[DEBUG] Overall winner: idea1\n",
            "Match results include success.\n",
            "Match results include success.\n",
            "Match 4/6: idea_002 vs idea_003\n",
            "\n",
            "[DEBUG] Starting match: idea_002 vs idea_003\n",
            "[DEBUG] Variant: coi_5axis, Model: anthropic/claude-3.7-sonnet\n",
            "[DEBUG] Prompt kwargs: {'topic': 'Artificial Intelligence'}\n",
            "\n",
            "[DEBUG] Direction: forward\n",
            "[DEBUG] Generated prompt length: 2866\n",
            "[DEBUG] First 200 chars of prompt: \n",
            "    You are a judge in a competition. You have to decide which idea is better.\n",
            "\n",
            "    The idea0 is: Implementing a federated learning framework with differential privacy for collaborative medical diagn...\n",
            "[DEBUG] LLM call success: True\n",
            "[DEBUG] Response length: 1582\n",
            "[DEBUG] First 200 chars: Your thinking process:\n",
            "\n",
            "I need to evaluate two AI-related ideas:\n",
            "- Idea 0: Federated learning with differential privacy for medical diagnosis across hospitals\n",
            "- Idea 1: Meta-learning algorithm using g...\n",
            "[DEBUG] Parser success: True\n",
            "[DEBUG] Overall winner: idea0\n",
            "\n",
            "[DEBUG] Direction: reverse\n",
            "[DEBUG] Generated prompt length: 2866\n",
            "[DEBUG] First 200 chars of prompt: \n",
            "    You are a judge in a competition. You have to decide which idea is better.\n",
            "\n",
            "    The idea0 is: Developing a meta-learning algorithm that can quickly adapt to new robotics tasks by learning a share...\n",
            "[DEBUG] LLM call success: True\n",
            "[DEBUG] Response length: 2050\n",
            "[DEBUG] First 200 chars: Your thinking process:\n",
            "I need to evaluate both ideas in the context of artificial intelligence, comparing them on novelty, significance, feasibility, clarity, and effectiveness.\n",
            "\n",
            "Idea 0: Meta-learning...\n",
            "[DEBUG] Parser success: True\n",
            "[DEBUG] Overall winner: idea1\n",
            "Match results include success.\n",
            "Match results include success.\n",
            "Match 5/6: idea_002 vs idea_004\n",
            "\n",
            "[DEBUG] Starting match: idea_002 vs idea_004\n",
            "[DEBUG] Variant: coi_5axis, Model: anthropic/claude-3.7-sonnet\n",
            "[DEBUG] Prompt kwargs: {'topic': 'Artificial Intelligence'}\n",
            "\n",
            "[DEBUG] Direction: forward\n",
            "[DEBUG] Generated prompt length: 2873\n",
            "[DEBUG] First 200 chars of prompt: \n",
            "    You are a judge in a competition. You have to decide which idea is better.\n",
            "\n",
            "    The idea0 is: Implementing a federated learning framework with differential privacy for collaborative medical diagn...\n",
            "[DEBUG] LLM call success: True\n",
            "[DEBUG] Response length: 2090\n",
            "[DEBUG] First 200 chars: Your thinking process:\n",
            "\n",
            "I'll evaluate both ideas based on the provided criteria:\n",
            "\n",
            "**Idea 0: Federated learning framework with differential privacy for medical diagnosis**\n",
            "- Novelty: Combines establish...\n",
            "[DEBUG] Parser success: True\n",
            "[DEBUG] Overall winner: idea0\n",
            "\n",
            "[DEBUG] Direction: reverse\n",
            "[DEBUG] Generated prompt length: 2873\n",
            "[DEBUG] First 200 chars of prompt: \n",
            "    You are a judge in a competition. You have to decide which idea is better.\n",
            "\n",
            "    The idea0 is: Creating an LLM-powered code review system that not only identifies bugs but also suggests architectu...\n",
            "[DEBUG] LLM call success: True\n",
            "[DEBUG] Response length: 2037\n",
            "[DEBUG] First 200 chars: Your thinking process:\n",
            "\n",
            "I'll evaluate both ideas on the five criteria:\n",
            "\n",
            "Novelty:\n",
            "- Idea 0 (LLM code review system): Uses established LLM and RAG technologies in a domain-specific way. While code revie...\n",
            "[DEBUG] Parser success: True\n",
            "[DEBUG] Overall winner: idea1\n",
            "Match results include success.\n",
            "Match results include success.\n",
            "Match 6/6: idea_003 vs idea_004\n",
            "\n",
            "[DEBUG] Starting match: idea_003 vs idea_004\n",
            "[DEBUG] Variant: coi_5axis, Model: anthropic/claude-3.7-sonnet\n",
            "[DEBUG] Prompt kwargs: {'topic': 'Artificial Intelligence'}\n",
            "\n",
            "[DEBUG] Direction: forward\n",
            "[DEBUG] Generated prompt length: 2843\n",
            "[DEBUG] First 200 chars of prompt: \n",
            "    You are a judge in a competition. You have to decide which idea is better.\n",
            "\n",
            "    The idea0 is: Developing a meta-learning algorithm that can quickly adapt to new robotics tasks by learning a share...\n",
            "[DEBUG] LLM call success: True\n",
            "[DEBUG] Response length: 2164\n",
            "[DEBUG] First 200 chars: Your thinking process:\n",
            "\n",
            "Let me evaluate each idea based on the five criteria.\n",
            "\n",
            "Idea 0: Meta-learning algorithm for robot morphology adaptation\n",
            "- Novelty: Graph neural networks for robot structure enco...\n",
            "[DEBUG] Parser success: True\n",
            "[DEBUG] Overall winner: tie\n",
            "\n",
            "[DEBUG] Direction: reverse\n",
            "[DEBUG] Generated prompt length: 2843\n",
            "[DEBUG] First 200 chars of prompt: \n",
            "    You are a judge in a competition. You have to decide which idea is better.\n",
            "\n",
            "    The idea0 is: Creating an LLM-powered code review system that not only identifies bugs but also suggests architectu...\n",
            "[DEBUG] LLM call success: True\n",
            "[DEBUG] Response length: 1526\n",
            "[DEBUG] First 200 chars: Your thinking process:\n",
            "\n",
            "Let me evaluate both ideas on the five criteria:\n",
            "\n",
            "Novelty:\n",
            "- Idea0 (LLM code review): Combines existing LLM capabilities with RAG for code review, which is an incremental advan...\n",
            "[DEBUG] Parser success: True\n",
            "[DEBUG] Overall winner: idea1\n",
            "Match results include success.\n",
            "Match results include success.\n",
            "--------------------------------------------------\n",
            "Tournament complete! Failed comparisons: 0\n",
            "[DEBUG] Match 0 has 2 results\n",
            "[DEBUG] Match 1 has 2 results\n",
            "[DEBUG] Match 2 has 2 results\n",
            "[DEBUG] Match 3 has 2 results\n",
            "[DEBUG] Match 4 has 2 results\n",
            "[DEBUG] Match 5 has 2 results\n",
            "\n",
            "Tournament results saved to tournament_results_coi_5axis_20250611_212448.jsonl\n",
            "Total lines written: 7\n",
            "\n",
            "Top 3 ideas:\n",
            "1. idea_002 (wins: 5.5, rate: 183.33%)\n",
            "   Implementing a federated learning framework with differential privacy for collaborative medical diag...\n",
            "2. idea_001 (wins: 4.0, rate: 133.33%)\n",
            "   Using diffusion models to generate synthetic training data for rare event detection in autonomous ve...\n",
            "3. idea_003 (wins: 2.0, rate: 66.67%)\n",
            "   Developing a meta-learning algorithm that can quickly adapt to new robotics tasks by learning a shar...\n",
            "\n",
            "âœ“ Tournament completed successfully!\n",
            "\n",
            "Show all rankings? (y/n): y\n",
            "\n",
            "=== Complete Rankings ===\n",
            " 1. idea_002 - Wins: 5.5 (183.3%)\n",
            "    Implementing a federated learning framework with differential privacy for collaborative medical diag...\n",
            "\n",
            " 2. idea_001 - Wins: 4.0 (133.3%)\n",
            "    Using diffusion models to generate synthetic training data for rare event detection in autonomous ve...\n",
            "\n",
            " 3. idea_003 - Wins: 2.0 (66.7%)\n",
            "    Developing a meta-learning algorithm that can quickly adapt to new robotics tasks by learning a shar...\n",
            "\n",
            " 4. idea_004 - Wins: 0.5 (16.7%)\n",
            "    Creating an LLM-powered code review system that not only identifies bugs but also suggests architect...\n",
            "\n"
          ]
        }
      ]
    }
  ]
}