{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhruvtre/Lossfunk_Code/blob/main/Empirical_Observation_Extraction_and_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSj2mVeZmDub"
      },
      "source": [
        "## PDF Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RT02GpEg4L2"
      },
      "outputs": [],
      "source": [
        "!pip install pyMuPDF\n",
        "!pip install pymupdf4llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sJ2F5hzSg5rW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import pymupdf\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hLZ51zl-lCPK"
      },
      "outputs": [],
      "source": [
        "def prepare_urls_for_download(url_string: str) -> list:\n",
        "    \"\"\"\n",
        "    Takes a comma-separated string of PDF URLs and prepares a list of dictionaries\n",
        "    with 'pdf_url' and 'paper_title' for the download function.\n",
        "\n",
        "    Args:\n",
        "        url_string: A string containing one or more PDF URLs separated by commas.\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries, each with 'pdf_url' and 'paper_title'.\n",
        "    \"\"\"\n",
        "    urls = [url.strip() for url in url_string.split(',') if url.strip()]\n",
        "    prepared_list = []\n",
        "    for i, url in enumerate(urls):\n",
        "        # Create a simple paper title from the URL or just use an index\n",
        "        # You might want a more sophisticated way to get a title if possible\n",
        "        paper_title = f\"paper_{i+1}\" # Simple title for now\n",
        "\n",
        "        prepared_list.append({\n",
        "            'pdf_url': url,\n",
        "            'paper_title': paper_title\n",
        "        })\n",
        "    return prepared_list\n",
        "\n",
        "# Example usage:\n",
        "# url_input = \"https://arxiv.org/pdf/2407.12345.pdf, https://proceedings.mlr.press/v1/paper_a.pdf\"\n",
        "# paper_list_for_download = prepare_urls_for_download(url_input)\n",
        "# print(paper_list_for_download)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "G2zQM4PJiZfW"
      },
      "outputs": [],
      "source": [
        "def download_pdf(pdf_url, paper_title, download_dir=\"Hypothesis_Generator_Explorer_Test\"):\n",
        "    \"\"\"Download a PDF from a URL and save it locally\"\"\"\n",
        "    # Create download directory if it doesn't exist\n",
        "    if not os.path.exists(download_dir):\n",
        "        os.makedirs(download_dir)\n",
        "\n",
        "    file_path = os.path.join(download_dir, f\"{paper_title}.pdf\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(pdf_url, timeout=30)\n",
        "        response.raise_for_status()  # Raise exception for HTTP errors\n",
        "\n",
        "        with open(file_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        return file_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {pdf_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# # Quick Test to Check Download\n",
        "# download_test_pdf_url = first_paper_sample[0][\"pdf_url\"]\n",
        "# download_test_paper_title = first_paper_sample[0][\"title\"]\n",
        "# file_path = download_pdf(pdf_url=download_test_pdf_url, paper_title=download_test_paper_title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fE0IxjdEiaNC"
      },
      "outputs": [],
      "source": [
        "def extract_pdf_text_md(pdf_path, paper_title, download=False):\n",
        "    \"\"\"Extract text from a PDF file and convert to markdown\"\"\"\n",
        "    print(\"Checking for pdf path before markdown extraction.\")\n",
        "    if not pdf_path or not os.path.exists(pdf_path):\n",
        "      print(\"No PDF path found for markdown extraction.\")\n",
        "      return None\n",
        "\n",
        "    try:\n",
        "        # Get the directory from the pdf_path\n",
        "        print(\"Getting PDF directory name.\")\n",
        "        pdf_dir = os.path.dirname(pdf_path)\n",
        "\n",
        "        # Convert the document to markdown\n",
        "        import pymupdf4llm\n",
        "        print(f\"Getting markdown from {paper_title}.\")\n",
        "        md_text = pymupdf4llm.to_markdown(pdf_path)\n",
        "\n",
        "        if download:\n",
        "          # Create markdown file path using the same directory and paper title\n",
        "          md_file_path = os.path.join(pdf_dir, f\"{paper_title}.md\")\n",
        "\n",
        "          # Write the text to file in UTF8-encoding\n",
        "          with open(md_file_path, 'wb') as f:\n",
        "            f.write(md_text.encode())\n",
        "\n",
        "          return md_text, md_file_path\n",
        "\n",
        "        else:\n",
        "          return md_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting markdown from {pdf_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# # Quick Test for Extract Markdown Function\n",
        "# file_path=\"/content/downloaded_pdfs_sample_1/TimeSuite: Improving MLLMs for Long Video Understanding via Grounded Tuning.pdf\"\n",
        "# download_test_content = extract_pdf_text_md(file_path, download_test_paper_title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "k0fQJUoTidPr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "def cache_paper_text(paper_id, paper_title, text_data, cache_dir=\"paper_text_cache\"):\n",
        "    \"\"\"\n",
        "    Cache or retrieve paper text data.\n",
        "\n",
        "    Args:\n",
        "        paper_id: Unique paper ID for filename\n",
        "        paper_title: Paper title (for logging)\n",
        "        text_data: If provided, save to cache. If None, try to load from cache\n",
        "        cache_dir: Directory to store cached files\n",
        "\n",
        "    Returns:\n",
        "        tuple: (success: bool, data: any)\n",
        "        - If saving: (True, text_data) on success\n",
        "        - If loading: (True, loaded_data) if exists, (False, None) if not\n",
        "    \"\"\"\n",
        "    # Create cache directory if needed\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "    # Use paper_id for filename to avoid issues with special characters in titles\n",
        "    cache_file = os.path.join(cache_dir, f\"{paper_id}.pkl\")\n",
        "\n",
        "    if text_data is not None:\n",
        "        # SAVE mode\n",
        "        try:\n",
        "            with open(cache_file, 'wb') as f:\n",
        "                pickle.dump(text_data, f)\n",
        "            print(f\"‚úÖ Cached: {paper_title[:50]}...\")\n",
        "            return True, text_data\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Cache save failed for {paper_title}: {e}\")\n",
        "            return False, None\n",
        "    else:\n",
        "        print(\"Text data is empty. Skipping caching.\")\n",
        "        return False, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elJm7iz3qQ0Z"
      },
      "source": [
        "## LLM Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "cWSruYawgSbk"
      },
      "outputs": [],
      "source": [
        "# Getting OpenRouter API Key\n",
        "from google.colab import userdata\n",
        "openrouter_api_key = userdata.get('OpenRouter_dhruv_key')\n",
        "\n",
        "import requests\n",
        "import json\n",
        "from typing import Optional\n",
        "import re\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "i7H_gEcAir8C"
      },
      "outputs": [],
      "source": [
        "def send_ai_request(user_message, system_prompt=None, model=\"google/gemini-2.5-pro-preview-03-25\", file=None, file_data=None, file_name=None, temperature=0.7):\n",
        "    \"\"\"Send a request to the OpenRouter API and return the response\"\"\"\n",
        "    # Construction message\n",
        "    messages_array = []\n",
        "    if system_prompt:\n",
        "      system_prompt_message = {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": system_prompt\n",
        "      }\n",
        "      messages_array.append(system_prompt_message)\n",
        "\n",
        "    user_message_prompt_message = [{\n",
        "        \"type\": \"text\",\n",
        "        \"text\": user_message\n",
        "    }]\n",
        "    if file:\n",
        "        user_message_prompt_message.append({\n",
        "            \"type\": \"file\",\n",
        "            \"file\": {\n",
        "            \"filename\": file_name,\n",
        "            \"file_data\": file_data\n",
        "            }\n",
        "        }\n",
        "        )\n",
        "    user_message_prompt_message = str(user_message_prompt_message)\n",
        "\n",
        "    user_message = {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_message_prompt_message\n",
        "    }\n",
        "\n",
        "    messages_array.append(user_message)\n",
        "\n",
        "    response = requests.post(\n",
        "        url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
        "        headers={\n",
        "            \"Authorization\": f\"Bearer {openrouter_api_key}\"\n",
        "        },\n",
        "        data=json.dumps({\n",
        "            \"model\": model,\n",
        "            \"messages\": messages_array,\n",
        "            \"temperature\": temperature,\n",
        "            # \"max_tokens\": 5000,\n",
        "            \"transforms\" : [\"middle-out\"]\n",
        "        })\n",
        "    )\n",
        "    return response.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bHxIFFu1iv-7"
      },
      "outputs": [],
      "source": [
        "def parse_ai_response(response, reasoning=False):\n",
        "    \"\"\"Extract the content and usage metrics from API response\"\"\"\n",
        "    try:\n",
        "        content = response['choices'][0]['message']['content']\n",
        "        usage = response['usage']\n",
        "        if reasoning:\n",
        "            reasoning_text = response['choices'][0]['message']['reasoning']\n",
        "            return {\n",
        "              'content': content,\n",
        "              'usage': usage,\n",
        "              'reasoning': reasoning_text,\n",
        "              'success': True\n",
        "          }\n",
        "        elif reasoning is False:\n",
        "            return {\n",
        "              'content': content,\n",
        "              'usage': usage,\n",
        "              'success': True\n",
        "          }\n",
        "    except (KeyError, IndexError) as e:\n",
        "        return {\n",
        "            'content': None,\n",
        "            'usage': None,\n",
        "            'success': False,\n",
        "            'reasoning': None,\n",
        "            'error': str(e),\n",
        "            'response': response\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JcNEpmyNt2u6"
      },
      "outputs": [],
      "source": [
        "def extract_json_between_markers(llm_output: str) -> dict | None:\n",
        "    # Regular expression pattern to find JSON content between ```json and ```\n",
        "    json_pattern = r\"```json(.*?)```\"\n",
        "    matches = re.findall(json_pattern, llm_output, re.DOTALL)\n",
        "\n",
        "    if not matches:\n",
        "        # Fallback: Try to find any JSON-like content in the output\n",
        "        json_pattern = r\"\\{.*?\\}\"\n",
        "        matches = re.findall(json_pattern, llm_output, re.DOTALL)\n",
        "\n",
        "    for json_string in matches:\n",
        "        json_string = json_string.strip()\n",
        "        try:\n",
        "            parsed_json = json.loads(json_string)\n",
        "            return parsed_json\n",
        "        except json.JSONDecodeError:\n",
        "            # Attempt to fix common JSON issues\n",
        "            try:\n",
        "                # Remove invalid control characters\n",
        "                json_string_clean = re.sub(r\"[\\x00-\\x1F\\x7F]\", \"\", json_string)\n",
        "                parsed_json = json.loads(json_string_clean)\n",
        "                return parsed_json\n",
        "            except json.JSONDecodeError:\n",
        "                continue  # Try next match\n",
        "\n",
        "    return None  # No valid JSON found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GEadRm9mnVd"
      },
      "source": [
        "## Paper Loading and Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "SemoBQINqsZg"
      },
      "outputs": [],
      "source": [
        "# Function for Loading Paper Text from Cache\n",
        "def load_cached_paper_text(paper_id, cache_dir=\"paper_text_cache\"):\n",
        "   \"\"\"\n",
        "   Load cached paper text for a given paper ID.\n",
        "\n",
        "   Args:\n",
        "       paper_id: Unique paper ID\n",
        "       cache_dir: Directory where cached files are stored\n",
        "\n",
        "   Returns:\n",
        "       tuple: (success: bool, text_data: any)\n",
        "       - If found: (True, loaded_text_data)\n",
        "       - If not found or error: (False, None)\n",
        "   \"\"\"\n",
        "   cache_file = os.path.join(cache_dir, f\"{paper_id}.pkl\")\n",
        "\n",
        "   if not os.path.exists(cache_file):\n",
        "       print(f\"‚ùå No cached text found for paper ID: {paper_id}\")\n",
        "       return False, None\n",
        "\n",
        "   try:\n",
        "       with open(cache_file, 'rb') as f:\n",
        "           text_data = pickle.load(f)\n",
        "       print(f\"‚úÖ Loaded cached text for paper ID: {paper_id}\")\n",
        "       return True, text_data\n",
        "   except Exception as e:\n",
        "       print(f\"‚ùå Error loading cache for paper ID {paper_id}: {e}\")\n",
        "       return False, None\n",
        "\n",
        "\n",
        "# Function for Chunking by Paragraph\n",
        "def chunk_by_paragraph(text, min_length=1000):\n",
        "   \"\"\"\n",
        "   Chunk text by paragraphs, merging short chunks with previous ones.\n",
        "\n",
        "   Args:\n",
        "       text: The full paper text\n",
        "       min_length: Minimum character length for a standalone chunk\n",
        "\n",
        "   Returns:\n",
        "       List of paragraph chunks\n",
        "   \"\"\"\n",
        "   # Split on double newlines\n",
        "   raw_paragraphs = text.split('\\n\\n')\n",
        "\n",
        "   # Clean paragraphs first\n",
        "   paragraphs = [para.strip() for para in raw_paragraphs if para.strip()]\n",
        "\n",
        "   if not paragraphs:\n",
        "       return []\n",
        "\n",
        "   chunks = []\n",
        "\n",
        "   for i in range(len(paragraphs)):\n",
        "       para = paragraphs[i]\n",
        "\n",
        "       if len(para) < min_length and chunks:\n",
        "           # Short paragraph and we have a previous chunk - merge with previous\n",
        "           chunks[-1] += \"\\n\\n\" + para\n",
        "       else:\n",
        "           # Either long enough or it's the first chunk\n",
        "           chunks.append(para)\n",
        "\n",
        "   return chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDZl9RnRm1Td"
      },
      "source": [
        "## Prompt and Chunk Labelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "zsRrZ_SkbgSy"
      },
      "outputs": [],
      "source": [
        "def property_labelling_prompt(property, property_description, paper_title, chunk, paper_abstract=None):\n",
        "    property_labelling_prompt = f'''You are a scientist and expert information extractor following philosophy of science principles.\n",
        "\n",
        "    Your task is to identify and extract {property} from research text.\n",
        "\n",
        "    {property_description}\n",
        "\n",
        "    Your final output should be in the following format:\n",
        "    <thinking>\n",
        "    [Your reasoning about whether the property is present]\n",
        "    </thinking>\n",
        "    <label>\n",
        "    [PROPERTY_NAME if property present, NOT otherwise]\n",
        "    </label>\n",
        "    <extracted_content>\n",
        "    [If property present: State the extracted content as a complete, self-contained claim. If NOT: Write \"NONE\"]\n",
        "    </extracted_content>\n",
        "    <citations>\n",
        "    [If property present: Include citation markers from the chunk. If NOT or no citations: Write \"NONE\"]\n",
        "    </citations>\n",
        "\n",
        "    Chunk to analyze:\n",
        "    {chunk}\n",
        "\n",
        "    Paper context:\n",
        "    Title: {paper_title}\n",
        "\n",
        "    Use paper context to understand the chunk, but only extract {property} directly stated IN the chunk.\n",
        "\n",
        "    '''\n",
        "    return property_labelling_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "VicTdhsR0KIc"
      },
      "outputs": [],
      "source": [
        "LABELLING_MODEL = \"openai/gpt-5\"\n",
        "\n",
        "# Single paper chunk labelling\n",
        "def label_all_chunks_per_paper(paper_dict, property_name, property_description, model=LABELLING_MODEL, max_chunks=None):\n",
        "  # paper_id = paper_dict['id']\n",
        "  paper_title = paper_dict['paper_title']\n",
        "  pdf_url = paper_dict['pdf_url']\n",
        "  # paper_abstract = paper_dict['abstract']\n",
        "  paper_chunks = paper_dict['chunks']\n",
        "  print(f\"Labelling chunks for paper: {paper_title}, {pdf_url}\")\n",
        "  print(f\"Total chunks: {len(paper_chunks)}\")\n",
        "  print(f\"{'='*60}\\n\")\n",
        "  labeled_chunks = []\n",
        "  if max_chunks:\n",
        "      paper_chunks = paper_chunks[:max_chunks]\n",
        "\n",
        "  print(f\"Starting labelling for: {len(paper_chunks)}\")\n",
        "  for i in range(len(paper_chunks)):\n",
        "    print(f\"Labelling chunk {i+1}/{len(paper_chunks)}\")\n",
        "    chunk = paper_chunks[i]\n",
        "    # print(chunk[:100])\n",
        "    prompt = property_labelling_prompt(property_name, property_description, paper_title, chunk)\n",
        "    # print(prompt[:100])\n",
        "    # print(f\"Prompt for chunk {i+1}/{len(paper_chunks)} created.\")\n",
        "    response = send_ai_request(prompt, model=model)\n",
        "    # print(response)\n",
        "    print(f\"AI request for chunk {i+1}/{len(paper_chunks)} completed.\")\n",
        "    parsed_response = parse_ai_response(response)\n",
        "    # print(parsed_response)\n",
        "\n",
        "    if parsed_response['success']:\n",
        "      print(f\"AI response for chunk {i+1}/{len(paper_chunks)} parsed.\")\n",
        "      labeled_chunk = {\n",
        "              'chunk_n': i,\n",
        "              'chunk_text': chunk,\n",
        "              'label_output': parsed_response\n",
        "              }\n",
        "      labeled_chunks.append(labeled_chunk)\n",
        "      print(f\"AI request for chunk {i+1}/{len(paper_chunks)} appended.\")\n",
        "\n",
        "  # Add to paper dict\n",
        "  if max_chunks:\n",
        "    paper_dict[f'labeled_chunks_{max_chunks}'] = labeled_chunks\n",
        "    print(f\"AI request for labelling {max_chunks} chunks complete. Appended.\")\n",
        "    return True\n",
        "  else:\n",
        "    paper_dict[f'labeled_chunks_all'] = labeled_chunks\n",
        "    print(f\"AI request for labelling {max_chunks} chunks complete. Appended.\")\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvJRMgvpbgSz"
      },
      "source": [
        "## Embedding Preparation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "wyBRaYNtliqF"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "328EaTl2Ju1R"
      },
      "outputs": [],
      "source": [
        "def parse_property_output(content):\n",
        "    \"\"\"\n",
        "    Extract thinking, label, extracted_content, and citations from the LLM output.\n",
        "\n",
        "    Args:\n",
        "        content: The raw LLM response string\n",
        "\n",
        "    Returns:\n",
        "        dict with keys: thinking, label, extracted_content, citations\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    # Initialize with defaults\n",
        "    parsed = {\n",
        "        'thinking': '',\n",
        "        'label': 'NOT',\n",
        "        'extracted_content': 'NONE',\n",
        "        'citations': 'NONE'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Extract thinking\n",
        "        thinking_match = re.search(r'<thinking>(.*?)</thinking>', content, re.DOTALL)\n",
        "        if thinking_match:\n",
        "            parsed['thinking'] = thinking_match.group(1).strip()\n",
        "\n",
        "        # Extract label\n",
        "        label_match = re.search(r'<label>(.*?)</label>', content, re.DOTALL)\n",
        "        if label_match:\n",
        "            parsed['label'] = label_match.group(1).strip()\n",
        "\n",
        "        # Extract extracted content\n",
        "        ext_content_match = re.search(r'<extracted_content>(.*?)</extracted_content>', content, re.DOTALL)\n",
        "        if ext_content_match:\n",
        "            parsed['extracted_content'] = ext_content_match.group(1).strip()\n",
        "\n",
        "        # Extract citations\n",
        "        cite_match = re.search(r'<citations>(.*?)</citations>', content, re.DOTALL)\n",
        "        if cite_match:\n",
        "            parsed['citations'] = cite_match.group(1).strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing observation output: {e}\")\n",
        "\n",
        "    return parsed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "OqbYcdwebgS1"
      },
      "outputs": [],
      "source": [
        "def prepare_chunks_for_embedding(papers, property_label, labeled_chunks_key='labeled_chunks_20'):\n",
        "    \"\"\"\n",
        "    Extract and prepare chunks with a specific property label for embedding.\n",
        "\n",
        "    Args:\n",
        "        papers: List of paper dictionaries with labeled chunks\n",
        "        property_label: Label to filter for (e.g., 'OBS', 'METHODOLOGY', 'LIMITATION')\n",
        "        labeled_chunks_key: Key in paper dict for labeled chunks (e.g., 'labeled_chunks_20', 'labeled_chunks_all')\n",
        "\n",
        "    Returns:\n",
        "        List of dictionaries with paper_id, paper_title, extracted_content, has_citations, citations\n",
        "    \"\"\"\n",
        "    extracted_content_for_embedding = []\n",
        "\n",
        "    for paper in papers:\n",
        "        if labeled_chunks_key not in paper:\n",
        "            continue\n",
        "\n",
        "        for chunk in paper[labeled_chunks_key]:\n",
        "          parsed_content = parse_property_output(chunk['label_output']['content'])\n",
        "          chunk['parsed'] = parsed_content\n",
        "          if chunk['parsed']['label'] == property_label:\n",
        "                extracted_content_for_embedding.append({\n",
        "                    # 'paper_id': paper['id'],\n",
        "                    'paper_url' : paper['pdf_url'],\n",
        "                    'paper_title': paper['paper_title'],\n",
        "                    'extracted_content': chunk['parsed']['extracted_content'],\n",
        "                    'has_citations': chunk['parsed']['citations'] != 'NONE',\n",
        "                    'citations': chunk['parsed']['citations']\n",
        "                })\n",
        "          else:\n",
        "            print(f\"{chunk['parsed']['label']} is not the same as {property_label}\")\n",
        "\n",
        "    return extracted_content_for_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "rOqpVigXMnFz"
      },
      "outputs": [],
      "source": [
        "def create_embeddings(extracted_content_for_embedding, model_name='allenai/scibert_scivocab_uncased', batch_size=16, save_path=None):\n",
        "   \"\"\"\n",
        "   Create embeddings for a list of extracted content using sentence-transformers.\n",
        "\n",
        "   Args:\n",
        "       extracted_content_for_embedding: List of dicts with 'extracted_content' key\n",
        "       model_name: Name of the sentence-transformer model\n",
        "       batch_size: Number of texts to encode at once,\n",
        "       save_path: Optional path to save embeddings as pickle. If None, don't save.\n",
        "\n",
        "   Returns:\n",
        "       numpy array of embeddings, same order as input list\n",
        "   \"\"\"\n",
        "\n",
        "   print(f\"Loading embedding model: {model_name}\")\n",
        "   model = SentenceTransformer(model_name)\n",
        "\n",
        "   # Extract just the text\n",
        "   texts = [item['extracted_content'] for item in extracted_content_for_embedding]\n",
        "\n",
        "   print(f\"Embedding {len(texts)} extractions...\")\n",
        "   print(f\"Model embedding dimension: {model.get_sentence_embedding_dimension()}\")\n",
        "\n",
        "   # Encode in batches (more efficient)\n",
        "   embeddings = []\n",
        "   for i in range(0, len(texts), batch_size):\n",
        "       batch = texts[i:i+batch_size]\n",
        "       batch_embeddings = model.encode(batch, show_progress_bar=True)\n",
        "       embeddings.extend(batch_embeddings)\n",
        "\n",
        "   embeddings_array = np.array(embeddings)\n",
        "   print(f\"‚úÖ Created embeddings matrix: {embeddings_array.shape}\")\n",
        "\n",
        "   # Optional save\n",
        "   if save_path:\n",
        "        with open(save_path, 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'embeddings': embeddings_array,\n",
        "                'extractions': extracted_content_for_embedding\n",
        "            }, f)\n",
        "\n",
        "   return embeddings_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxGMrBv0n0cf"
      },
      "source": [
        "## Embedding Clustering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster_extractions_kmeans(embeddings, k=4):\n",
        "      \"\"\"\n",
        "      Cluster extractions using KMeans.\n",
        "\n",
        "      Args:\n",
        "          embeddings: numpy array of embeddings (n_samples, n_features)\n",
        "          k: Number of clusters (default: 5)\n",
        "\n",
        "      Returns:\n",
        "          cluster_labels: Array of cluster assignments\n",
        "          clusterer: Fitted KMeans object\n",
        "      \"\"\"\n",
        "      from sklearn.cluster import KMeans\n",
        "      # from sklearn.preprocessing import normalize\n",
        "\n",
        "      print(f\"Clustering {embeddings.shape[0]} extractions with KMeans (k={k})...\")\n",
        "\n",
        "      # Normalize embeddings for cosine distance\n",
        "      # embeddings_normalized = normalize(embeddings, norm='l2')\n",
        "\n",
        "      # Perform clustering\n",
        "      clusterer = KMeans(n_clusters=k, random_state=42)\n",
        "      cluster_labels = clusterer.fit_predict(embeddings)\n",
        "\n",
        "      # Print summary\n",
        "      print(f\"\\nüìä Clustering Results:\")\n",
        "      print(f\"   Clusters found: {k}\")\n",
        "      for i in range(k):\n",
        "          size = list(cluster_labels).count(i)\n",
        "          print(f\"   Cluster {i}: {size} extractions\")\n",
        "\n",
        "      return cluster_labels, clusterer"
      ],
      "metadata": {
        "id": "X8JBTvRVv5qg"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "Uts0-ZtANlAq"
      },
      "outputs": [],
      "source": [
        "import hdbscan\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "8slwNxETNNu9"
      },
      "outputs": [],
      "source": [
        "def cluster_extractions(embeddings, min_cluster_size=5, min_samples=3):\n",
        "    \"\"\"\n",
        "    Cluster extractions using HDBSCAN on high-dimensional embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Clustering {embeddings.shape[0]} extractions...\")\n",
        "    print(f\"Parameters: min_cluster_size={min_cluster_size}, min_samples={min_samples}\")\n",
        "\n",
        "    # Ensure embeddings are float64\n",
        "    embeddings_float64 = embeddings.astype(np.float64)\n",
        "\n",
        "    # Calculate cosine distance matrix\n",
        "    distance_matrix = cosine_distances(embeddings_float64)\n",
        "\n",
        "    clusterer = hdbscan.HDBSCAN(\n",
        "        min_cluster_size=min_cluster_size,\n",
        "        min_samples=min_samples,\n",
        "        metric='precomputed',\n",
        "        cluster_selection_method='eom',\n",
        "        prediction_data=True\n",
        "    )\n",
        "\n",
        "    cluster_labels = clusterer.fit_predict(distance_matrix)\n",
        "\n",
        "    # Print summary\n",
        "    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
        "    n_noise = list(cluster_labels).count(-1)\n",
        "\n",
        "    print(f\"\\nüìä Clustering Results:\")\n",
        "    print(f\"   Clusters found: {n_clusters}\")\n",
        "    print(f\"   Noise points: {n_noise} ({n_noise/len(cluster_labels)*100:.1f}%)\")\n",
        "\n",
        "    # Cluster sizes\n",
        "    if n_clusters > 0:\n",
        "        print(\"\\n   Cluster sizes:\")\n",
        "        for i in range(n_clusters):\n",
        "            size = list(cluster_labels).count(i)\n",
        "            print(f\"   Cluster {i}: {size} extractions\")\n",
        "\n",
        "    return cluster_labels, clusterer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC9OM_8Uown3"
      },
      "source": [
        "## Summarising Clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "AjjMMfkaQ4GO"
      },
      "outputs": [],
      "source": [
        "def summarize_cluster_extractions(cluster_extractions, property_name, property_description, cluster_id, model=\"openai/gpt-5\"):\n",
        "    \"\"\"\n",
        "    Generate a short summary label for a cluster based on its extractions.\n",
        "\n",
        "    Args:\n",
        "        cluster_extractions: List of extraction dictionaries in the cluster\n",
        "        property_name: Name of the property being summarized\n",
        "        property_description: Description of the property being summarized\n",
        "        cluster_id: Cluster identifier\n",
        "        model: LLM model to use\n",
        "\n",
        "    Returns:\n",
        "        Summary string (3-4 words)\n",
        "    \"\"\"\n",
        "    # Sample up to 10 extractions for the summary\n",
        "    sample_size = min(10, len(cluster_extractions))\n",
        "    sampled_extractions = cluster_extractions[:sample_size]\n",
        "\n",
        "    # Create extraction list for prompt\n",
        "    extraction_list = \"\\n\".join([f\"{i+1}. {ext_cont['extracted_content']}\" for i, ext_cont in enumerate(sampled_extractions)])\n",
        "    print(extraction_list)\n",
        "\n",
        "    prompt = f\"\"\"You are analyzing clusters of {property_name} from research papers.\n",
        "\n",
        "    {property_name} is defined as:\n",
        "    {property_description}\n",
        "\n",
        "    Review these {len(sampled_extractions)} extractions from Cluster {cluster_id} and provide a SHORT label (4-5 words maximum) that captures their common theme or phenomenon.\n",
        "\n",
        "    You should also output a brief description of the label covering any specific details showcasing what the label encapsulates.\n",
        "\n",
        "    Your final output should only include:\n",
        "\n",
        "    Label:\n",
        "    Description:\n",
        "\n",
        "    Output only the cluster label and description, nothing else. Keep them as specific as possible.\n",
        "\n",
        "    Extractions:\n",
        "    {extraction_list}\n",
        "    \"\"\"\n",
        "\n",
        "    response = send_ai_request(prompt, model=model)\n",
        "    print(response)\n",
        "    parsed = parse_ai_response(response)\n",
        "\n",
        "    if parsed['success']:\n",
        "        # Clean up the response - just get the label\n",
        "        label = parsed['content'].strip()\n",
        "        # # Remove quotes if present\n",
        "        # label = label.strip('\"\\'')\n",
        "        # # Take first 4 words if longer\n",
        "        # words = label.split()\n",
        "        # if len(words) > 4:\n",
        "        #     label = \" \".join(words[:4])\n",
        "        return label\n",
        "    else:\n",
        "        return f\"Cluster {cluster_id}\"\n",
        "\n",
        "    cluster_summaries = {}\n",
        "    print(\"Generating cluster summaries...\\n\")\n",
        "\n",
        "    # Determine max cluster ID\n",
        "    max_cluster_id = int(cluster_labels.max()) if len(cluster_labels) > 0 else -1\n",
        "\n",
        "    for cluster_id in range(max_cluster_id + 1):  # Loop through all clusters\n",
        "        cluster_extractions = [ext for i, ext in enumerate(extracted_content_for_embedding) if cluster_labels[i] == cluster_id]\n",
        "\n",
        "        if len(cluster_extractions) > 0:\n",
        "            summary = summarize_cluster_extractions(cluster_extractions, property_name, property_description, cluster_id)\n",
        "            cluster_summaries[cluster_id] = summary\n",
        "            print(f\"Cluster {cluster_id}: {summary} ({len(cluster_extractions)} extractions)\")\n",
        "\n",
        "    print(\"\\n‚úÖ Cluster summaries generated!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irHB2o4zpIOw"
      },
      "source": [
        "## UMAP Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "Ow4O3709bgS8"
      },
      "outputs": [],
      "source": [
        "def create_visualization_data(embeddings, cluster_labels, extracted_content_list,\n",
        "                              umap_neighbors=15, umap_min_dist=0.1,\n",
        "                              random_state=42, save_path=None):\n",
        "    \"\"\"\n",
        "    Create 2D visualization data from embeddings using existing cluster labels and UMAP.\n",
        "\n",
        "    Args:\n",
        "        embeddings: numpy array of high-dimensional embeddings (n_samples, n_features)\n",
        "        cluster_labels: Cluster assignments from cluster_extractions() (n_samples,)\n",
        "        extracted_content_list: List of dictionaries with extracted content (will be modified in-place)\n",
        "        umap_neighbors: Number of neighbors for UMAP (default: 15)\n",
        "        umap_min_dist: Minimum distance for UMAP (default: 0.1)\n",
        "        random_state: Random seed for reproducibility (default: 42)\n",
        "        save_path: Optional path to save visualization data as pickle. If None, don't save.\n",
        "\n",
        "    Returns:\n",
        "        dict with keys:\n",
        "            - 'embeddings_2d': 2D UMAP projections (n_samples, 2)\n",
        "            - 'cluster_labels': The input cluster_labels (unchanged)\n",
        "            - 'extractions': The input extracted_content_list (with cluster labels added)\n",
        "    \"\"\"\n",
        "    import umap\n",
        "    import numpy as np\n",
        "\n",
        "\n",
        "    # Step 2: UMAP dimensionality reduction for visualization\n",
        "    print(\"Projecting to 2D with UMAP...\")\n",
        "    reducer = umap.UMAP(\n",
        "        n_neighbors=umap_neighbors,\n",
        "        min_dist=umap_min_dist,\n",
        "        metric='cosine',\n",
        "        random_state=random_state\n",
        "    )\n",
        "    embeddings_2d = reducer.fit_transform(embeddings)\n",
        "\n",
        "    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
        "    print(f\"‚úÖ UMAP complete! Shape: {embeddings_2d.shape}\")\n",
        "    print(f\"   Visualizing {n_clusters} clusters\")\n",
        "\n",
        "    # Step 3: Prepare return data\n",
        "    visualization_data = {\n",
        "        'embeddings_2d': embeddings_2d,\n",
        "        'cluster_labels': cluster_labels,\n",
        "        'extractions': extracted_content_list\n",
        "    }\n",
        "\n",
        "    # Step 4: Optional save\n",
        "    if save_path:\n",
        "        import pickle\n",
        "        with open(save_path, 'wb') as f:\n",
        "            pickle.dump(visualization_data, f)\n",
        "        print(f\"‚úÖ Saved visualization data to {save_path}\")\n",
        "\n",
        "    return visualization_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "yC2xaZQQbgS9"
      },
      "outputs": [],
      "source": [
        "def visualize_clusters_with_summaries(embeddings_2d, cluster_labels,\n",
        "                                      extracted_content_list, cluster_summaries,\n",
        "                                      property_name=\"Extractions\", title_suffix=\"\",\n",
        "                                      width=1200, height=800, show=True):\n",
        "    \"\"\"\n",
        "    Create enhanced scatter plot with cluster summary labels as annotations.\n",
        "\n",
        "    Args:\n",
        "        embeddings_2d: 2D UMAP projections (n_samples, 2)\n",
        "        cluster_labels: Cluster assignments (n_samples,)\n",
        "        extracted_content_list: List of extraction dicts with 'extracted_content', 'paper_title', 'has_citations'\n",
        "        cluster_summaries: Dict mapping cluster_id to summary string\n",
        "        property_name: Name of property being visualized (for title)\n",
        "        title_suffix: Optional suffix for title\n",
        "        width: Plot width in pixels\n",
        "        height: Plot height in pixels\n",
        "        show: Whether to display the plot immediately\n",
        "\n",
        "    Returns:\n",
        "        plotly.graph_objects.Figure object\n",
        "    \"\"\"\n",
        "    import plotly.graph_objects as go\n",
        "    import plotly.express as px\n",
        "    import numpy as np\n",
        "\n",
        "    # Determine number of clusters dynamically (exclude noise points with -1)\n",
        "    unique_clusters = sorted([c for c in set(cluster_labels) if c != -1])\n",
        "    n_clusters = len(unique_clusters)\n",
        "\n",
        "    # Generate color palette dynamically\n",
        "    colors = px.colors.qualitative.Set3[:n_clusters] if n_clusters <= 12 else px.colors.qualitative.Set3\n",
        "\n",
        "    # Create the scatter plot\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Add points for each cluster with summary labels\n",
        "    for idx, cluster_id in enumerate(unique_clusters):\n",
        "        mask = cluster_labels == cluster_id\n",
        "        cluster_points = embeddings_2d[mask]\n",
        "\n",
        "        # Get cluster extractions\n",
        "        cluster_extractions = [ext for i, ext in enumerate(extracted_content_list) if cluster_labels[i] == cluster_id]\n",
        "\n",
        "        # Create hover text\n",
        "        hover_texts = []\n",
        "        for ext in cluster_extractions:\n",
        "            hover_text = f\"<b>{ext['paper_url']}...</b><br><br>{ext['extracted_content'][:200]}...<br><br>Citations: {ext['has_citations']}\"\n",
        "            hover_texts.append(hover_text)\n",
        "\n",
        "        # Use summary as legend label if available, otherwise use cluster ID\n",
        "        if cluster_id in cluster_summaries:\n",
        "            cluster_name = f\"{cluster_summaries[cluster_id]} (n={len(cluster_extractions)})\"\n",
        "        else:\n",
        "            cluster_name = f\"Cluster {cluster_id} (n={len(cluster_extractions)})\"\n",
        "\n",
        "        # Use color from palette (cycle if more clusters than colors)\n",
        "        color = colors[idx % len(colors)]\n",
        "\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=cluster_points[:, 0],\n",
        "            y=cluster_points[:, 1],\n",
        "            mode='markers',\n",
        "            name=cluster_name,\n",
        "            marker=dict(\n",
        "                size=12,\n",
        "                color=color,\n",
        "                line=dict(width=1, color='white')\n",
        "            ),\n",
        "            text=hover_texts,\n",
        "            hoverinfo='text'\n",
        "        ))\n",
        "\n",
        "    # Add cluster labels as annotations (only for clusters with summaries)\n",
        "    for cluster_id in unique_clusters:\n",
        "        if cluster_id in cluster_summaries:\n",
        "            mask = cluster_labels == cluster_id\n",
        "            if mask.any():\n",
        "                cluster_points = embeddings_2d[mask]\n",
        "                # Place label at cluster centroid\n",
        "                center_x = cluster_points[:, 0].mean()\n",
        "                center_y = cluster_points[:, 1].mean()\n",
        "\n",
        "                # Get color for this cluster\n",
        "                cluster_idx = unique_clusters.index(cluster_id)\n",
        "                color = colors[cluster_idx % len(colors)]\n",
        "\n",
        "                fig.add_annotation(\n",
        "                    x=center_x,\n",
        "                    y=center_y,\n",
        "                    text=cluster_summaries[cluster_id],\n",
        "                    showarrow=False,\n",
        "                    font=dict(size=14, color='black'),\n",
        "                    bgcolor='rgba(255,255,255,0.8)',\n",
        "                    bordercolor=color,\n",
        "                    borderwidth=2\n",
        "                )\n",
        "\n",
        "    # Build title\n",
        "    title = f\"{property_name} by Cluster Type\"\n",
        "    if title_suffix:\n",
        "        title += f\": {title_suffix}\"\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        xaxis_title=\"UMAP 1\",\n",
        "        yaxis_title=\"UMAP 2\",\n",
        "        width=width,\n",
        "        height=height,\n",
        "        hovermode='closest',\n",
        "        font=dict(size=12)\n",
        "    )\n",
        "\n",
        "    if show:\n",
        "        fig.show()\n",
        "\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "RRBzI1gsbgS_"
      },
      "outputs": [],
      "source": [
        "def visualize_clusters(embeddings_2d, cluster_labels, extracted_content_list,\n",
        "                      property_name=\"Extractions\", title_suffix=\"\",\n",
        "                      width=1000, height=700, show=True):\n",
        "    \"\"\"\n",
        "    Create a basic scatter plot visualization of clusters.\n",
        "\n",
        "    Args:\n",
        "        embeddings_2d: 2D UMAP projections (n_samples, 2)\n",
        "        cluster_labels: Cluster assignments (n_samples,)\n",
        "        extracted_content_list: List of extraction dicts with 'extracted_content', 'paper_title', 'has_citations'\n",
        "        property_name: Name of property being visualized (for title)\n",
        "        title_suffix: Optional suffix for title\n",
        "        width: Plot width in pixels\n",
        "        height: Plot height in pixels\n",
        "        show: Whether to display the plot immediately\n",
        "\n",
        "    Returns:\n",
        "        plotly.graph_objects.Figure object\n",
        "    \"\"\"\n",
        "    import plotly.graph_objects as go\n",
        "    import plotly.express as px\n",
        "    import numpy as np\n",
        "\n",
        "    # Determine number of clusters dynamically (exclude noise points with -1)\n",
        "    unique_clusters = sorted([c for c in set(cluster_labels) if c != -1])\n",
        "    n_clusters = len(unique_clusters)\n",
        "\n",
        "    # Generate color palette dynamically\n",
        "    colors = px.colors.qualitative.Set3[:n_clusters] if n_clusters <= 12 else px.colors.qualitative.Set3\n",
        "\n",
        "    # Create the scatter plot\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Add points for each cluster\n",
        "    for idx, cluster_id in enumerate(unique_clusters):\n",
        "        mask = cluster_labels == cluster_id\n",
        "        cluster_points = embeddings_2d[mask]\n",
        "\n",
        "        # Get cluster extractions\n",
        "        cluster_extractions = [ext for i, ext in enumerate(extracted_content_list) if cluster_labels[i] == cluster_id]\n",
        "\n",
        "        # Create hover text\n",
        "        hover_texts = []\n",
        "        for ext in cluster_extractions:\n",
        "            hover_text = f\"<b>{ext['paper_title'][:50]}...</b><br><br>{ext['extracted_content'][:200]}...<br><br>Citations: {ext['has_citations']}\"\n",
        "            hover_texts.append(hover_text)\n",
        "\n",
        "        # Use color from palette (cycle if more clusters than colors)\n",
        "        color = colors[idx % len(colors)]\n",
        "\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=cluster_points[:, 0],\n",
        "            y=cluster_points[:, 1],\n",
        "            mode='markers',\n",
        "            name=f'Cluster {cluster_id}',\n",
        "            marker=dict(\n",
        "                size=10,\n",
        "                color=color,\n",
        "                line=dict(width=1, color='white')\n",
        "            ),\n",
        "            text=hover_texts,\n",
        "            hoverinfo='text'\n",
        "        ))\n",
        "\n",
        "    # Build title\n",
        "    title = f\"{property_name} Cluster Map\"\n",
        "    if title_suffix:\n",
        "        title += f\": {title_suffix}\"\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        xaxis_title=\"UMAP 1\",\n",
        "        yaxis_title=\"UMAP 2\",\n",
        "        width=width,\n",
        "        height=height,\n",
        "        hovermode='closest'\n",
        "    )\n",
        "\n",
        "    if show:\n",
        "        fig.show()\n",
        "\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Code"
      ],
      "metadata": {
        "id": "xZ4gOCOO7GRv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "6Vpkyq_rbgSx"
      },
      "outputs": [],
      "source": [
        "property_dictionary = {\n",
        "    \"empirical_observations\" : {\n",
        "        \"title\" : \"Empirical Observations\",\n",
        "        \"description\" : '''An empirical observation is:\n",
        "                            - Information gathered through direct or indirect observation/experimentation\n",
        "                            - Evidence that could confirm, disconfirm, or arbitrate between scientific hypotheses\n",
        "                            - Includes: measured results, experimental findings, observed phenomena, detected patterns, or statistical outcomes\n",
        "                            - Can be from this paper's work OR cited from other studies (both are scientifically valuable)\"'''\n",
        "    },\n",
        "    \"observational_statements\" : {\n",
        "        \"title\" : \"Observational Statements\",\n",
        "        \"description\" : '''An observation statement is a sentence that:\n",
        "                        Is about publicly observable, intersubjective features of the world.\n",
        "                        Uses only: observational terms, logical terms (and, or, not, etc.),and math.\n",
        "                        Is directly testable by suitable observation by (in principle) any competent observer.'''\n",
        "    },\n",
        "    \"theoretical_statements\" : {\n",
        "        \"title\" : \"Theoretical Statements\",\n",
        "        \"description\" : '''A theoretical statement is a sentence that:\n",
        "                          Contains at least one theoretical term‚Äîi.e. terms introduced by a scientific theory, typically referring to unobservable or highly theory-laden entities/structures.\n",
        "                          Gets its empirical bite only indirectly: via laws, correspondence rules, bridge principles, models that connect it to observation statements.'''\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking List of URLs as Input\n",
        "url_input = input(\"Enter a list of arxiv pdf urls separated with a comma.\")\n",
        "\n",
        "# Making List Object\n",
        "paper_list_for_download = prepare_urls_for_download(url_input)\n",
        "print(f\"Length of paper list for download: {len(paper_list_for_download)}\")\n",
        "\n",
        "# Taking Download Directory Name\n",
        "download_directory_name = input(\"Enter the name of the directory you want to download to.\")\n",
        "print(f\"Downloading to directory: {download_directory_name}\")\n",
        "\n",
        "# Downloading Each URL Content\n",
        "for item in paper_list_for_download:\n",
        "  print(f\"Downloading {item['paper_title']}...\")\n",
        "  file_path = download_pdf(pdf_url=item[\"pdf_url\"], paper_title=item[\"paper_title\"], download_dir=download_directory_name)\n",
        "  if file_path:\n",
        "    print(f\"Downloaded {item['paper_title']} to {file_path}\")\n",
        "\n",
        "    # If Downloaded, Extracting Text PDF to MD\n",
        "    download_text = extract_pdf_text_md(file_path, item[\"paper_title\"], download=False)\n",
        "    if download_text:\n",
        "      print(f\"Extracted text for {item['paper_title']}\")\n",
        "      print(f\"Caching the text data.\")\n",
        "\n",
        "      # If Extracted, Saving to Cache\n",
        "      status, text_data = cache_paper_text(paper_id=item['paper_title'], paper_title=item['paper_title'], text_data=download_text, cache_dir=download_directory_name)\n",
        "      if status:\n",
        "        print(f\"Cached text for {item['paper_title']}\")\n",
        "    time.sleep(1)\n",
        "  else:\n",
        "    print(f\"Download failed for {item['paper_title']}\")\n",
        "    time.sleep(1)\n",
        "\n",
        "\n",
        "# Chunking MD Text\n",
        "for item in paper_list_for_download:\n",
        "  print(f\"Loading cached paper text for {item['paper_title']} on {item['pdf_url']}.\")\n",
        "  status, text_data = load_cached_paper_text(paper_id=item['paper_title'], cache_dir=download_directory_name)\n",
        "  if status:\n",
        "    chunks = chunk_by_paragraph(text_data)\n",
        "    item['chunks'] = chunks\n",
        "  else:\n",
        "    print(f\"‚ùå No cached text found for paper ID: {item['paper_title']}\")\n",
        "    item['chunks'] = []\n",
        "\n",
        "\n",
        "# Labelling Each Chunk\n",
        "for item in paper_list_for_download:\n",
        "  status = label_all_chunks_per_paper(item, \"empirical_observations\", property_dictionary['empirical_observations']['description'])\n",
        "  if status:\n",
        "    print(f\"‚úÖ Paper {item['paper_title']} labelled.\")\n",
        "\n",
        "\n",
        "# Checking Labelling Output\n",
        "for item in paper_list_for_download:\n",
        "  print(item.keys())\n",
        "  print(item['labeled_chunks_all'][0].keys())\n",
        "  print(item['labeled_chunks_all'][0]['label_output'])\n",
        "\n",
        "# Preparing Output for Embedding\n",
        "content_for_embedding = prepare_chunks_for_embedding(paper_list_for_download, property_label=\"empirical_observations\", labeled_chunks_key='labeled_chunks_all')\n",
        "print(len(content_for_embedding))\n",
        "print(type(content_for_embedding[0]))\n",
        "print(content_for_embedding[0].keys())\n",
        "\n",
        "# Checking Content for Embedding\n",
        "for item in content_for_embedding:\n",
        "  print(item['paper_url'])\n",
        "  print(item['extracted_content'])\n",
        "\n",
        "# Creating Embeddings\n",
        "embeddings = create_embeddings(content_for_embedding, model_name='allenai/scibert_scivocab_uncased', save_path='label_all_embeddings')\n",
        "\n",
        "# First, let's check what we're working with\n",
        "print(\"Debug info:\")\n",
        "print(f\"Embeddings shape: {embeddings.shape}\")\n",
        "print(f\"Embedding dimensions: {embeddings.shape[1]}\")\n",
        "\n",
        "# Let's look at the similarity distribution\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Calculate similarities\n",
        "similarities = cosine_similarity(embeddings)\n",
        "# Get upper triangle (excluding diagonal)\n",
        "upper_tri = np.triu(similarities, k=1)\n",
        "flat_sims = upper_tri[upper_tri > 0]\n",
        "\n",
        "print(f\"\\nSimilarity stats:\")\n",
        "print(f\"Mean similarity: {flat_sims.mean():.3f}\")\n",
        "print(f\"Std similarity: {flat_sims.std():.3f}\")\n",
        "print(f\"Min similarity: {flat_sims.min():.3f}\")\n",
        "print(f\"Max similarity: {flat_sims.max():.3f}\")\n",
        "\n",
        "# Try simpler clustering - KMeans first to see if there ARE patterns\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Try different k values\n",
        "for k in [3, 4, 5, 8, 6]:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans_labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "    print(f\"\\nKMeans with k={k}:\")\n",
        "    for i in range(k):\n",
        "        count = list(kmeans_labels).count(i)\n",
        "        print(f\"  Cluster {i}: {count} observations\")\n",
        "\n",
        "\n",
        "# Doing final clustering with k=4\n",
        "k=4\n",
        "k_means_clustering_labels, clusterer = cluster_extractions_kmeans(embeddings, k=k)\n",
        "\n",
        "for i in range(len(content_for_embedding)):\n",
        "    content_for_embedding[i]['cluster_id'] = k_means_clustering_labels[i]\n",
        "\n",
        "# Generating cluster summary\n",
        "\n",
        "cluster_summaries = {}\n",
        "for cluster_id in range(k):\n",
        "      # Get all extractions for this cluster\n",
        "      cluster_extractions = [ext for ext in content_for_embedding if ext['cluster_id'] == cluster_id]\n",
        "\n",
        "      if len(cluster_extractions) > 0:\n",
        "          summary = summarize_cluster_extractions(\n",
        "              cluster_extractions,\n",
        "              property_name=\"empirical_observations\",\n",
        "              property_description=property_dictionary['empirical_observations']['description'],\n",
        "              cluster_id=cluster_id\n",
        "          )\n",
        "          cluster_summaries[cluster_id] = summary\n",
        "          print(f\"Cluster {cluster_id}: {summary} ({len(cluster_extractions)} extractions)\")\n",
        "\n",
        "      print(\"\\n‚úÖ Cluster summaries generated!\")\n",
        "      print(\"\\nFinal summaries:\", cluster_summaries)\n",
        "\n",
        "\n",
        "# Generating 2d visualisation data using UMAP\n",
        "\n",
        "visualisation_data = create_visualization_data(embeddings, k_means_clustering_labels, content_for_embedding)\n",
        "\n",
        "# Generating visualisation with summary\n",
        "\n",
        "visualize_clusters_with_summaries(visualisation_data['embeddings_2d'], visualisation_data['cluster_labels'], visualisation_data['extractions'], cluster_summaries=cluster_summaries, property_name=\"Empirical Observations\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "cec026e7ae804e49b9de3ec0b259a499",
            "3347e5b54efe4871915b0dca3d13133f",
            "b033aafdddf44012af1aeef196bfc748",
            "b5df3d46a0d54059b4ccf16c1027b45e",
            "c053f97cfa574181830ce73d925a3281",
            "1b4673833f3b47c5b91f6ca7d7b1a105",
            "a1abfae7e2464509868a0904a2d0107c",
            "8b83632db8bc48c5b151873b13113da6",
            "ad6e469fbc34466498a611c4376490fd",
            "b49045c4ca524b4da8aff291ec4db2a1",
            "095a7c77e1a1438981177879650b5fee",
            "393ab0f70c474a7abc15ef2faa2050db",
            "4c88c424e6764c269431877033dd7cf5",
            "4660fa80bdb94166a842fd1518ca7161",
            "aa550f5c82d04e8288b59e5eebc2fe5e",
            "2e1fcebf6e144cd3b2d7d94c2a07e12d",
            "62be1755e416413d93a4a3feb35749e1",
            "a06a0385e0714c8e92a0f4cf8e9688d9",
            "7df272ebcde942928d6c92deb626c528",
            "f8b446a74d474622b5ee98e3bbeb6a93",
            "8d05a01ca10f4802bdca6855e73e712c",
            "adc1401c38e34a649ad31faf90080b80",
            "93f85936f9a8475b9f6f11a9b2f12ac4",
            "e920c236d48a4bc39e566d0d2d6d67bf",
            "9dca721589ad4d869ad971b8b38aa1fd",
            "6124f717bee345dc8808e1083d2ee518",
            "d49da52b0b3142cc9b0203c215b6710f",
            "ffbab51695c54d6e83229c6bf0968764",
            "83781d3ae9804c7c9c5b0b5b65968fcb",
            "709bfcff404c4323892ec4eeb77506bf",
            "ebd3879d5ba244c8911347c93bba0193",
            "10a9316d5d3b4ba7a5a161f624c0befa",
            "09fd6104d3c740919460f5cb88f2fca5",
            "490a5678dd4c41ca8b8d7b4b6fda1562",
            "afa14ab48cfa4c4cb3e294b6981a87a0",
            "54bb90079d1f42739b1c34da424d04f3",
            "017d666c95544e3cbe1e14ba590c684f",
            "13e251cc88ff4a38a28231926dbeaa4a",
            "53dd6eb8cc67421fb94538f193bd6bf7",
            "82f56a83a772493c88e7f3ee2d40db1e",
            "2ce589b0184c4029a00b525c491d00f1",
            "3808e7a798394a31a59291a4a5a8ba2a",
            "9d09c68324354afebd38954ffa7e8bea",
            "4330ec8c6b4c4bee903805241d7466ef",
            "1b991fc1889645a7b2e9eaf4e5cec163",
            "2212cf6e40764f03b34373ff346584bd",
            "35a3be50cfa54842bf6374b0fc769236",
            "766ee24808cd42eda8fa0695f57ff0c5",
            "a6306d9e41e24be59ad9e4dffc094594",
            "8b8aa50e003446ab9d7a325990074055",
            "4d5221209103404f89549a3e3906e088",
            "3c2e70b24ad74f6e9aa6e747fd5bbe96",
            "90874638ee3e486389ee68d53d57346f",
            "90ae052fb21441279b15bdf9db2d4e29",
            "424eecb711b249ab904d78f8652cdeec",
            "e1c9846b2b2d45aab66653674c0421ef",
            "3679742eea1442428647ac650f2e565d",
            "2c3ad49515794dd5b8a514a35fd0ad9b",
            "37befc77db40419ab5f73b4f40d20737",
            "46773632372b47b990d327f6a96abf69",
            "3f07f7d0de1e46e0aeb59b925b7ee9f3",
            "4d9803d64aa94efe8a80e48ea53245e8",
            "d16e7be449e7422682d96d1ee628517e",
            "93388392a8c8474782610305f46b1482",
            "68614738649b4da9bf5915c21a7016f5",
            "09a2a5887b8444acab90dfba4b25d67c"
          ]
        },
        "id": "p07unVgp7HX7",
        "outputId": "f22150a4-6590-44f4-9444-d928427b01ff"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a list of arxiv pdf urls separated with a comma.https://arxiv.org/pdf/2510.09901, https://arxiv.org/pdf/2410.07076, https://arxiv.org/pdf/2307.10635, https://arxiv.org/pdf/2505.04651, https://arxiv.org/pdf/2505.04651, https://arxiv.org/pdf/2503.24047\n",
            "Length of paper list for download: 6\n",
            "Enter the name of the directory you want to download to.Hypothesis_generator_test\n",
            "Downloading to directory: Hypothesis_generator_test\n",
            "Downloading paper_1...\n",
            "Downloaded paper_1 to Hypothesis_generator_test/paper_1.pdf\n",
            "Checking for pdf path before markdown extraction.\n",
            "Getting PDF directory name.\n",
            "Getting markdown from paper_1.\n",
            "Extracted text for paper_1\n",
            "Caching the text data.\n",
            "‚úÖ Cached: paper_1...\n",
            "Cached text for paper_1\n",
            "Downloading paper_2...\n",
            "Downloaded paper_2 to Hypothesis_generator_test/paper_2.pdf\n",
            "Checking for pdf path before markdown extraction.\n",
            "Getting PDF directory name.\n",
            "Getting markdown from paper_2.\n",
            "Extracted text for paper_2\n",
            "Caching the text data.\n",
            "‚úÖ Cached: paper_2...\n",
            "Cached text for paper_2\n",
            "Downloading paper_3...\n",
            "Downloaded paper_3 to Hypothesis_generator_test/paper_3.pdf\n",
            "Checking for pdf path before markdown extraction.\n",
            "Getting PDF directory name.\n",
            "Getting markdown from paper_3.\n",
            "Extracted text for paper_3\n",
            "Caching the text data.\n",
            "‚úÖ Cached: paper_3...\n",
            "Cached text for paper_3\n",
            "Downloading paper_4...\n",
            "Downloaded paper_4 to Hypothesis_generator_test/paper_4.pdf\n",
            "Checking for pdf path before markdown extraction.\n",
            "Getting PDF directory name.\n",
            "Getting markdown from paper_4.\n",
            "Extracted text for paper_4\n",
            "Caching the text data.\n",
            "‚úÖ Cached: paper_4...\n",
            "Cached text for paper_4\n",
            "Downloading paper_5...\n",
            "Downloaded paper_5 to Hypothesis_generator_test/paper_5.pdf\n",
            "Checking for pdf path before markdown extraction.\n",
            "Getting PDF directory name.\n",
            "Getting markdown from paper_5.\n",
            "Extracted text for paper_5\n",
            "Caching the text data.\n",
            "‚úÖ Cached: paper_5...\n",
            "Cached text for paper_5\n",
            "Downloading paper_6...\n",
            "Downloaded paper_6 to Hypothesis_generator_test/paper_6.pdf\n",
            "Checking for pdf path before markdown extraction.\n",
            "Getting PDF directory name.\n",
            "Getting markdown from paper_6.\n",
            "Extracted text for paper_6\n",
            "Caching the text data.\n",
            "‚úÖ Cached: paper_6...\n",
            "Cached text for paper_6\n",
            "Loading cached paper text for paper_1 on https://arxiv.org/pdf/2510.09901.\n",
            "‚úÖ Loaded cached text for paper ID: paper_1\n",
            "Loading cached paper text for paper_2 on https://arxiv.org/pdf/2410.07076.\n",
            "‚úÖ Loaded cached text for paper ID: paper_2\n",
            "Loading cached paper text for paper_3 on https://arxiv.org/pdf/2307.10635.\n",
            "‚úÖ Loaded cached text for paper ID: paper_3\n",
            "Loading cached paper text for paper_4 on https://arxiv.org/pdf/2505.04651.\n",
            "‚úÖ Loaded cached text for paper ID: paper_4\n",
            "Loading cached paper text for paper_5 on https://arxiv.org/pdf/2505.04651.\n",
            "‚úÖ Loaded cached text for paper ID: paper_5\n",
            "Loading cached paper text for paper_6 on https://arxiv.org/pdf/2503.24047.\n",
            "‚úÖ Loaded cached text for paper ID: paper_6\n",
            "Labelling chunks for paper: paper_1, https://arxiv.org/pdf/2510.09901\n",
            "Total chunks: 36\n",
            "============================================================\n",
            "\n",
            "Starting labelling for: 36\n",
            "Labelling chunk 1/36\n",
            "AI request for chunk 1/36 completed.\n",
            "AI response for chunk 1/36 parsed.\n",
            "AI request for chunk 1/36 appended.\n",
            "Labelling chunk 2/36\n",
            "AI request for chunk 2/36 completed.\n",
            "AI response for chunk 2/36 parsed.\n",
            "AI request for chunk 2/36 appended.\n",
            "Labelling chunk 3/36\n",
            "AI request for chunk 3/36 completed.\n",
            "AI response for chunk 3/36 parsed.\n",
            "AI request for chunk 3/36 appended.\n",
            "Labelling chunk 4/36\n",
            "AI request for chunk 4/36 completed.\n",
            "AI response for chunk 4/36 parsed.\n",
            "AI request for chunk 4/36 appended.\n",
            "Labelling chunk 5/36\n",
            "AI request for chunk 5/36 completed.\n",
            "AI response for chunk 5/36 parsed.\n",
            "AI request for chunk 5/36 appended.\n",
            "Labelling chunk 6/36\n",
            "AI request for chunk 6/36 completed.\n",
            "AI response for chunk 6/36 parsed.\n",
            "AI request for chunk 6/36 appended.\n",
            "Labelling chunk 7/36\n",
            "AI request for chunk 7/36 completed.\n",
            "AI response for chunk 7/36 parsed.\n",
            "AI request for chunk 7/36 appended.\n",
            "Labelling chunk 8/36\n",
            "AI request for chunk 8/36 completed.\n",
            "AI response for chunk 8/36 parsed.\n",
            "AI request for chunk 8/36 appended.\n",
            "Labelling chunk 9/36\n",
            "AI request for chunk 9/36 completed.\n",
            "AI response for chunk 9/36 parsed.\n",
            "AI request for chunk 9/36 appended.\n",
            "Labelling chunk 10/36\n",
            "AI request for chunk 10/36 completed.\n",
            "AI response for chunk 10/36 parsed.\n",
            "AI request for chunk 10/36 appended.\n",
            "Labelling chunk 11/36\n",
            "AI request for chunk 11/36 completed.\n",
            "AI response for chunk 11/36 parsed.\n",
            "AI request for chunk 11/36 appended.\n",
            "Labelling chunk 12/36\n",
            "AI request for chunk 12/36 completed.\n",
            "AI response for chunk 12/36 parsed.\n",
            "AI request for chunk 12/36 appended.\n",
            "Labelling chunk 13/36\n",
            "AI request for chunk 13/36 completed.\n",
            "AI response for chunk 13/36 parsed.\n",
            "AI request for chunk 13/36 appended.\n",
            "Labelling chunk 14/36\n",
            "AI request for chunk 14/36 completed.\n",
            "AI response for chunk 14/36 parsed.\n",
            "AI request for chunk 14/36 appended.\n",
            "Labelling chunk 15/36\n",
            "AI request for chunk 15/36 completed.\n",
            "AI response for chunk 15/36 parsed.\n",
            "AI request for chunk 15/36 appended.\n",
            "Labelling chunk 16/36\n",
            "AI request for chunk 16/36 completed.\n",
            "AI response for chunk 16/36 parsed.\n",
            "AI request for chunk 16/36 appended.\n",
            "Labelling chunk 17/36\n",
            "AI request for chunk 17/36 completed.\n",
            "AI response for chunk 17/36 parsed.\n",
            "AI request for chunk 17/36 appended.\n",
            "Labelling chunk 18/36\n",
            "AI request for chunk 18/36 completed.\n",
            "AI response for chunk 18/36 parsed.\n",
            "AI request for chunk 18/36 appended.\n",
            "Labelling chunk 19/36\n",
            "AI request for chunk 19/36 completed.\n",
            "AI response for chunk 19/36 parsed.\n",
            "AI request for chunk 19/36 appended.\n",
            "Labelling chunk 20/36\n",
            "AI request for chunk 20/36 completed.\n",
            "AI response for chunk 20/36 parsed.\n",
            "AI request for chunk 20/36 appended.\n",
            "Labelling chunk 21/36\n",
            "AI request for chunk 21/36 completed.\n",
            "AI response for chunk 21/36 parsed.\n",
            "AI request for chunk 21/36 appended.\n",
            "Labelling chunk 22/36\n",
            "AI request for chunk 22/36 completed.\n",
            "AI response for chunk 22/36 parsed.\n",
            "AI request for chunk 22/36 appended.\n",
            "Labelling chunk 23/36\n",
            "AI request for chunk 23/36 completed.\n",
            "AI response for chunk 23/36 parsed.\n",
            "AI request for chunk 23/36 appended.\n",
            "Labelling chunk 24/36\n",
            "AI request for chunk 24/36 completed.\n",
            "AI response for chunk 24/36 parsed.\n",
            "AI request for chunk 24/36 appended.\n",
            "Labelling chunk 25/36\n",
            "AI request for chunk 25/36 completed.\n",
            "AI response for chunk 25/36 parsed.\n",
            "AI request for chunk 25/36 appended.\n",
            "Labelling chunk 26/36\n",
            "AI request for chunk 26/36 completed.\n",
            "AI response for chunk 26/36 parsed.\n",
            "AI request for chunk 26/36 appended.\n",
            "Labelling chunk 27/36\n",
            "AI request for chunk 27/36 completed.\n",
            "AI response for chunk 27/36 parsed.\n",
            "AI request for chunk 27/36 appended.\n",
            "Labelling chunk 28/36\n",
            "AI request for chunk 28/36 completed.\n",
            "AI response for chunk 28/36 parsed.\n",
            "AI request for chunk 28/36 appended.\n",
            "Labelling chunk 29/36\n",
            "AI request for chunk 29/36 completed.\n",
            "AI response for chunk 29/36 parsed.\n",
            "AI request for chunk 29/36 appended.\n",
            "Labelling chunk 30/36\n",
            "AI request for chunk 30/36 completed.\n",
            "AI response for chunk 30/36 parsed.\n",
            "AI request for chunk 30/36 appended.\n",
            "Labelling chunk 31/36\n",
            "AI request for chunk 31/36 completed.\n",
            "AI response for chunk 31/36 parsed.\n",
            "AI request for chunk 31/36 appended.\n",
            "Labelling chunk 32/36\n",
            "AI request for chunk 32/36 completed.\n",
            "AI response for chunk 32/36 parsed.\n",
            "AI request for chunk 32/36 appended.\n",
            "Labelling chunk 33/36\n",
            "AI request for chunk 33/36 completed.\n",
            "AI response for chunk 33/36 parsed.\n",
            "AI request for chunk 33/36 appended.\n",
            "Labelling chunk 34/36\n",
            "AI request for chunk 34/36 completed.\n",
            "AI response for chunk 34/36 parsed.\n",
            "AI request for chunk 34/36 appended.\n",
            "Labelling chunk 35/36\n",
            "AI request for chunk 35/36 completed.\n",
            "AI response for chunk 35/36 parsed.\n",
            "AI request for chunk 35/36 appended.\n",
            "Labelling chunk 36/36\n",
            "AI request for chunk 36/36 completed.\n",
            "AI response for chunk 36/36 parsed.\n",
            "AI request for chunk 36/36 appended.\n",
            "AI request for labelling None chunks complete. Appended.\n",
            "‚úÖ Paper paper_1 labelled.\n",
            "Labelling chunks for paper: paper_2, https://arxiv.org/pdf/2410.07076\n",
            "Total chunks: 13\n",
            "============================================================\n",
            "\n",
            "Starting labelling for: 13\n",
            "Labelling chunk 1/13\n",
            "AI request for chunk 1/13 completed.\n",
            "AI response for chunk 1/13 parsed.\n",
            "AI request for chunk 1/13 appended.\n",
            "Labelling chunk 2/13\n",
            "AI request for chunk 2/13 completed.\n",
            "AI response for chunk 2/13 parsed.\n",
            "AI request for chunk 2/13 appended.\n",
            "Labelling chunk 3/13\n",
            "AI request for chunk 3/13 completed.\n",
            "AI response for chunk 3/13 parsed.\n",
            "AI request for chunk 3/13 appended.\n",
            "Labelling chunk 4/13\n",
            "AI request for chunk 4/13 completed.\n",
            "AI response for chunk 4/13 parsed.\n",
            "AI request for chunk 4/13 appended.\n",
            "Labelling chunk 5/13\n",
            "AI request for chunk 5/13 completed.\n",
            "AI response for chunk 5/13 parsed.\n",
            "AI request for chunk 5/13 appended.\n",
            "Labelling chunk 6/13\n",
            "AI request for chunk 6/13 completed.\n",
            "AI response for chunk 6/13 parsed.\n",
            "AI request for chunk 6/13 appended.\n",
            "Labelling chunk 7/13\n",
            "AI request for chunk 7/13 completed.\n",
            "AI response for chunk 7/13 parsed.\n",
            "AI request for chunk 7/13 appended.\n",
            "Labelling chunk 8/13\n",
            "AI request for chunk 8/13 completed.\n",
            "AI response for chunk 8/13 parsed.\n",
            "AI request for chunk 8/13 appended.\n",
            "Labelling chunk 9/13\n",
            "AI request for chunk 9/13 completed.\n",
            "AI response for chunk 9/13 parsed.\n",
            "AI request for chunk 9/13 appended.\n",
            "Labelling chunk 10/13\n",
            "AI request for chunk 10/13 completed.\n",
            "AI response for chunk 10/13 parsed.\n",
            "AI request for chunk 10/13 appended.\n",
            "Labelling chunk 11/13\n",
            "AI request for chunk 11/13 completed.\n",
            "AI response for chunk 11/13 parsed.\n",
            "AI request for chunk 11/13 appended.\n",
            "Labelling chunk 12/13\n",
            "AI request for chunk 12/13 completed.\n",
            "AI response for chunk 12/13 parsed.\n",
            "AI request for chunk 12/13 appended.\n",
            "Labelling chunk 13/13\n",
            "AI request for chunk 13/13 completed.\n",
            "AI response for chunk 13/13 parsed.\n",
            "AI request for chunk 13/13 appended.\n",
            "AI request for labelling None chunks complete. Appended.\n",
            "‚úÖ Paper paper_2 labelled.\n",
            "Labelling chunks for paper: paper_3, https://arxiv.org/pdf/2307.10635\n",
            "Total chunks: 10\n",
            "============================================================\n",
            "\n",
            "Starting labelling for: 10\n",
            "Labelling chunk 1/10\n",
            "AI request for chunk 1/10 completed.\n",
            "AI response for chunk 1/10 parsed.\n",
            "AI request for chunk 1/10 appended.\n",
            "Labelling chunk 2/10\n",
            "AI request for chunk 2/10 completed.\n",
            "AI response for chunk 2/10 parsed.\n",
            "AI request for chunk 2/10 appended.\n",
            "Labelling chunk 3/10\n",
            "AI request for chunk 3/10 completed.\n",
            "AI response for chunk 3/10 parsed.\n",
            "AI request for chunk 3/10 appended.\n",
            "Labelling chunk 4/10\n",
            "AI request for chunk 4/10 completed.\n",
            "AI response for chunk 4/10 parsed.\n",
            "AI request for chunk 4/10 appended.\n",
            "Labelling chunk 5/10\n",
            "AI request for chunk 5/10 completed.\n",
            "AI response for chunk 5/10 parsed.\n",
            "AI request for chunk 5/10 appended.\n",
            "Labelling chunk 6/10\n",
            "AI request for chunk 6/10 completed.\n",
            "AI response for chunk 6/10 parsed.\n",
            "AI request for chunk 6/10 appended.\n",
            "Labelling chunk 7/10\n",
            "AI request for chunk 7/10 completed.\n",
            "AI response for chunk 7/10 parsed.\n",
            "AI request for chunk 7/10 appended.\n",
            "Labelling chunk 8/10\n",
            "AI request for chunk 8/10 completed.\n",
            "AI response for chunk 8/10 parsed.\n",
            "AI request for chunk 8/10 appended.\n",
            "Labelling chunk 9/10\n",
            "AI request for chunk 9/10 completed.\n",
            "AI response for chunk 9/10 parsed.\n",
            "AI request for chunk 9/10 appended.\n",
            "Labelling chunk 10/10\n",
            "AI request for chunk 10/10 completed.\n",
            "AI response for chunk 10/10 parsed.\n",
            "AI request for chunk 10/10 appended.\n",
            "AI request for labelling None chunks complete. Appended.\n",
            "‚úÖ Paper paper_3 labelled.\n",
            "Labelling chunks for paper: paper_4, https://arxiv.org/pdf/2505.04651\n",
            "Total chunks: 40\n",
            "============================================================\n",
            "\n",
            "Starting labelling for: 40\n",
            "Labelling chunk 1/40\n",
            "AI request for chunk 1/40 completed.\n",
            "AI response for chunk 1/40 parsed.\n",
            "AI request for chunk 1/40 appended.\n",
            "Labelling chunk 2/40\n",
            "AI request for chunk 2/40 completed.\n",
            "AI response for chunk 2/40 parsed.\n",
            "AI request for chunk 2/40 appended.\n",
            "Labelling chunk 3/40\n",
            "AI request for chunk 3/40 completed.\n",
            "AI response for chunk 3/40 parsed.\n",
            "AI request for chunk 3/40 appended.\n",
            "Labelling chunk 4/40\n",
            "AI request for chunk 4/40 completed.\n",
            "AI response for chunk 4/40 parsed.\n",
            "AI request for chunk 4/40 appended.\n",
            "Labelling chunk 5/40\n",
            "AI request for chunk 5/40 completed.\n",
            "AI response for chunk 5/40 parsed.\n",
            "AI request for chunk 5/40 appended.\n",
            "Labelling chunk 6/40\n",
            "AI request for chunk 6/40 completed.\n",
            "AI response for chunk 6/40 parsed.\n",
            "AI request for chunk 6/40 appended.\n",
            "Labelling chunk 7/40\n",
            "AI request for chunk 7/40 completed.\n",
            "AI response for chunk 7/40 parsed.\n",
            "AI request for chunk 7/40 appended.\n",
            "Labelling chunk 8/40\n",
            "AI request for chunk 8/40 completed.\n",
            "AI response for chunk 8/40 parsed.\n",
            "AI request for chunk 8/40 appended.\n",
            "Labelling chunk 9/40\n",
            "AI request for chunk 9/40 completed.\n",
            "AI response for chunk 9/40 parsed.\n",
            "AI request for chunk 9/40 appended.\n",
            "Labelling chunk 10/40\n",
            "AI request for chunk 10/40 completed.\n",
            "AI response for chunk 10/40 parsed.\n",
            "AI request for chunk 10/40 appended.\n",
            "Labelling chunk 11/40\n",
            "AI request for chunk 11/40 completed.\n",
            "AI response for chunk 11/40 parsed.\n",
            "AI request for chunk 11/40 appended.\n",
            "Labelling chunk 12/40\n",
            "AI request for chunk 12/40 completed.\n",
            "AI response for chunk 12/40 parsed.\n",
            "AI request for chunk 12/40 appended.\n",
            "Labelling chunk 13/40\n",
            "AI request for chunk 13/40 completed.\n",
            "AI response for chunk 13/40 parsed.\n",
            "AI request for chunk 13/40 appended.\n",
            "Labelling chunk 14/40\n",
            "AI request for chunk 14/40 completed.\n",
            "AI response for chunk 14/40 parsed.\n",
            "AI request for chunk 14/40 appended.\n",
            "Labelling chunk 15/40\n",
            "AI request for chunk 15/40 completed.\n",
            "AI response for chunk 15/40 parsed.\n",
            "AI request for chunk 15/40 appended.\n",
            "Labelling chunk 16/40\n",
            "AI request for chunk 16/40 completed.\n",
            "AI response for chunk 16/40 parsed.\n",
            "AI request for chunk 16/40 appended.\n",
            "Labelling chunk 17/40\n",
            "AI request for chunk 17/40 completed.\n",
            "AI response for chunk 17/40 parsed.\n",
            "AI request for chunk 17/40 appended.\n",
            "Labelling chunk 18/40\n",
            "AI request for chunk 18/40 completed.\n",
            "AI response for chunk 18/40 parsed.\n",
            "AI request for chunk 18/40 appended.\n",
            "Labelling chunk 19/40\n",
            "AI request for chunk 19/40 completed.\n",
            "AI response for chunk 19/40 parsed.\n",
            "AI request for chunk 19/40 appended.\n",
            "Labelling chunk 20/40\n",
            "AI request for chunk 20/40 completed.\n",
            "AI response for chunk 20/40 parsed.\n",
            "AI request for chunk 20/40 appended.\n",
            "Labelling chunk 21/40\n",
            "AI request for chunk 21/40 completed.\n",
            "AI response for chunk 21/40 parsed.\n",
            "AI request for chunk 21/40 appended.\n",
            "Labelling chunk 22/40\n",
            "AI request for chunk 22/40 completed.\n",
            "AI response for chunk 22/40 parsed.\n",
            "AI request for chunk 22/40 appended.\n",
            "Labelling chunk 23/40\n",
            "AI request for chunk 23/40 completed.\n",
            "AI response for chunk 23/40 parsed.\n",
            "AI request for chunk 23/40 appended.\n",
            "Labelling chunk 24/40\n",
            "AI request for chunk 24/40 completed.\n",
            "AI response for chunk 24/40 parsed.\n",
            "AI request for chunk 24/40 appended.\n",
            "Labelling chunk 25/40\n",
            "AI request for chunk 25/40 completed.\n",
            "AI response for chunk 25/40 parsed.\n",
            "AI request for chunk 25/40 appended.\n",
            "Labelling chunk 26/40\n",
            "AI request for chunk 26/40 completed.\n",
            "AI response for chunk 26/40 parsed.\n",
            "AI request for chunk 26/40 appended.\n",
            "Labelling chunk 27/40\n",
            "AI request for chunk 27/40 completed.\n",
            "AI response for chunk 27/40 parsed.\n",
            "AI request for chunk 27/40 appended.\n",
            "Labelling chunk 28/40\n",
            "AI request for chunk 28/40 completed.\n",
            "AI response for chunk 28/40 parsed.\n",
            "AI request for chunk 28/40 appended.\n",
            "Labelling chunk 29/40\n",
            "AI request for chunk 29/40 completed.\n",
            "AI response for chunk 29/40 parsed.\n",
            "AI request for chunk 29/40 appended.\n",
            "Labelling chunk 30/40\n",
            "AI request for chunk 30/40 completed.\n",
            "AI response for chunk 30/40 parsed.\n",
            "AI request for chunk 30/40 appended.\n",
            "Labelling chunk 31/40\n",
            "AI request for chunk 31/40 completed.\n",
            "AI response for chunk 31/40 parsed.\n",
            "AI request for chunk 31/40 appended.\n",
            "Labelling chunk 32/40\n",
            "AI request for chunk 32/40 completed.\n",
            "AI response for chunk 32/40 parsed.\n",
            "AI request for chunk 32/40 appended.\n",
            "Labelling chunk 33/40\n",
            "AI request for chunk 33/40 completed.\n",
            "AI response for chunk 33/40 parsed.\n",
            "AI request for chunk 33/40 appended.\n",
            "Labelling chunk 34/40\n",
            "AI request for chunk 34/40 completed.\n",
            "AI response for chunk 34/40 parsed.\n",
            "AI request for chunk 34/40 appended.\n",
            "Labelling chunk 35/40\n",
            "AI request for chunk 35/40 completed.\n",
            "AI response for chunk 35/40 parsed.\n",
            "AI request for chunk 35/40 appended.\n",
            "Labelling chunk 36/40\n",
            "AI request for chunk 36/40 completed.\n",
            "AI response for chunk 36/40 parsed.\n",
            "AI request for chunk 36/40 appended.\n",
            "Labelling chunk 37/40\n",
            "AI request for chunk 37/40 completed.\n",
            "AI response for chunk 37/40 parsed.\n",
            "AI request for chunk 37/40 appended.\n",
            "Labelling chunk 38/40\n",
            "AI request for chunk 38/40 completed.\n",
            "AI response for chunk 38/40 parsed.\n",
            "AI request for chunk 38/40 appended.\n",
            "Labelling chunk 39/40\n",
            "AI request for chunk 39/40 completed.\n",
            "AI response for chunk 39/40 parsed.\n",
            "AI request for chunk 39/40 appended.\n",
            "Labelling chunk 40/40\n",
            "AI request for chunk 40/40 completed.\n",
            "AI response for chunk 40/40 parsed.\n",
            "AI request for chunk 40/40 appended.\n",
            "AI request for labelling None chunks complete. Appended.\n",
            "‚úÖ Paper paper_4 labelled.\n",
            "Labelling chunks for paper: paper_5, https://arxiv.org/pdf/2505.04651\n",
            "Total chunks: 40\n",
            "============================================================\n",
            "\n",
            "Starting labelling for: 40\n",
            "Labelling chunk 1/40\n",
            "AI request for chunk 1/40 completed.\n",
            "AI response for chunk 1/40 parsed.\n",
            "AI request for chunk 1/40 appended.\n",
            "Labelling chunk 2/40\n",
            "AI request for chunk 2/40 completed.\n",
            "AI response for chunk 2/40 parsed.\n",
            "AI request for chunk 2/40 appended.\n",
            "Labelling chunk 3/40\n",
            "AI request for chunk 3/40 completed.\n",
            "AI response for chunk 3/40 parsed.\n",
            "AI request for chunk 3/40 appended.\n",
            "Labelling chunk 4/40\n",
            "AI request for chunk 4/40 completed.\n",
            "AI response for chunk 4/40 parsed.\n",
            "AI request for chunk 4/40 appended.\n",
            "Labelling chunk 5/40\n",
            "AI request for chunk 5/40 completed.\n",
            "AI response for chunk 5/40 parsed.\n",
            "AI request for chunk 5/40 appended.\n",
            "Labelling chunk 6/40\n",
            "AI request for chunk 6/40 completed.\n",
            "AI response for chunk 6/40 parsed.\n",
            "AI request for chunk 6/40 appended.\n",
            "Labelling chunk 7/40\n",
            "AI request for chunk 7/40 completed.\n",
            "AI response for chunk 7/40 parsed.\n",
            "AI request for chunk 7/40 appended.\n",
            "Labelling chunk 8/40\n",
            "AI request for chunk 8/40 completed.\n",
            "AI response for chunk 8/40 parsed.\n",
            "AI request for chunk 8/40 appended.\n",
            "Labelling chunk 9/40\n",
            "AI request for chunk 9/40 completed.\n",
            "AI response for chunk 9/40 parsed.\n",
            "AI request for chunk 9/40 appended.\n",
            "Labelling chunk 10/40\n",
            "AI request for chunk 10/40 completed.\n",
            "AI response for chunk 10/40 parsed.\n",
            "AI request for chunk 10/40 appended.\n",
            "Labelling chunk 11/40\n",
            "AI request for chunk 11/40 completed.\n",
            "AI response for chunk 11/40 parsed.\n",
            "AI request for chunk 11/40 appended.\n",
            "Labelling chunk 12/40\n",
            "AI request for chunk 12/40 completed.\n",
            "AI response for chunk 12/40 parsed.\n",
            "AI request for chunk 12/40 appended.\n",
            "Labelling chunk 13/40\n",
            "AI request for chunk 13/40 completed.\n",
            "AI response for chunk 13/40 parsed.\n",
            "AI request for chunk 13/40 appended.\n",
            "Labelling chunk 14/40\n",
            "AI request for chunk 14/40 completed.\n",
            "AI response for chunk 14/40 parsed.\n",
            "AI request for chunk 14/40 appended.\n",
            "Labelling chunk 15/40\n",
            "AI request for chunk 15/40 completed.\n",
            "AI response for chunk 15/40 parsed.\n",
            "AI request for chunk 15/40 appended.\n",
            "Labelling chunk 16/40\n",
            "AI request for chunk 16/40 completed.\n",
            "AI response for chunk 16/40 parsed.\n",
            "AI request for chunk 16/40 appended.\n",
            "Labelling chunk 17/40\n",
            "AI request for chunk 17/40 completed.\n",
            "AI response for chunk 17/40 parsed.\n",
            "AI request for chunk 17/40 appended.\n",
            "Labelling chunk 18/40\n",
            "AI request for chunk 18/40 completed.\n",
            "AI response for chunk 18/40 parsed.\n",
            "AI request for chunk 18/40 appended.\n",
            "Labelling chunk 19/40\n",
            "AI request for chunk 19/40 completed.\n",
            "AI response for chunk 19/40 parsed.\n",
            "AI request for chunk 19/40 appended.\n",
            "Labelling chunk 20/40\n",
            "AI request for chunk 20/40 completed.\n",
            "AI response for chunk 20/40 parsed.\n",
            "AI request for chunk 20/40 appended.\n",
            "Labelling chunk 21/40\n",
            "AI request for chunk 21/40 completed.\n",
            "AI response for chunk 21/40 parsed.\n",
            "AI request for chunk 21/40 appended.\n",
            "Labelling chunk 22/40\n",
            "AI request for chunk 22/40 completed.\n",
            "AI response for chunk 22/40 parsed.\n",
            "AI request for chunk 22/40 appended.\n",
            "Labelling chunk 23/40\n",
            "AI request for chunk 23/40 completed.\n",
            "AI response for chunk 23/40 parsed.\n",
            "AI request for chunk 23/40 appended.\n",
            "Labelling chunk 24/40\n",
            "AI request for chunk 24/40 completed.\n",
            "AI response for chunk 24/40 parsed.\n",
            "AI request for chunk 24/40 appended.\n",
            "Labelling chunk 25/40\n",
            "AI request for chunk 25/40 completed.\n",
            "AI response for chunk 25/40 parsed.\n",
            "AI request for chunk 25/40 appended.\n",
            "Labelling chunk 26/40\n",
            "AI request for chunk 26/40 completed.\n",
            "AI response for chunk 26/40 parsed.\n",
            "AI request for chunk 26/40 appended.\n",
            "Labelling chunk 27/40\n",
            "AI request for chunk 27/40 completed.\n",
            "AI response for chunk 27/40 parsed.\n",
            "AI request for chunk 27/40 appended.\n",
            "Labelling chunk 28/40\n",
            "AI request for chunk 28/40 completed.\n",
            "AI response for chunk 28/40 parsed.\n",
            "AI request for chunk 28/40 appended.\n",
            "Labelling chunk 29/40\n",
            "AI request for chunk 29/40 completed.\n",
            "AI response for chunk 29/40 parsed.\n",
            "AI request for chunk 29/40 appended.\n",
            "Labelling chunk 30/40\n",
            "AI request for chunk 30/40 completed.\n",
            "AI response for chunk 30/40 parsed.\n",
            "AI request for chunk 30/40 appended.\n",
            "Labelling chunk 31/40\n",
            "AI request for chunk 31/40 completed.\n",
            "AI response for chunk 31/40 parsed.\n",
            "AI request for chunk 31/40 appended.\n",
            "Labelling chunk 32/40\n",
            "AI request for chunk 32/40 completed.\n",
            "AI response for chunk 32/40 parsed.\n",
            "AI request for chunk 32/40 appended.\n",
            "Labelling chunk 33/40\n",
            "AI request for chunk 33/40 completed.\n",
            "AI response for chunk 33/40 parsed.\n",
            "AI request for chunk 33/40 appended.\n",
            "Labelling chunk 34/40\n",
            "AI request for chunk 34/40 completed.\n",
            "AI response for chunk 34/40 parsed.\n",
            "AI request for chunk 34/40 appended.\n",
            "Labelling chunk 35/40\n",
            "AI request for chunk 35/40 completed.\n",
            "AI response for chunk 35/40 parsed.\n",
            "AI request for chunk 35/40 appended.\n",
            "Labelling chunk 36/40\n",
            "AI request for chunk 36/40 completed.\n",
            "AI response for chunk 36/40 parsed.\n",
            "AI request for chunk 36/40 appended.\n",
            "Labelling chunk 37/40\n",
            "AI request for chunk 37/40 completed.\n",
            "AI response for chunk 37/40 parsed.\n",
            "AI request for chunk 37/40 appended.\n",
            "Labelling chunk 38/40\n",
            "AI request for chunk 38/40 completed.\n",
            "AI response for chunk 38/40 parsed.\n",
            "AI request for chunk 38/40 appended.\n",
            "Labelling chunk 39/40\n",
            "AI request for chunk 39/40 completed.\n",
            "AI response for chunk 39/40 parsed.\n",
            "AI request for chunk 39/40 appended.\n",
            "Labelling chunk 40/40\n",
            "AI request for chunk 40/40 completed.\n",
            "AI response for chunk 40/40 parsed.\n",
            "AI request for chunk 40/40 appended.\n",
            "AI request for labelling None chunks complete. Appended.\n",
            "‚úÖ Paper paper_5 labelled.\n",
            "Labelling chunks for paper: paper_6, https://arxiv.org/pdf/2503.24047\n",
            "Total chunks: 37\n",
            "============================================================\n",
            "\n",
            "Starting labelling for: 37\n",
            "Labelling chunk 1/37\n",
            "AI request for chunk 1/37 completed.\n",
            "AI response for chunk 1/37 parsed.\n",
            "AI request for chunk 1/37 appended.\n",
            "Labelling chunk 2/37\n",
            "AI request for chunk 2/37 completed.\n",
            "AI response for chunk 2/37 parsed.\n",
            "AI request for chunk 2/37 appended.\n",
            "Labelling chunk 3/37\n",
            "AI request for chunk 3/37 completed.\n",
            "AI response for chunk 3/37 parsed.\n",
            "AI request for chunk 3/37 appended.\n",
            "Labelling chunk 4/37\n",
            "AI request for chunk 4/37 completed.\n",
            "AI response for chunk 4/37 parsed.\n",
            "AI request for chunk 4/37 appended.\n",
            "Labelling chunk 5/37\n",
            "AI request for chunk 5/37 completed.\n",
            "AI response for chunk 5/37 parsed.\n",
            "AI request for chunk 5/37 appended.\n",
            "Labelling chunk 6/37\n",
            "AI request for chunk 6/37 completed.\n",
            "AI response for chunk 6/37 parsed.\n",
            "AI request for chunk 6/37 appended.\n",
            "Labelling chunk 7/37\n",
            "AI request for chunk 7/37 completed.\n",
            "AI response for chunk 7/37 parsed.\n",
            "AI request for chunk 7/37 appended.\n",
            "Labelling chunk 8/37\n",
            "AI request for chunk 8/37 completed.\n",
            "AI response for chunk 8/37 parsed.\n",
            "AI request for chunk 8/37 appended.\n",
            "Labelling chunk 9/37\n",
            "AI request for chunk 9/37 completed.\n",
            "AI response for chunk 9/37 parsed.\n",
            "AI request for chunk 9/37 appended.\n",
            "Labelling chunk 10/37\n",
            "AI request for chunk 10/37 completed.\n",
            "AI response for chunk 10/37 parsed.\n",
            "AI request for chunk 10/37 appended.\n",
            "Labelling chunk 11/37\n",
            "AI request for chunk 11/37 completed.\n",
            "AI response for chunk 11/37 parsed.\n",
            "AI request for chunk 11/37 appended.\n",
            "Labelling chunk 12/37\n",
            "AI request for chunk 12/37 completed.\n",
            "AI response for chunk 12/37 parsed.\n",
            "AI request for chunk 12/37 appended.\n",
            "Labelling chunk 13/37\n",
            "AI request for chunk 13/37 completed.\n",
            "AI response for chunk 13/37 parsed.\n",
            "AI request for chunk 13/37 appended.\n",
            "Labelling chunk 14/37\n",
            "AI request for chunk 14/37 completed.\n",
            "AI response for chunk 14/37 parsed.\n",
            "AI request for chunk 14/37 appended.\n",
            "Labelling chunk 15/37\n",
            "AI request for chunk 15/37 completed.\n",
            "AI response for chunk 15/37 parsed.\n",
            "AI request for chunk 15/37 appended.\n",
            "Labelling chunk 16/37\n",
            "AI request for chunk 16/37 completed.\n",
            "AI response for chunk 16/37 parsed.\n",
            "AI request for chunk 16/37 appended.\n",
            "Labelling chunk 17/37\n",
            "AI request for chunk 17/37 completed.\n",
            "AI response for chunk 17/37 parsed.\n",
            "AI request for chunk 17/37 appended.\n",
            "Labelling chunk 18/37\n",
            "AI request for chunk 18/37 completed.\n",
            "AI response for chunk 18/37 parsed.\n",
            "AI request for chunk 18/37 appended.\n",
            "Labelling chunk 19/37\n",
            "AI request for chunk 19/37 completed.\n",
            "AI response for chunk 19/37 parsed.\n",
            "AI request for chunk 19/37 appended.\n",
            "Labelling chunk 20/37\n",
            "AI request for chunk 20/37 completed.\n",
            "AI response for chunk 20/37 parsed.\n",
            "AI request for chunk 20/37 appended.\n",
            "Labelling chunk 21/37\n",
            "AI request for chunk 21/37 completed.\n",
            "AI response for chunk 21/37 parsed.\n",
            "AI request for chunk 21/37 appended.\n",
            "Labelling chunk 22/37\n",
            "AI request for chunk 22/37 completed.\n",
            "AI response for chunk 22/37 parsed.\n",
            "AI request for chunk 22/37 appended.\n",
            "Labelling chunk 23/37\n",
            "AI request for chunk 23/37 completed.\n",
            "AI response for chunk 23/37 parsed.\n",
            "AI request for chunk 23/37 appended.\n",
            "Labelling chunk 24/37\n",
            "AI request for chunk 24/37 completed.\n",
            "AI response for chunk 24/37 parsed.\n",
            "AI request for chunk 24/37 appended.\n",
            "Labelling chunk 25/37\n",
            "AI request for chunk 25/37 completed.\n",
            "AI response for chunk 25/37 parsed.\n",
            "AI request for chunk 25/37 appended.\n",
            "Labelling chunk 26/37\n",
            "AI request for chunk 26/37 completed.\n",
            "AI response for chunk 26/37 parsed.\n",
            "AI request for chunk 26/37 appended.\n",
            "Labelling chunk 27/37\n",
            "AI request for chunk 27/37 completed.\n",
            "AI response for chunk 27/37 parsed.\n",
            "AI request for chunk 27/37 appended.\n",
            "Labelling chunk 28/37\n",
            "AI request for chunk 28/37 completed.\n",
            "AI response for chunk 28/37 parsed.\n",
            "AI request for chunk 28/37 appended.\n",
            "Labelling chunk 29/37\n",
            "AI request for chunk 29/37 completed.\n",
            "AI response for chunk 29/37 parsed.\n",
            "AI request for chunk 29/37 appended.\n",
            "Labelling chunk 30/37\n",
            "AI request for chunk 30/37 completed.\n",
            "AI response for chunk 30/37 parsed.\n",
            "AI request for chunk 30/37 appended.\n",
            "Labelling chunk 31/37\n",
            "AI request for chunk 31/37 completed.\n",
            "AI response for chunk 31/37 parsed.\n",
            "AI request for chunk 31/37 appended.\n",
            "Labelling chunk 32/37\n",
            "AI request for chunk 32/37 completed.\n",
            "AI response for chunk 32/37 parsed.\n",
            "AI request for chunk 32/37 appended.\n",
            "Labelling chunk 33/37\n",
            "AI request for chunk 33/37 completed.\n",
            "AI response for chunk 33/37 parsed.\n",
            "AI request for chunk 33/37 appended.\n",
            "Labelling chunk 34/37\n",
            "AI request for chunk 34/37 completed.\n",
            "AI response for chunk 34/37 parsed.\n",
            "AI request for chunk 34/37 appended.\n",
            "Labelling chunk 35/37\n",
            "AI request for chunk 35/37 completed.\n",
            "AI response for chunk 35/37 parsed.\n",
            "AI request for chunk 35/37 appended.\n",
            "Labelling chunk 36/37\n",
            "AI request for chunk 36/37 completed.\n",
            "AI response for chunk 36/37 parsed.\n",
            "AI request for chunk 36/37 appended.\n",
            "Labelling chunk 37/37\n",
            "AI request for chunk 37/37 completed.\n",
            "AI response for chunk 37/37 parsed.\n",
            "AI request for chunk 37/37 appended.\n",
            "AI request for labelling None chunks complete. Appended.\n",
            "‚úÖ Paper paper_6 labelled.\n",
            "dict_keys(['pdf_url', 'paper_title', 'chunks', 'labeled_chunks_all'])\n",
            "dict_keys(['chunk_n', 'chunk_text', 'label_output'])\n",
            "{'content': '<thinking>\\nThe chunk contains only the paper title, author list, affiliations, and an abstract header without any results or observations. There are no experimental findings, measurements, or observed phenomena stated.\\n</thinking>\\n<label>\\nNOT\\n</label>\\n<extracted_content>\\nNONE\\n</extracted_content>\\n<citations>\\nNONE\\n</citations>', 'usage': {'prompt_tokens': 658, 'completion_tokens': 269, 'total_tokens': 927, 'completion_tokens_details': {'reasoning_tokens': 192}}, 'success': True}\n",
            "dict_keys(['pdf_url', 'paper_title', 'chunks', 'labeled_chunks_all'])\n",
            "dict_keys(['chunk_n', 'chunk_text', 'label_output'])\n",
            "{'content': '<thinking>\\nThe chunk contains only the paper title, authors, affiliations, and publication note, with no experimental results, observations, measurements, or cited findings. Therefore, there are no empirical observations stated in this text.\\n</thinking>\\n<label>\\nNOT\\n</label>\\n<extracted_content>\\nNONE\\n</extracted_content>\\n<citations>\\nNONE\\n</citations>', 'usage': {'prompt_tokens': 553, 'completion_tokens': 274, 'total_tokens': 827, 'completion_tokens_details': {'reasoning_tokens': 192}}, 'success': True}\n",
            "dict_keys(['pdf_url', 'paper_title', 'chunks', 'labeled_chunks_all'])\n",
            "dict_keys(['chunk_n', 'chunk_text', 'label_output'])\n",
            "{'content': '<thinking>\\nThe chunk reports measured outcomes from benchmarking (best score 43.22%), comparative observations about prompting strategies, a categorized error taxonomy from a user study, and a cited external performance result (GPT-4 scoring 163/170, 80th percentile on GRE Quant). These are empirical observations with quantification or observed patterns.\\n</thinking>\\n<label>\\nempirical_observations\\n</label>\\n<extracted_content>\\n- In SCIBENCH benchmarking across representative open-source and proprietary LLMs with various prompting strategies, the best overall performance was 43.22%, indicating current LLMs fall short on collegiate-level scientific problem solving.\\n- Analysis found no single prompting strategy significantly outperforms others; strategies that improve certain problem-solving skills can produce declines in other skills.\\n- A detailed user study categorized LLM errors into ten distinct problem-solving abilities.\\n- Cited prior result: GPT-4 scored 163 out of 170 on the GRE Quantitative Exam, corresponding to the 80th percentile.\\n</extracted_content>\\n<citations>\\n(OpenAI., 2023)\\n</citations>', 'usage': {'prompt_tokens': 1048, 'completion_tokens': 1448, 'total_tokens': 2496, 'completion_tokens_details': {'reasoning_tokens': 1216}}, 'success': True}\n",
            "dict_keys(['pdf_url', 'paper_title', 'chunks', 'labeled_chunks_all'])\n",
            "dict_keys(['chunk_n', 'chunk_text', 'label_output'])\n",
            "{'content': '<thinking>\\nThe chunk provides a general statement about the history and framing of scientific discovery in computational modeling, with citations to prior work. It does not present specific measured results, experimental findings, observed phenomena, statistical outcomes, or concrete evidence. Therefore, no empirical observations are directly stated in this chunk.\\n</thinking>\\n<label>\\nNOT\\n</label>\\n<extracted_content>\\nNONE\\n</extracted_content>\\n<citations>\\nNONE\\n</citations>', 'usage': {'prompt_tokens': 512, 'completion_tokens': 354, 'total_tokens': 866, 'completion_tokens_details': {'reasoning_tokens': 256}}, 'success': True}\n",
            "dict_keys(['pdf_url', 'paper_title', 'chunks', 'labeled_chunks_all'])\n",
            "dict_keys(['chunk_n', 'chunk_text', 'label_output'])\n",
            "{'content': '<thinking>\\nThe chunk contains introductory context and literature references without reporting any concrete measurements, experimental results, statistical outcomes, or observed phenomena. Therefore, it lacks empirical observations.\\n</thinking>\\n<label>\\nNOT\\n</label>\\n<extracted_content>\\nNONE\\n</extracted_content>\\n<citations>\\nNONE\\n</citations>', 'usage': {'prompt_tokens': 512, 'completion_tokens': 456, 'total_tokens': 968, 'completion_tokens_details': {'reasoning_tokens': 384}}, 'success': True}\n",
            "dict_keys(['pdf_url', 'paper_title', 'chunks', 'labeled_chunks_all'])\n",
            "dict_keys(['chunk_n', 'chunk_text', 'label_output'])\n",
            "{'content': '<thinking>\\nThe chunk contains only the paper title, author names, affiliations, and contact information. It does not report any measurements, experiments, observed phenomena, statistical results, or findings. Therefore, there are no empirical observations stated in this chunk.\\n</thinking>\\n<label>\\nNOT\\n</label>\\n<extracted_content>\\nNONE\\n</extracted_content>\\n<citations>\\nNONE\\n</citations>', 'usage': {'prompt_tokens': 547, 'completion_tokens': 344, 'total_tokens': 891, 'completion_tokens_details': {'reasoning_tokens': 256}}, 'success': True}\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            " is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "EMPIRICAL_OBSERVATION is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "NOT is not the same as empirical_observations\n",
            "94\n",
            "<class 'dict'>\n",
            "dict_keys(['paper_url', 'paper_title', 'extracted_content', 'has_citations', 'citations'])\n",
            "https://arxiv.org/pdf/2510.09901\n",
            "A prior review systematically surveyed more than 260 scientific large language models spanning general science, mathematics, physics, chemistry, materials science, biology, medicine, and geoscience.\n",
            "https://arxiv.org/pdf/2510.09901\n",
            "In an isolated physical system, the total entropy can never decrease, consistent with the Second Law of Thermodynamics.\n",
            "https://arxiv.org/pdf/2510.09901\n",
            "- Early evidence indicates that large language models can propose scientific hypotheses in biomedicine in a zero-shot manner using only pre-trained knowledge [Qi et al., 2023].\n",
            "- Domain-specific foundation models such as BioBERT, pre-trained on biomedical texts, more accurately extract entities (genes, diseases, chemicals) and their relationships from research articles than general models [Lee et al., 2020].\n",
            "- The generative model BioGPT can extract and coherently summarize complex biological information from literature [Luo et al., 2022].\n",
            "- The Galactica model performs large-scale structuring and organization of scientific knowledge from papers, lecture notes, and textbooks, effectively carrying out knowledge extraction at massive scale [Taylor et al., 2022].\n",
            "- Retrieval-Augmented Generation produces factually grounded, traceable outputs (precise summaries and entity‚Äìrelationship pairs) by retrieving relevant documents, enabling the construction of verified, structured scientific knowledge bases [Lewis et al., 2020, Guu et al., 2020, Gao et al., 2023, Izacard et al., 2023, Fan et al., 2024, Garcia et al., 2024, Lopez et al., 2025, Feng et al., 2025a, Krotkov et al., 2025, Lee et al., 2025, Xie et al., 2024, Gao et al., 2025a].\n",
            "- Multimodal systems like ChemMiner extract chemical information from both text and diagrams within scientific papers [Chen et al., 2024a].\n",
            "- ChartAssistant can reverse-engineer scientific charts by converting visual data back into structured tables [Meng et al., 2024].\n",
            "- Multimodal models such as GPT-4o integrate information across text, tables, and diagrams to answer questions about complex biomedical documents [Hurst et al., 2024].\n",
            "- LLM-generated hypotheses are susceptible to being plausible but factually incorrect or to restating known facts [Xiong et al., 2025].\n",
            "https://arxiv.org/pdf/2510.09901\n",
            "- A framework combining LLMs with causal knowledge graphs analyzed over 43,000 psychology articles and generated well-being hypotheses whose novelty matched expert-level assessments, significantly outperforming LLM-only methods.\n",
            "- ResearchLink generated cross-domain hypotheses with high precision by integrating graph embeddings, path-based features, and bibliometric data.\n",
            "- KG-CoI grounded hypotheses using structured knowledge graphs, improving the accuracy of reasoning chains and reducing factual errors.\n",
            "https://arxiv.org/pdf/2510.09901\n",
            "In the AstroAgents system, over 30% of the generated hypotheses were validated as scientifically plausible by expert reviewers.\n",
            "https://arxiv.org/pdf/2510.09901\n",
            "- POPPER‚Äôs automated validation framework achieves human-level accuracy at significantly greater speed by applying rigorous statistical checks. [Huang et al., 2025a]\n",
            "- The Logit-based Calibrated Prior technique enables high-accuracy ranking of hypotheses by novelty and relevance in real-world data using LLM-generated expectations. [Gong and Fernandez, 2025]\n",
            "https://arxiv.org/pdf/2510.09901\n",
            "The Virtual Lab multi-agent system generated 92 nanobody candidates and experimentally validated two that showed improved binding to SARS-CoV-2 variants.\n",
            "https://arxiv.org/pdf/2510.09901\n",
            "- The evolutionary LLM-based code mutation process has produced entirely new algorithms that are more efficient than human-designed counterparts. [Nagda et al., 2025]\n",
            "- ShinkaEvolve improves sample efficiency in program evolution by combining exploration‚Äìexploitation-balanced parent selection, novelty-based rejection sampling, and bandit-based, task-dependent LLM prioritization. [Lange et al., 2025]\n",
            "- TOOLMAKER‚Äôs cross-domain evaluations demonstrate substantially higher reliability than prior software-engineering agents. [W√∂lflein et al., 2025]\n",
            "- Coscientist has demonstrated the ability to use bioinformatics software and control liquid handling robots to execute experiments. [Boiko et al., 2023]\n",
            "https://arxiv.org/pdf/2510.09901\n",
            "- The Self-Refine framework, which enables LLMs to critique and iteratively refine their outputs, has shown significant improvements on reasoning tasks. [Madaan et al., 2023]\n",
            "- Iterative introspection significantly enhances code-generation accuracy in LLMs. [Chen et al., 2023b]\n",
            "- Self-correction methods alone are often insufficient due to inherent limitations in LLM self-assessment capabilities. [Huang et al., 2023]\n",
            "https://arxiv.org/pdf/2510.09901\n",
            "- ResearchAgent‚Äôs human-aligned reviewing agents simulate structured peer review and significantly enhance the feasibility and novelty of scientific research proposals.\n",
            "- MAPPS uses human scientists to verify generated workflows for materials discovery, leading to more robust results.\n",
            "- AI co-scientist integrates human expertise within an iterative ‚Äúgenerate, debate, and evolve‚Äù framework, ensuring robust scientific outcomes.\n",
            "- Across general research, RLHF guides models toward behaviors reflecting expert evaluations.\n",
            "- Domain-specific scientific agents are beginning to tackle tasks such as designing novel molecules, predicting protein structures, and discovering new materials with unprecedented speed.\n",
            "https://arxiv.org/pdf/2510.09901\n",
            "- SpatialAgent autonomously processed over 2 million cells across spatial transcriptomics and MERFISH pipelines and achieved parity or gains versus automated baselines while handling full projects with minimal human input [Wang et al., 2025a].\n",
            "- Team of AI-made Scientists (TAIS) analyzed gene expression datasets to identify disease-predictive genes and completed the gene discovery pipeline from raw data to insights without human intervention [Liu et al., 2024a].\n",
            "- CRISPR-GPT‚Äôs agent-generated gene-editing experiment designs were successfully translated into real-world wet-lab execution [Qu et al., 2025].\n",
            "- BioDiscoveryAgent outperformed trained Bayesian-optimization baselines by an average of 21% (and 46% on non-essential genes) across six datasets and doubled the hit-rate for multi-gene combinations over random [Roohani et al., 2024].\n",
            "- PerTurboAgent improved next-round gene panel quality over static or heuristic baselines in retrospective/simulation studies and shortened cycles in pooled genetic screens [Hao et al., 2025].\n",
            "- BioAgents achieved near-expert performance on conceptual genomics tasks in locally runnable, privacy-preserving multi-agent workflows with RAG [Mehandru et al., 2025].\n",
            "- scBaseCount expanded a harmonized single-cell compendium to over 230 million cells spanning 21 organisms and 72 tissues [Youngblut et al., 2025].\n",
            "- scAgent achieved state-of-the-art accuracy and generalization for universal cell-type annotation across 160 cell types in 35 tissues, with data-efficient extension to unseen types [Mao et al., 2025].\n",
            "- CASSIA improved low-quality cell-type annotations by retrieving external evidence and consolidating tool outputs into audit-ready documentation [Xie et al., 2024].\n",
            "- GeneAgent reduced hallucinations by cross-checking claims against expert-curated biological databases and produced auditable rationales for functional descriptions [Wang et al., 2025b].\n",
            "https://arxiv.org/pdf/2510.09901\n",
            "- SAMPLE experimentally identified GH1 hydrolase variants with at least 12 ¬∞C higher thermal stability than the starting sequences using autonomous design‚Äìbuild‚Äìtest cycles in a robotic lab. [Rapp et al., 2024]\n",
            "- The Virtual Lab designed 92 novel SARS-CoV-2 nanobodies and experimentally validated expression and binding across variants, with more than 90% expression among tested constructs. [Swanson et al., 2024]\n",
            "- ProtAgents generated de novo protein sequences that met specified vibrational-frequency profiles and structural constraints, validated via structure prediction and physics-aware analyses. [Ghafarollahi and Buehler, 2024a]\n",
            "- Sparks uncovered two design rules: Œ≤-biased peptides exceed Œ±-helical peptides in unfolding force for chain lengths beyond approximately 80 residues; and a chain-length/secondary-structure stability map exhibits a high-variance ‚Äúfrustration zone‚Äù for Œ±/Œ≤ folds. [Ghafarollahi and Buehler, 2025]\n",
            "- VibeGen produced de novo proteins whose all-atom molecular dynamics trajectories reproduced prescribed normal-mode amplitudes. [Ni and Buehler, 2025]\n",
            "- AutoProteinEngine achieved substantially higher accuracy than zero-shot approaches and manual fine-tuning on two real protein-engineering tasks. [Liu et al., 2024b]\n",
            "https://arxiv.org/pdf/2510.09901\n",
            "- DrugAgent achieved approximately 0.92 F1 on PAMPA absorption prediction in case studies.\n",
            "- CLADD improved task performance compared to general LLMs and classical deep-learning baselines on drug discovery tasks involving design, docking, and triage.\n",
            "- BioResearcher demonstrated measurable gains on complex research objectives without manual ‚Äúglue code.‚Äù\n",
            "https://arxiv.org/pdf/2510.09901\n",
            "- LIDDiA generated molecules meeting pharmaceutical criteria across many targets and surfaced promising EGFR candidates. [Averly et al., 2025]\n",
            "- AgentMD automatically selected and executed from 2,164 curated clinical calculators (RiskCalcs) to return risk estimates with formula provenance, substantially outperforming strong prompting baselines on RiskQA. [Jin et al., 2024a]\n",
            "- MedAgents enabled multi-specialist deliberation in zero-shot settings to reach consensus diagnoses and plans across standard medical QA suites. [Tang et al., 2023]\n",
            "- ClinicalGPT is a domain LLM for clinical tasks; an EHR-integrated example, EHRAgent, auto-generated and executed code against EHR data to answer complex patient queries and compute scores. [Wang et al., 2023b]\n",
            "- BehaveAgent provided turn-key cross-species behavior analysis from raw video‚Äîincluding planning, tracking/pose estimation, sequence labeling, and report generation‚Äîwithout retraining across paradigms. [Aljovic et al., 2025]\n",
            "https://arxiv.org/pdf/2510.09901\n",
            "- Coscientist autonomously executed palladium-catalyzed cross-coupling reactions in under 4 minutes. [Boiko et al., 2023]\n",
            "- ChemCrow integrated 18 expert-designed tools and successfully performed autonomous planning and multi-tool analysis across organic synthesis, drug discovery, and materials science. [M. Bran et al., 2024]\n",
            "- Chemist-X achieved a fully automated, closed-loop system that proposes reaction conditions via retrieval-augmented generation and directs a robotic platform for wet-lab validation. [Chen et al., 2023a]\n",
            "- El Agente Q autonomously handled an entire computational chemistry workflow‚Äîfrom file preparation to cluster submission and result parsing‚Äîbased on a simple natural language prompt. [Zou et al., 2025]\n",
            "- LLM-RDF enabled an end-to-end synthesis workflow from literature search to product purification through a conversational six-agent framework for users without coding expertise. [Ruan et al., 2024]\n",
            "- FROGENT showed superior performance on multistep drug discovery benchmarks by integrating diverse biochemical databases and predictive models into a unified framework. [Pan et al., 2025]\n",
            "- LARC achieved a near-human-level success rate on diverse retrosynthesis tasks using an Agent-as-a-Judge mechanism to apply practical constraints. [Baker et al., 2025]\n",
            "- FMG achieved expert-level molecular design by adopting graph representations rendered as images, demonstrating benefits of multimodal inputs. [Sun et al., 2025]\n",
            "- MOOSE-Chem rediscovered core innovations from recent high-impact chemistry papers without prior knowledge of their content via automated hypothesis generation. [Yang et al., 2024]\n",
            "- The A-Lab synthesized 41 novel compounds from 58 targets over 17 days of continuous operation using an autonomous facility with three robotic arms, furnaces, and X-ray diffractometers, achieving a 71% success rate. [Szymanski et al., 2023]\n",
            "https://arxiv.org/pdf/2510.09901\n",
            "- MDAgent automated the MD workflow and reduced the total task time for thermodynamic calculations by over 40%. [Shi et al., 2025]\n",
            "- The Materials Laws Multi-Agent Framework used LLM agents for symbolic regression to derive a low-complexity, highly accurate formula for predicting glass-forming ability in metallic glasses. [Hu et al., 2024]\n",
            "- HoneyComb, grounded with a curated materials knowledge base and validated tools, achieved significantly higher accuracy in both reasoning and computation than general-purpose LLMs. [Zhang et al., 2024b]\n",
            "- The ‚ÄúChatGPT Research Group‚Äù system significantly accelerated the experimental optimization cycle for advanced materials synthesis via a collaborative multiagent setup. [Zheng et al., 2023]\n",
            "- LLMatDesign enabled effective discovery of materials with specific, user-defined properties in low-data regimes by adapting strategy through self-reflection on computational outcomes. [Jia et al., 2024]\n",
            "- MatAgent‚Äôs integrated human-in-the-loop multi-agent framework enhanced both the speed and reproducibility of materials research workflows. [Bazgir et al., 2025a]\n",
            "https://arxiv.org/pdf/2510.09901\n",
            "- The k-agents framework enabled a fully autonomous laboratory where LLM agents planned and executed experiments on superconducting quantum processors, producing entangled states with performance equivalent to human experts [Cao et al., 2024].\n",
            "- AtomAgents integrated knowledge retrieval with physics-based simulations to autonomously propose, simulate, and refine alloy compositions that met specified performance targets [Ghafarollahi and Buehler, 2024c].\n",
            "- Mephisto automated interpretation of multi-band galaxy observations by calling the CIGALE code, successfully proposing and testing hypotheses against observational data in an iterative loop [Sun et al., 2024].\n",
            "- QCopilot achieved a 100-fold speedup over manual procedures by autonomously performing modeling, optimization, and anomaly detection in atom cooling experiments [Sha et al., 2025].\n",
            "- OpenFOAMGPT successfully automated the entire OpenFOAM workflow from case setup to iterative correction, lowering the barrier to advanced CFD usage [Pandey et al., 2025].\n",
            "- MetaOpenFOAM used a multi-agent system to decompose complex natural language instructions and achieved strong performance across a diverse range of flow simulations [Chen et al., 2024b].\n",
            "https://arxiv.org/pdf/2510.09901\n",
            "- LP-COMDA automated the design of complex modulation strategies for power converters with a physics-informed planner, accelerating design time by over 30x and reducing errors by over 60% [Liu et al., 2024d].\n",
            "- The Autonomous GIS Agent and GIS Copilot automated end-to-end geospatial workflows‚Äîfrom data retrieval to generating maps and statistics‚Äîby translating natural language requests into executable programs, enabling advanced geospatial analysis without specialized programming [Ning et al., 2025] [Akinboyewa et al., 2025].\n",
            "- A domain-specific ReAct agent for gas turbines integrated expert knowledge with predefined tools to perform iterative gas path analysis, demonstrating feasibility of AI-driven diagnostics in power engineering [Song et al., 2024].\n",
            "https://arxiv.org/pdf/2510.09901\n",
            "- Supervised fine-tuning, as a form of imitation learning, limits LLMs‚Äô ability to generalize to new domains. \n",
            "- Reinforcement learning from verifiable rewards elicits generalizable reasoning abilities from base policy models. \n",
            "- In DeepSeek-R1, rewarding models solely on final-answer correctness led to learned complex reasoning behaviors, including self-verification and self-correction.\n",
            "https://arxiv.org/pdf/2510.09901\n",
            "- Integrating code interpreters enhances agents‚Äô mathematical abilities, and RL further optimizes multi-turn reasoning via real-time code execution.\n",
            "- With search engine tools, RL helps agents generate more effective search queries to acquire external and up-to-date information.\n",
            "- Web browsing and GUI agent studies show agents operating in dynamic environments where states change in response to their actions, necessitating interactive decision-making.\n",
            "- RL has been used to train agents to coordinate multiple tools concurrently to solve harder tasks such as deep research.\n",
            "https://arxiv.org/pdf/2510.09901\n",
            "Historical evidence indicates that major scientific breakthroughs‚Äîsuch as penicillin, the cosmic microwave background radiation, and graphene‚Äîoriginated from chance or unexpected observations, underscoring the pivotal role of serendipity in discovery.\n",
            "https://arxiv.org/pdf/2410.07076\n",
            "- Yang et al. (2024b) found via expert evaluation that LLMs can generate hypotheses in social science that are both novel and sufficiently valid.\n",
            "- Si et al. (2024) showed in the NLP domain that LLMs generate research hypotheses that are more novel but slightly less valid than those from human researchers.\n",
            "- In this work, using a benchmark of 51 high-impact chemistry papers published after January 2024, the proposed MOOSE-Chem framework rediscovered many target hypotheses with high similarity to the ground truth and captured core innovations while using an LLM with a pre-2024 knowledge cutoff.\n",
            "- The framework achieved surprisingly high accuracy on the inspiration-retrieval subtask despite its out-of-distribution nature.\n",
            "https://arxiv.org/pdf/2410.07076\n",
            "- From extensive discussions with chemistry experts, the authors observed that the social-science assumption that a hypothesis can be divided into background and inspiration applies to a majority of chemistry hypotheses.\n",
            "- Prior cognitive science research has shown that creative ideas often result from the cohesive association of two seemingly unrelated pieces of knowledge.\n",
            "https://arxiv.org/pdf/2410.07076\n",
            "- Experiments using the constructed benchmark found that LLMs are highly capable of addressing the three fundamental questions posed for hypothesis rediscovery.\n",
            "- When tested with only a background and a corpus of up to 3,000 chemistry papers for inspiration selection, MOOSE-CHEM rediscovered many hypotheses with very high similarity to the ground-truth hypotheses, capturing the main innovations.\n",
            "- The study shows that an LLM-based framework can largely rediscover the main innovations of many chemistry hypotheses published in Nature and Science, despite controls to prevent data contamination (LLM training data cutoff in December 2023; target papers only available online in 2024).\n",
            "https://arxiv.org/pdf/2410.07076\n",
            "- The TOMATO-Chem benchmark contains 51 chemistry/materials papers: Polymer Chemistry 21, Organic Chemistry 22, Inorganic Chemistry 3, Analytical Chemistry 5; publication venues include Nature/Science 27, Nature subjournals 20, and other top journals 4. [Table 1; Table 2]\n",
            "- In inspiration retrieval (Q1), GPT-4o achieves high hit ratios even with aggressive down-selection: for corpus sizes 150/300/1000/3000, hit ratios are 92.8/76.8/61.4/NA% at top 20%, 96.7/83.7/60.8/NA% at top 4%, 96.4/88.9/69.0/46.7% at top 0.8%, and 95.8/86.9/70.6/52.0% at top 0.016%. [Table 3]\n",
            "- Smaller screening windows improve retrieval: with a screen window size of 60 selecting 3 in one round (5% of corpus) the hit ratio is 71.6%, whereas a window of 15 selecting 3 for two rounds (4% of corpus) yields 83.7%. [Table 4]\n",
            "- Model comparison for inspiration retrieval (corpus size 300) shows GPT-4o outperforming Llama-3.1 models: GPT-4o achieves 96.7% (top 20%), 83.7% (top 4%), 60.8% (top 0.8%); Llama-3.1-405B: 95.7%, 78.7%, 52.7%; Llama-3.1-70B: 83.0%, 59.5%, 43.5%; Llama-3.1-8B: 71.6%, 43.5%, 26.8%. [Table 5]\n",
            "- In hypothesis synthesis evaluation (Q2), with a background survey, GPT-4o Top Matched Score (MS) distribution across 51 tasks is: 28 with MS=5, 1 with MS=4, 19 with MS=3, 3 with MS=2, 0 with MS=1, 0 with MS=0; expert Top MS distribution is: 9 with MS=5, 12 with MS=4, 22 with MS=3, 6 with MS=2, 2 with MS=1, 0 with MS=0. Without a background survey, GPT-4o Top MS distribution is: 25 with MS=5, 2 with MS=4, 19 with MS=3, 5 with MS=2, 0 with MS=1, 0 with MS=0. [Table 7]\n",
            "- LLM-based ranking correlates with use of ground-truth inspirations (Q3): hypotheses that match 2 inspirations have average rank ratio 0.411 (n=302), 1 inspiration 0.474 (n=2458), and 0 inspirations 0.521 (n=4899). [Table 8]\n",
            "- LLM-based ranking also trends with content quality: hypotheses with GPT-4o-labeled MS of 4 have average rank ratio 0.439 (n=36), MS=3 has 0.488 (n=404), MS=2 has 0.501 (n=427), while those with no matched inspirations (MS=-1) have 0.503 (n=6451). [Table 9]\n",
            "- In copilot-like experiments (|I|=300, strict background), expert Top MS over 51 tasks is: 0 with MS=5, 2 with MS=4, 19 with MS=3, 16 with MS=2, 8 with MS=1, 6 with MS=0. [Table 11]\n",
            "- MOOSE-Chem outperforms baselines on hypothesis quality: Top MS/Average MS are 4.020/3.765 versus SciMON 2.549/2.464, MOOSE 2.281/2.686, and Qi et al. (2024) 2.882/2.356; removing multi-step and evolutionary units reduces performance (e.g., w/o multi-step: 2.564/2.730; w/o multi-step & EU: 2.863/2.578). [Table 10]\n",
            "- Prior studies report: (i) LLM-generated NLP/biochemical hypotheses lag scientific papers in novelty, depth, and utility (Wang et al., 2024b); (ii) LLMs can generate novel and valid social science hypotheses as judged by PhD students (Yang et al., 2024b); (iii) FunSearch finds solutions to specific conjectures but not new theorems (Romera-Paredes et al., 2024); (iv) catalyst discovery evaluations relying on rediscovery of commercial catalysts risk data contamination (Sprueill et al., 2023; 2024); (v) word embeddings from 3.3M materials abstracts recommended materials years pre-discovery (Tshitoyan et al., 2019); (vi) literature sentiment predicts emerging thermoelectric materials (Xie et al., 2024). [(Wang et al., 2024b); (Yang et al., 2024b); (Romera-Paredes et al., 2024); (Sprueill et al., 2023; 2024); (Kumar et al., 2024); (Tshitoyan et al., 2019); (Xie et al., 2024)]\n",
            "https://arxiv.org/pdf/2410.07076\n",
            "Experiments on a 2024 chemistry and materials science benchmark using an LLM-based multi-agent framework (with models trained up to October 2023 in a copilot in-the-wild setting) showed the system could rediscover many published hypotheses with very high similarity to the ground-truth innovations.\n",
            "https://arxiv.org/pdf/2410.07076\n",
            "- Under Claude-3.5-Sonnet evaluation, MOOSE-Chem achieved the highest Top Matched Score (4.471) and a higher Average Matched Score (3.697) than SciMON (TopMS 3.824, AvgMS 3.559), MOOSE (TopMS 3.529, AvgMS 3.431), and Qi et al. (TopMS 3.902, AvgMS 3.092); removing multi-step (TopMS 4.216, AvgMS 3.592) and removing both multi-step & EU (TopMS 3.941, AvgMS 3.614) reduced performance relative to full MOOSE-Chem.\n",
            "- Under Gemini-1.5-Pro evaluation, MOOSE-Chem attained the highest Top Matched Score (3.686), while its Average Matched Score (2.443) was lower than SciMON (2.690) and the w/o multi-step variant (2.529); removing multi-step & EU further reduced TopMS to 2.902.\n",
            "- Counting high-quality hypotheses by branch shows: at MS ‚â• 5, only non-EU = 16, only EU branches = 19, only EU-recombination branch = 20; at MS ‚â• 4, only non-EU = 46, only EU branches = 54, only EU-recombination branch = 24; this indicates roughly one-third of high-quality hypotheses can be obtained without mutations and that recombination yields more high-quality hypotheses than the non-EU branch.\n",
            "- Significance feedback ablation (Claude-3.5-Sonnet) shows that omitting significance feedback improves Matched Score distributions (e.g., Average MS count at score 5 increases from 4 to 8; Top MS count at score 4 increases from 7 to 13), leading to better performance on the Matched Score metric.\n",
            "- When ranking ground truth hypotheses mixed with generated ones, average rank ratios were Overall = 0.65, Validness = 0.75, Novelty = 0.76, Significance = 0.73, Potential = 0.70, indicating ground truth hypotheses are not consistently ranked at the top.\n",
            "- Background options ablation for Q1 shows that including a background survey improves inspiration retrieval: with strict background + survey, Hit Ratios are 96.7% (top 20%), 83.7% (top 4%), 60.8% (top 0.8%); without strict background (with survey), 95.1%, 77.8%, 54.2%; with strict background without survey, 96.7%, 80.1%, 57.8%.\n",
            "https://arxiv.org/pdf/2410.07076\n",
            "- Across 392 comparison pairs, the agreement between expert evaluation and GPT-4o on the Matched Score yielded a hard consistency of 0.345 and a soft consistency of 0.542.\n",
            "- Across 48 comparison pairs, inter-expert agreement on the Matched Score yielded a hard consistency of 0.438 and a soft consistency of 0.854.\n",
            "- In practice, GPT-4o‚Äôs automatic evaluation scores on the Matched Score are typically 1‚Äì2 points higher than expert evaluation scores.\n",
            "- Findings reported by Bu et al. (2024) validate that a nitrogen-doped ruthenium electrode in the presence of D2O enables highly efficient, scalable, and versatile electrocatalytic reductive deuteration suitable for a wide range of substrates.\n",
            "- Wang et al. (2024a) report that integrating (Gdm)2SO4 into a PVA hydrogel with directional freezing achieves a flexible thermogalvanic armor with Carnot-relative efficiency exceeding 8% while maintaining high mechanical strength, with thermopower and mechanical robustness surpassing traditional quasi-solid thermocells.\n",
            "https://arxiv.org/pdf/2410.07076\n",
            "In the actual experiment, dual-wavelength catalysis and solvent mixing were not employed; acetone and acetonitrile were the two best-performing single solvents in the actual research.\n",
            "https://arxiv.org/pdf/2307.10635\n",
            "- In SCIBENCH benchmarking across representative open-source and proprietary LLMs with various prompting strategies, the best overall performance was 43.22%, indicating current LLMs fall short on collegiate-level scientific problem solving.\n",
            "- Analysis found no single prompting strategy significantly outperforms others; strategies that improve certain problem-solving skills can produce declines in other skills.\n",
            "- A detailed user study categorized LLM errors into ten distinct problem-solving abilities.\n",
            "- Cited prior result: GPT-4 scored 163 out of 170 on the GRE Quantitative Exam, corresponding to the 80th percentile.\n",
            "https://arxiv.org/pdf/2307.10635\n",
            "- Existing benchmarks ScienceQA and GSM8K predominantly contain problems grounded in grade-level subjects (Lu et al., 2022; Cobbe et al., 2021).\n",
            "- The MATH benchmark introduces high-school level questions but primarily focuses on math problems (Hendrycks et al., 2021).\n",
            "- Benchmarks such as MMLU, AGIEval, and JEEBench, though spanning multiple disciplines, mainly require basic computations (addition, subtraction, multiplication, exponentiation), which do not adequately assess deep scientific reasoning (Hendrycks et al., 2020; Zhong et al., 2023; Arora et al., 2023).\n",
            "- Most of these benchmarks are restricted to textual problems and omit visual elements such as figures or diagrams (Lu et al., 2022; Cobbe et al., 2021; Hendrycks et al., 2021; Hendrycks et al., 2020; Zhong et al., 2023; Arora et al., 2023).\n",
            "- The Chain-of-Thought approach prompts LLMs to produce detailed, step-by-step solutions intended to stimulate deeper problem thinking (Huang et al., 2022; Wang et al., 2022; Wei et al., 2022; Zhou et al., 2022).\n",
            "- Strategies that enable LLMs to use external tools improve numerical computation capability (Lu et al., 2023b; Schick et al., 2023).\n",
            "https://arxiv.org/pdf/2307.10635\n",
            "- In the Physical Chemistry example problem requiring use of the Planck distribution to compare energy outputs at 450 nm and 700 nm at 298 K, GPT-4 with Chain-of-Thought prompting generated the correct formula but made errors in the final numerical calculation.\n",
            "- For the same problem, when GPT-4 was instructed to generate Python code alongside CoT reasoning, it misplaced Œª1 in the numerator rather than the denominator in the formula, indicating a misunderstanding of the mathematical relationships when employing external tools.\n",
            "- SCIBENCH comprises 869 open-ended, free-response college-level scientific problems across Chemistry, Physics, and Mathematics that require multi-step reasoning, conceptual understanding, domain-specific knowledge retrieval, and complex numeric computation.\n",
            "- SCIBENCH includes a multimodal subset of 177 problems incorporating visual elements (graphs and figures) to enable evaluation of multimodal LLMs.\n",
            "- A separate closed dataset encompasses 103 problems drawn from seven sets of midterm and final exams from collegiate Computer Science and Math courses.\n",
            "- SCIBENCH provides step-by-step solutions for example problems to enable detailed error analysis.\n",
            "- The datasets were manually extracted from PDF documents and formatted into LaTeX to minimize the risk of training data leakage.\n",
            "https://arxiv.org/pdf/2307.10635\n",
            "- On SCIBENCH, even with the strongest configuration (combining chain-of-thought prompting and external tools), the best model achieves average scores of 43.22% on the textual dataset, 13.8% on the multimodal dataset, and 51.57% on the closed exam dataset.\n",
            "- Experimental results indicate the dataset‚Äôs complexity and difficulty are sufficient to differentiate performance levels across different LLMs.\n",
            "- Analysis shows that chain-of-thought prompting significantly improves calculation ability but is less effective for other skill dimensions.\n",
            "- Prompts that use external tools can compromise other fundamental skills.\n",
            "- Few-shot learning does not universally improve scientific problem-solving skills.\n",
            "https://arxiv.org/pdf/2307.10635\n",
            "- Existing LLM benchmarks for lower educational levels predominantly focus on basic arithmetic operations rather than advanced mathematical computations.\n",
            "- Most existing benchmarks are textual-only and omit visual elements such as graphs or diagrams.\n",
            "- In the SCIBENCH textbook dataset, the Fundamentals of Physics subset contains 142 problems, with 9.2% including detailed solutions and 43.0% including visual elements (Halliday et al., 2013).\n",
            "- In the SCIBENCH textbook dataset, the Statistical Thermodynamics subset contains 83 problems, with 20.5% including detailed solutions and 0.0% including visual elements (Engel & Reid, 2010).\n",
            "- In the SCIBENCH textbook dataset, the Classical Dynamics of Particles and Systems subset contains 66 problems, with 12.1% including detailed solutions and 4.5% including visual elements (Thornton & Marion, 2021).\n",
            "- In the SCIBENCH textbook dataset, the Calculus: Early Transcendentals subset contains 161 problems, with 19.3% including detailed solutions and 67.7% including visual elements (Stewart et al., 2012).\n",
            "- In the SCIBENCH textbook dataset, the Probability and Statistical Inference subset contains 93 problems, with 21.5% including detailed solutions and 1.1% including visual elements (Hogg et al., 1977).\n",
            "- In the SCIBENCH textbook dataset, the Elementary Differential Equations and Boundary Value Problems subset contains 55 problems, with 9.1% including detailed solutions and 0.0% including visual elements (Boyce et al., 2021).\n",
            "https://arxiv.org/pdf/2307.10635\n",
            "- On the SCIBENCH textbook dataset in the zero-shot setting, GPT-4-Turbo achieved an average accuracy of 40.99%, outperforming GPT-4 at 33.79% and Mistral-7B at 6.23%; GPT-4-Turbo outperformed Mistral-7B by 34.76 percentage points.\n",
            "- In few-shot Chain-of-Thought prompting on the textbook dataset, GPT-4-Turbo achieved 39.45% average accuracy versus GPT-4 at 30.36%.\n",
            "- LLaMA-2-70B improved from 2.41% average accuracy (zero-shot) to 8.40% (few-shot) on the textbook dataset.\n",
            "- Incorporating external tools improved GPT-4‚Äôs textbook performance from 30.36% (few-shot CoT) to 43.22%.\n",
            "- Despite superior zero-shot performance, GPT-4-Turbo underperformed GPT-4 in the few-shot setting when leveraging Python for numerical computation.\n",
            "- On the multimodal subset, proprietary GPT-4 augmented with image captions and OCR-detected text outperformed open-source LMMs; specifically, GPT-4 with Program-of-Thoughts achieved 13.8% accuracy versus 7.4% for the best open model LLaVA-LLaMA-2-13B.\n",
            "- On the closed exam subset, GPT-4 achieved an averaged score of 57.54%; in the Data Mining course, GPT-4 scored 64.44% on the midterm and 42.67% on the final, below average student scores of 80.18% and 72.71%, respectively.\n",
            "- In the automated skill-classification verification process for error analysis, approximately 20% of incorrectly classified skills were discarded after human scrutiny.\n",
            "https://arxiv.org/pdf/2307.10635\n",
            "- In zero-shot settings, Chain-of-Thought (CoT) prompting reduces calculation-error rates to 13.6% compared to 29.0% with the vanilla zero-shot baseline.\n",
            "- Zero-shot CoT increases errors in other skills: causal reasoning error rate rises to 32.2% and logical decomposition to 25.4%, versus 18.3% and 18.3% in zero-shot without CoT.\n",
            "- An observed case shows zero-shot without CoT produced the correct formula but failed in calculation steps, while CoT misinterpreted problem conditions and failed to use the correct formula.\n",
            "- Using external tools reduces calculation errors compared to few-shot CoT, decreasing from 14.5% to 6.2%, but weakens other skills, particularly code conversion (program generation for solutions).\n",
            "- Few-shot CoT is not universally beneficial: it reduces causal-reasoning errors by 12.8% but increases errors in other skills such as logical decomposition.\n",
            "https://arxiv.org/pdf/2307.10635\n",
            "On the SCIBENCH exam dataset under zero-shot settings (with and without chain-of-thought), GPT-4 achieved the highest scores in 6 of 7 exams‚Äî58/90, 44/75, 40/56, 50/100, 80/100, and 25/95‚Äîwhile Claude2 attained the top score in one exam with 41/75; LLaMA-2-7B scored substantially lower (e.g., 24/90, 14/75, 6/56, and 0/100 on some exams). Additionally, using Wolfram Language as an external tool led to a 46.9% code conversion error rate, predominantly due to syntax errors and reserved-symbol violations, indicating that external tool use can weaken code conversion skills.\n",
            "https://arxiv.org/pdf/2307.10635\n",
            "In a human evaluation of the model verifier‚Äôs error classifications, two annotators reviewed 151 samples across different settings and judged 123 to be correctly classified, yielding an accuracy of 81.45%.\n",
            "https://arxiv.org/pdf/2307.10635\n",
            "- In Figure S9, for the de Broglie wavelength problem, the ChatGPT solution using chain-of-thought was classified as error category 4 (Causal Reasoning) and produced 8.09 pm, whereas the non-chain-of-thought solution was classified as error case 10 (Calculation Skills) and produced 3.31 pm; the correct value is 243 pm.\n",
            "- In Figure S10, for the cardioid length problem, the model produced 32 instead of the correct 8, and the error reason was categorized as 10 (Calculation Skills).\n",
            "- In Figure S11, for Simpson‚Äôs Rule with n = 10, the model returned a Wolfram Language error message (‚ÄúObjects of unequal length...‚Äù), and the error category was 8 (Code Conversion Skills).\n",
            "- In Figure S12, for finding H√ºckel molecular orbitals for ethene, the model asserted visual representations were required and provided no solution; the error category was 3 (Spatial Perception).\n",
            "- In Figure S13, for the mean speed of N2 at 25¬∞C, the model computed 515 m s‚Åª¬π using the rms formula; the error category was 7 (Scientific Literacy); the correct value is 475 m s‚Åª¬π.\n",
            "- In Figure S14, for four-letter code words with all letters different, the model answered 456,976 (26‚Å¥) instead of the correct 358,800; the error category was 9 (Logical Reasoning).\n",
            "- In Figure S15, for ethene H√ºckel molecular orbitals, the model claimed two bonding and two antibonding orbitals and returned ‚ÄúNone‚Äù as final answer; the error category was 6 (Abstract Reasoning); the correct normalization constant is 0.70710678.\n",
            "- In Figure S16, for the weight of air in a room, the model computed 101,640 N using the ideal gas law; the error category was 2 (Identification of Assumptions); the correct value is 418 N.\n",
            "- In Figure S17, for the probability that the third spade appears on the sixth draw, the model answered 0.258 instead of the correct 0.064; the error category was 4 (Causal Reasoning).\n",
            "- In Figure S18, for the CO2 cylinder usage time, the model answered 24.1 days; the error category was 5 (Problem Deduction Skills); the correct duration is 52 days.\n",
            "- In Figure S19, for the gravitational acceleration difference between an astronaut‚Äôs feet and head in orbit, the model answered 1.0 √ó 10‚Åª‚Åµ m s‚Åª¬≤; the error category was 1 (Logical Decomposition and Analysis Skills); the correct difference is ‚àí4.37 √ó 10‚Åª‚Å∂ m s‚Åª¬≤.\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- Early symbolic and heuristic AI systems were able to simulate elements of scientific reasoning (e.g., rediscovering physical laws and inferring causal structures), but their scalability and adaptability to unstructured data were limited.\n",
            "- Large language models have demonstrated proficiency in extracting meaningful relationships from unstructured text, facilitating hypothesis discovery in biomedical research and materials science.\n",
            "- Agentic AI systems such as The AI Scientist and SciAgents autonomously perform significant parts of the scientific process, including experimental validation and manuscript drafting.\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "AlphaFold has revolutionized protein structure prediction, resolving key bottlenecks in drug discovery and expediting therapeutic innovation (Jumper et al. [2021]). Crispr-GPT streamlines the design of gene-editing experiments, reducing cognitive and procedural burdens on researchers while accelerating the pace of scientific advancement (Huang et al. [2024]). MOLIERE demonstrates that text mining and biomedical knowledge graphs can aid hypothesis validation by retrospectively testing hypotheses against historical data (Sybrandt et al. [2017, 2018]).\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- LLM-powered agentic AI systems automate routine scientific tasks such as data analysis, hypothesis formulation, and literature synthesis, enabling researchers to focus on more complex work (Paul et al. [2024], White [2024], Chan et al. [2023], Sulc et al. [2024], Qiu and Lan [2024]).\n",
            "- A comprehensive survey observes that agentic systems are deployed across chemistry, biology, and materials science; operate across the full research lifecycle (ideation, literature review, experimentation, scientific writing); and distinguishes autonomous vs. collaborative frameworks, highlighting human-AI collaboration and system calibration as key directions (Gridach et al. [2025]).\n",
            "- Domain-specific repositories and real-time feedback loops in tools such as SciAgents, AI Co-Scientist, and RL-driven materials discovery frameworks enhance the contextual relevance and applicability of generated hypotheses (Ghafarollahi and Buehler [2024b], Gottweis et al. [2025], Gruver et al. [2024]).\n",
            "- LLM-integrated scientific agents have been successfully applied to discover faster matrix multiplication algorithms and to causal inference in biomedicine, demonstrating cross-domain versatility (Fawzi et al. [2022], Jha et al. [2019]).\n",
            "- Documented limitations include over-reliance on pre-existing data, limited novelty of generated hypotheses, and ethical risks in high-stakes domains such as healthcare (Tang et al. [2024], Shavit et al. [2023]).\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- LLMs trained on historically grounded corpora exhibit regression to the mean in idea generation, prioritizing statistically likely continuations, producing variants of well-known ideas, and rarely proposing counterfactual or unconventional hypotheses Zhou et al. [2024], Ghafarollahi and Buehler [2024b], Fok and Weld [2024], Jha et al. [2019].\n",
            "- Simulation tools provide scalable virtual environments that can pre-test hypotheses effectively and have been especially useful in biomedical research and materials science, where experimental verification can be resource-intensive Qi et al. [2024], Schumann et al. [2024], Gruver et al. [2024].\n",
            "- Reinforcement learning with novelty-seeking reward signals has shown promise in promoting exploration of low-probability, high-impact hypotheses Gruver et al. [2024], Blanco-Gonzalez et al. [2023].\n",
            "- Autonomous robotic platforms have physically conducted experiments based on model-generated hypotheses; notably, the \"robot scientist\" Eve was used in drug discovery to select compounds, operate lab equipment, and analyze results, closing the loop between hypothesis generation and empirical testing King [2011], Fakhruldeen et al. [2022], Williams et al. [2015].\n",
            "- Simulation environments like ChemBench provide scalable virtual platforms for early-stage hypothesis testing Walker et al. [2010].\n",
            "- In biomedical and materials science, digital twin simulations emulate real-world experimental settings and offer higher-fidelity feasibility assessments Fawzi et al. [2022], Tang et al. [2024].\n",
            "- LLMs trained on biased data may reproduce and amplify social, demographic, and epistemic inequities Shavit et al. [2023], Weidinger et al. [2021].\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "Large language models have been observed to generate outputs that appear fluent and scientifically plausible yet are factually incorrect or unsupported by evidence (AI hallucination).\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- Tools such as MOLIERE for biomedical hypothesis validation and SciAgents for structured scientific discovery have demonstrated the efficacy of constrained knowledge generation (Sybrandt et al. [2018]; Ghafarollahi and Buehler [2024b]).\n",
            "- Studies report that LLMs may generate syntactically correct hypotheses that lack empirical feasibility (Shavit et al. [2023]; Aubin Le Qu√©r√© et al. [2024]).\n",
            "- Retrieval-augmented generation and knowledge graph-based reasoning have been employed to structure open-domain hypothesis generation, helping maintain scientific plausibility while preserving creativity (Chen et al. [2024]; Jha et al. [2019]).\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- PubMed Abstracts contains over 34 million abstracts. [2025]\n",
            "- MeSH comprises 27,883 descriptors for categorizing biomedical content. [2025]\n",
            "- ChEMBL includes over 2 million bioactive compounds. [2023]\n",
            "- The GENIA Corpus has over 2,000 abstracts annotated with biomedical terms. [2003]\n",
            "- The Open Graph Benchmark offers over 100 datasets for graph-based tasks. [2020]\n",
            "- UK Biobank includes data from over 500,000 participants. [2023]\n",
            "- MATBench contains over 100,000 material samples. [2020]\n",
            "- ClimateNet spans over 10 years of climate observation data. [2020]\n",
            "- The COCO dataset contains over 300,000 images with captions. [2014]\n",
            "- Gene Ontology comprises over 44,000 terms across biological process, molecular function, and cellular component. [2000]\n",
            "- The AHTech Electrolyte Additive dataset covers 180 candidates with 200-cycle efficiency measured per sample. [2025]\n",
            "- CSKG-600 contains 600 hypothesis statements with expert validation. [2025]\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- DrugBank contains over 14,000 drugs and 6,000 protein targets.\n",
            "- AI2 Science Questions contains over 10,000 multiple-choice science questions.\n",
            "- Materials Project includes over 133,500 materials.\n",
            "- KEGG enumerates 540 pathways across species.\n",
            "- American Community Survey (ACS) collects about 2.5 million responses per year.\n",
            "- USPTO Patent Data includes over 10 million granted patents.\n",
            "- XSum comprises 226,711 summaries.\n",
            "- Cosmic contains somatic mutation data spanning 30,000 genes and 2 million mutations.\n",
            "- ORKG includes 3 million triples of structured research contributions.\n",
            "- ChEMBL includes over 2 million molecules with bioactivity data, covering more than 14,000 biological targets.\n",
            "- GENIA Corpus comprises over 2,000 annotated abstracts with more than 36,000 unique terms, focusing on biomedical entities (proteins, genes, transcription factors, cellular signaling).\n",
            "- Open Graph Benchmark (OGB) is a collection of over 100 graph datasets and provides predefined training/validation/test splits.\n",
            "- UK Biobank contains data from 500,000 participants covering over 800 phenotypic traits and 96 million genetic variants.\n",
            "- MATBench features over 100,000 material samples across alloys, ceramics, and polymers.\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- Prior studies report that LLMs can automate bioinformatics workflows when paired with structured repositories such as cBioPortal. \n",
            "- The AHTech platform integrates automated electrochemical experimentation with machine learning and enables high-throughput hypothesis testing in battery research. \n",
            "- Modular systems modeling frameworks‚Äîincluding Robotics-LLM and proteomics-based KDD pipelines‚Äîfacilitate hypothesis refinement in chemical discovery and oncology, respectively. \n",
            "- Hybrid pipelines that combine LLM-driven generation with symbolic or simulation-based validation are increasingly being adopted to balance generative flexibility with interpretability and rigor. \n",
            "- LLMs often lack formal mechanisms for explanation, causality, and logical rigor, necessitating downstream validation via simulation, symbolic reasoning, or expert review.\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- The MOLIERE system, leveraging PubMed-scale repositories, identified novel gene‚Äìdisease associations that conventional analyses often failed to detect.\n",
            "- The SciAgents framework demonstrated that integrating dynamic knowledge graphs with large language models enables interdisciplinary hypothesis generation in domains such as pharmacology.\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- Knowledge graphs uncovered correlations between oceanic and atmospheric variables that enabled modeling of ecological phenomena.\n",
            "- In biomedicine, knowledge graphs and ontology-based reasoning mapped intricate relationships between genes, diseases, and drugs, facilitating advances in personalized medicine and rare disease research.\n",
            "- In materials science, these methods accelerated the discovery of novel materials by identifying promising chemical combinations.\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- Integrating ocean salinity data with atmospheric pressure measurements has yielded previously unattainable insights into climate dynamics (Wang et al. [2023]).\n",
            "- VirtualPlant integrates genomic, transcriptomic, and phenotypic data and has uncovered genetic pathways that enhance crop resistance to environmental stressors (Katari et al. [2010]; Qi et al. [2024]; Kim and Segev [2018]).\n",
            "- BioLunar uses multi-omics data to elucidate complex gene‚Äìprotein interactions, enabling the discovery of precision therapies and accelerating identification of drug‚Äìprotein interactions (Wysocki et al. [2024]; Kim and Segev [2018]; Qi et al. [2024]).\n",
            "- Climate KG links climate variables with ecological and biological datasets to uncover factors driving ecosystem resilience, informing conservation and adaptation strategies (Wu et al. [2022]).\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- Dynamic retrieval in RAG enables extraction of up-to-date information from diverse sources, enriching hypotheses with the latest knowledge Beltagy et al. [2019].\n",
            "- Carefully engineered prompts guide LLMs to generate contextually specific outputs that address discipline-specific challenges Kim and Segev [2018].\n",
            "- Memory integration in RAG, by incorporating feedback from previous outputs, fosters coherence and novelty Qi et al. [2024].\n",
            "- Model-free RL (e.g., Q-learning) facilitates hypothesis generation in environments with complex or poorly understood dynamics Gruver et al. [2024].\n",
            "- Multi-agent RL systems collaboratively explore diverse hypothesis spaces and significantly increase exploration efficiency Zhou et al. [2024].\n",
            "- VELMA integrates RAG with navigation and robotics, using textual and visual data to hypothesize optimal urban navigation strategies Schumann et al. [2024].\n",
            "- Chemist-X applies RAG to chemical databases to generate reaction pathways and identify novel drug candidates Chen et al. [2024].\n",
            "- RL-Discovery optimizes material properties and accelerates advancements in catalyst design by exploring hypothesis spaces Gruver et al. [2024].\n",
            "- DrugRL accelerates molecular structure design in drug discovery, reducing costs and timelines via iterative refinement of candidate molecules Qi et al. [2024].\n",
            "- SciAgents integrates RL and knowledge graph approaches to enhance hypothesis validation across interdisciplinary fields Ghafarollahi and Buehler [2024b].\n",
            "- RAG tools like SciAgents facilitate interdisciplinary queries, enabling researchers to tackle biomedical and computational challenges through systematic hypothesis refinement Ghafarollahi and Buehler [2024b].\n",
            "- Poorly curated retrieval sources constrain the novelty of hypotheses in RAG systems Beltagy et al. [2019].\n",
            "- Poorly defined reward functions in RL lead to suboptimal exploration Fawzi et al. [2022].\n",
            "- Topic modeling (e.g., LDA) clusters related information into coherent themes, revealing underlying patterns in large text corpora Beltagy et al. [2019].\n",
            "- Dynamic evolution models capture temporal shifts in research priorities and conceptual frameworks by analyzing changes in text representations Qi et al. [2024].\n",
            "- Dyport combines text mining with dynamic graph modeling to uncover temporal trends and co-occurrence networks in genomics, enabling identification of novel gene‚Äìdisease associations Tyagin and Safro [2024].\n",
            "- Dyport has analyzed temporal trends in gene-variant literature, revealing emerging links to rare diseases and prioritizing research directions Tyagin and Safro [2024].\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- SimHypoth employed neural network models to uncover non-linear relationships and generated hypotheses that included correlations between demographic factors and societal trends (Lin and Lucas [2023]).\n",
            "- IdeaFlow tracked the diversity of ideas from simulations and enabled the discovery of innovative design strategies and material combinations that advanced sustainable infrastructure, including novel approaches to bridge and building construction (Utley and Klebahn [2022]).\n",
            "- IdeaFlow proposed novel material combinations in construction that had not been previously considered (Kim and Segev [2018]).\n",
            "- GenCreative used GANs to explore unconventional solutions to unsolved problems‚Äîfrom subatomic particle interactions to alternative theoretical frameworks‚Äîexpanding both theoretical inquiry and experimental frontiers (de Oliveira et al. [2017]).\n",
            "- SciAgents accelerated the identification of disease biomarkers and drug-target interactions by integrating expert feedback, contributing to faster development of diagnostic and therapeutic solutions (Ghafarollahi and Buehler [2024b]).\n",
            "- CrowdScience facilitated collaborative generation of hypotheses on behavioral trends and social correlations, incorporating diverse stakeholder insights to uncover novel patterns (Zhou et al. [2024]).\n",
            "- ExplanatoryAI advanced policy development and regulatory frameworks by producing interpretable and actionable hypotheses that aid decision-makers in evaluating potential impacts (Shavit et al. [2023]).\n",
            "- Incorporating generative adversarial techniques to balance human feedback with machine-generated alternatives fostered broader and more creative exploration of hypotheses (Beltagy et al. [2019]).\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- Analyses using causal inference frameworks have identified unexpected treatment effects in biomedical datasets, challenging existing paradigms and opening new research avenues (Jha et al. [2019]).\n",
            "- Incomplete datasets and the presence of latent variables limit the capacity of causal inference frameworks to generate innovative hypotheses, underscoring the need for robust data collection and preprocessing (Shavit et al. [2023]).\n",
            "- Integrating causal inference with machine learning (e.g., deep generative models) enables simultaneous capture of causal and non-causal patterns, broadening the scope of hypothesis generation while maintaining scientific rigor (Beltagy et al. [2019]).\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "VirSci, a large-scale LLM-based multi-agent system, achieved significant gains in generating original research ideas compared to single-agent and prior multi-agent baselines.\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- Early symbolic and heuristic discovery systems could simulate elements of scientific reasoning (e.g., rediscovering physical laws and inferring causal structures), but their scalability and adaptability to unstructured data were limited.\n",
            "- Large Language Models (LLMs) trained on expansive corpora can synthesize diverse datasets, identify latent patterns, and accelerate hypothesis generation.\n",
            "- LLMs have demonstrated the ability to extract meaningful relationships from unstructured text that facilitate hypothesis discovery in domains such as biomedical research and materials science.\n",
            "- Agentic AI frameworks (e.g., The AI Scientist, SciAgents) autonomously perform parts of the scientific process, including experimental validation and manuscript drafting.\n",
            "- Incorporating retrieval-augmented generation, knowledge graphs, and causal inference enables generation of testable, interdisciplinary hypotheses and systematic mapping of cross-domain connections that humans may overlook.\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- AlphaFold has revolutionized protein structure prediction, resolving key bottlenecks in drug discovery and expediting therapeutic innovation. [Jumper et al. [2021]]\n",
            "- Crispr-GPT streamlines the design of gene-editing experiments, reducing researchers‚Äô cognitive and procedural burdens and accelerating scientific advancement. [Huang et al. [2024]]\n",
            "- MOLIERE demonstrates that text mining and biomedical knowledge graphs can aid hypothesis validation by retrospectively testing hypotheses against historical data. [Sybrandt et al. [2017, 2018]]\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- Simulation tools provide scalable virtual environments that can effectively pre-test scientific hypotheses. Qi et al. [2024], Schumann et al. [2024].\n",
            "- Simulation-based pre-testing methods have proven especially useful in biomedical research and materials science, where experimental verification is resource-intensive. Gruver et al. [2024].\n",
            "- Reinforcement learning with novelty-seeking reward signals has shown promise in promoting exploration of low-probability, high-impact hypotheses. Gruver et al. [2024], Blanco-Gonzalez et al. [2023].\n",
            "- In data-scarce scientific domains, synthetic data generation and few-shot learning can supplement existing datasets to improve generalization. Wang et al. [2023], Chen et al. [2024].\n",
            "- High-risk, potentially transformative hypotheses are often penalized by conventional validation metrics that favor incremental advances. Shavit et al. [2023], Fok and Weld [2024].\n",
            "- Robotic platforms integrated with models have been used to physically conduct experiments based on model-generated hypotheses, enabling real-time feedback and refinement; for example, the ‚Äúrobot scientist‚Äù Eve was used in drug discovery to select compounds, operate lab equipment, and analyze results, closing the loop between hypothesis generation and empirical testing. King [2011], Fakhruldeen et al. [2022], Williams et al. [2015].\n",
            "- Digital twin simulations offer higher-fidelity feasibility assessments by emulating real-world experimental settings. Fawzi et al. [2022], Tang et al. [2024].\n",
            "- Simulation environments such as ChemBench provide scalable virtual platforms for early-stage hypothesis testing. Walker et al. [2010].\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "Large language models have been observed to generate hallucinated outputs‚Äîfluent and scientifically plausible text that is factually incorrect or unsupported‚Äîand, without transparent reasoning or source attribution, such outputs can mislead researchers and corrupt hypothesis evaluation pipelines.\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- Open-domain hypothesis generation is more challenging to validate and assess for relevance, despite enabling novel interdisciplinary insights (observed in prior work) [Sybrandt et al. [2018], Wang et al. [2023]].\n",
            "- Retrieval-augmented generation and knowledge graph-based reasoning have been used to structure open-domain hypothesis generation, helping keep hypotheses scientifically plausible while preserving creativity [Chen et al. [2024], Jha et al. [2019]].\n",
            "- LLMs often generate syntactically correct hypotheses that lack empirical feasibility [Shavit et al. [2023], Aubin Le Qu√©r√© et al. [2024]].\n",
            "- In closed-domain settings, RAG and fine-tuned foundation models have been employed to improve domain-specific accuracy [Chen et al. [2024], Wang et al. [2023]].\n",
            "- Tools such as MOLIERE (for biomedical hypothesis validation) and SciAgents (for structured scientific discovery) demonstrate the efficacy of constrained knowledge generation [Sybrandt et al. [2018], Ghafarollahi and Buehler [2024b]].\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- Contrastive learning techniques enable LLMs to refine hypotheses by balancing novelty and plausibility, reducing the generation of implausible or irrelevant ideas.\n",
            "- Human-in-the-loop validation allows domain experts to iteratively refine LLM-generated hypotheses so that AI-driven scientific discoveries align with empirical research priorities.\n",
            "- Earlier studies have employed percentile-based thresholds derived from baseline distributions to set domain-specific cutoffs for novelty and feasibility.\n",
            "- The PubMed dataset contains over 34 million abstracts and citations across clinical medicine, pharmacology, and molecular biology.\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- The PubMed Abstracts database contains over 34 million abstracts.\n",
            "- MeSH comprises 27,883 descriptors.\n",
            "- ChEMBL contains over 2 million bioactive compounds.\n",
            "- The GENIA Corpus includes over 2,000 abstracts annotated with biomedical terms.\n",
            "- The Open Graph Benchmark includes over 100 datasets for benchmarking graph-based tasks.\n",
            "- UK Biobank contains data from over 500,000 participants.\n",
            "- MATBench includes over 100,000 material samples.\n",
            "- ClimateNet provides over 10 years of climate observation data.\n",
            "- The COCO dataset includes over 300,000 images with captions.\n",
            "- Gene Ontology comprises over 44,000 terms across biological process, molecular function, and cellular component.\n",
            "- The AHTech Electrolyte Additive Dataset contains 180 candidates with 200-cycle efficiency measured per sample.\n",
            "- CSKG-600 includes 600 hypothesis statements with expert validation.\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- DrugBank contains over 14,000 drugs and 6,000 protein targets for drug-target interaction studies.\n",
            "- The AI2 Science Questions dataset contains over 10,000 multiple-choice science questions.\n",
            "- The Materials Project database includes over 133,500 materials.\n",
            "- KEGG enumerates 540 metabolic and signaling pathways across species.\n",
            "- The American Community Survey (ACS) collects approximately 2.5 million responses per year.\n",
            "- The USPTO patent dataset contains over 10 million granted patents.\n",
            "- The XSum dataset comprises 226,711 summaries.\n",
            "- The COSMIC resource includes somatic mutation data spanning 30,000 genes and 2 million mutations.\n",
            "- The Open Research Knowledge Graph (ORKG) contains about 3 million triples describing research contributions.\n",
            "- The GENIA Corpus comprises over 2,000 biomedical abstracts annotated with more than 36,000 unique terms.\n",
            "- The Open Graph Benchmark (OGB) aggregates a collection of over 100 graph datasets across multiple domains.\n",
            "- The UK Biobank includes data from 500,000 participants, covering over 800 phenotypic traits and 96 million genetic variants.\n",
            "- MATBench contains over 100,000 material samples for material property prediction tasks.\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- The AHTech Electrolyte Additive dataset contains high-throughput screening measurements for 180 small-molecule electrolyte additives tested in aqueous zinc metal batteries, with each additive characterized over 200 electrochemical cycles to determine Coulombic efficiency (Lin et al. [2025]).\n",
            "- CSKG-600 comprises 600 candidate hypotheses that were manually labeled by domain experts as valid or invalid (Borrego et al. [2025]).\n",
            "- DrugBank includes data on over 14,000 drugs and 6,000 protein targets (Wishart et al. [2018]).\n",
            "- The AI2 Science Questions dataset contains over 10,000 multiple-choice questions spanning science topics (Clark et al. [2018]).\n",
            "- KEGG Pathway provides 540 curated pathways covering metabolic, regulatory, and signaling processes (Kanehisa and Goto [2000]).\n",
            "- The American Community Survey covers approximately 2.5 million U.S. households annually and contains over 35,000 variables (Bureau [2025]).\n",
            "- The USPTO patent dataset includes over 10 million granted patents with metadata such as inventor details, filing dates, and citations (Patent and Office [2025]).\n",
            "- XSum comprises 226,711 news articles paired with single-sentence human-written summaries across diverse topics (Narayan et al. [2018]).\n",
            "- Cosmic contains data on over 30,000 genes and 2 million somatic mutations, annotated with pathways, phenotypes, and clinical outcomes (Bamford et al. [2004]).\n",
            "- ORKG includes over 3 million triples linking research entities across disciplines (Jaradeh et al. [2019]).\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "Tools such as BACON and KEKADA have empirically rediscovered known scientific laws and relationships when applied to structured datasets.\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- LLMs often lack formal mechanisms for explanation, causality, and logical rigor, so their outputs require downstream validation via simulation, symbolic reasoning, or expert review [Wang et al. [2024b]].\n",
            "- Hybrid pipelines that combine LLM-driven generation with symbolic or simulation-based validation are being adopted to balance generative flexibility with interpretability and rigor [Langley [2024]; Ghafarollahi and Buehler [2024a]; Ren et al. [2025]].\n",
            "- In battery research, the AHTech platform integrates automated electrochemical experimentation with machine learning to enable high-throughput hypothesis testing [Lin et al. [2025]].\n",
            "- When paired with structured repositories like cBioPortal, LLMs have been shown to automate bioinformatics workflows [Ji et al. [2024]].\n",
            "- Modular systems modeling frameworks‚Äîsuch as Robotics-LLM and proteomics-based KDD pipelines‚Äîfacilitate hypothesis refinement in chemical discovery and oncology [Yin et al. [2025]; Resell et al. [2025]].\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "MOLIERE, leveraging knowledge graphs built from PubMed, identified novel gene‚Äìdisease associations that often elude conventional analysis.\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- Prompting strategies and architectural mechanisms can enable LLMs to emulate knowledge graph behavior by performing multi-hop reasoning and generating structured triples directly from natural language inputs (Kau et al. [2024]).\n",
            "- Link prediction using metrics such as embeddings, centrality, and similarity has been particularly impactful in drug discovery by identifying new drug‚Äìdisease interactions that can expedite therapeutic development (Kim and Segev [2018]).\n",
            "- Graph Neural Networks applied to knowledge graphs model intricate, high-dimensional relationships and excel at revealing latent patterns and dependencies, enabling analysis of multi-layered interactions in domains such as materials science and environmental modeling (Bai et al. [2024a]; Beltagy et al. [2019]).\n",
            "- Modern knowledge graphs can dynamically incorporate newly available data to refine their structure and remain relevant in rapidly evolving scientific contexts (Qi et al. [2024]).\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- Integration of multiple ontologies has unified knowledge across physics, biology, and chemistry, facilitating interdisciplinary research. Wang et al. [2023]\n",
            "- Ontology mapping aligns diverse ontologies to enable unified views essential for interdisciplinary research. Kim and Segev [2018]\n",
            "- Automated reasoning tools can derive novel hypotheses while maintaining consistency with existing knowledge structures. Beltagy et al. [2019]\n",
            "- Knowledge graphs excel at uncovering patterns and relationships hidden in large, unstructured datasets, while ontology-based reasoning ensures hypothesis precision and semantic consistency. Bai et al. [2024b]; Liu et al. [2010]\n",
            "- These techniques have shown utility across domains, including identifying novel drug targets and exploring complex ecological systems. Ji et al. [2024]; BlancoGonzalez et al. [2023]; Zhang et al. [2024b]; Qi et al. [2024]\n",
            "- In materials science, these methods have accelerated discovery of novel materials by identifying promising chemical combinations. Ghafarollahi and Buehler [2024a]\n",
            "- In environmental science, knowledge graphs have uncovered correlations between oceanic and atmospheric variables to model ecological phenomena. Wang et al. [2023]\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- Integrating genomic, transcriptomic, and phenotypic data in agriculture has uncovered genetic pathways that enhance crop resistance to environmental stressors, supporting sustainable farming practices.\n",
            "- Leveraging multi-omics data in pharmacology elucidates complex gene-protein interactions, enabling precision therapy discovery and accelerating identification of drug-protein interactions.\n",
            "- Linking climate variables with ecological and biological datasets uncovers factors that drive ecosystem resilience, informing conservation and adaptation strategies.\n",
            "- Integrating ocean salinity data with atmospheric pressure measurements has produced previously unattainable insights into climate dynamics.\n",
            "- Resource constraints and dataset heterogeneity are observed to hinder the full realization of data-driven integration‚Äôs potential.\n",
            "- Employing adaptive algorithms, reinforcement learning, and dynamic systems modeling improves computational efficiency and enables exploration of deeper, non-obvious correlations within interdisciplinary datasets.\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- DrugRL has accelerated the design of molecular structures in drug discovery, reducing costs and timelines by iteratively refining candidate molecules.\n",
            "- RL-Discovery has optimized catalysts and identified compounds with unique properties that address critical performance criteria.\n",
            "- Dyport‚Äôs analysis of temporal trends in gene-variant literature revealed emerging links to rare diseases and helped prioritize research directions.\n",
            "- Chemist-X, applying RAG to chemical databases, generated reaction pathways and identified novel drug candidates.\n",
            "- SciAgents, integrating RL and knowledge graphs, facilitated interdisciplinary queries and enhanced hypothesis validation across biomedical and computational domains.\n",
            "- VELMA demonstrated RAG integration with navigation and robotics, using textual and visual data to hypothesize optimal urban navigation strategies.\n",
            "- Poorly curated data sources in RAG constrain the novelty of generated hypotheses.\n",
            "- Poorly defined reward functions in RL lead to suboptimal exploration.\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- SimHypoth used neural-network models to uncover non-linear relationships and correlations between demographic factors and societal trends in social-science contexts. \n",
            "- IdeaFlow‚Äôs simulations led to discovery of innovative design strategies and material combinations that advanced sustainable infrastructure, including novel approaches to bridge and building construction. \n",
            "- IdeaFlow proposed novel construction material combinations that had not been previously considered. \n",
            "- GenCreative‚Äôs GAN-based exploration produced unconventional solutions spanning subatomic particle interactions and alternative theoretical frameworks, expanding both theoretical inquiry and experimental frontiers. \n",
            "- SciAgents, by integrating real-time expert feedback, accelerated identification of disease biomarkers and drug-target interactions, contributing to faster development of diagnostic and therapeutic solutions in biomedical research. \n",
            "- CrowdScience‚Äôs crowdsourced process facilitated collaborative hypothesis generation on behavioral trends and social correlations and uncovered novel patterns through diverse stakeholder input. \n",
            "- ExplanatoryAI advanced policy development and regulatory frameworks by generating interpretable and actionable hypotheses that aided decision-makers in evaluating potential impacts. \n",
            "- Incorporating generative adversarial techniques balanced human feedback with machine-generated alternatives, fostering broader and more creative exploration of hypotheses.\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- Causal inference frameworks have identified unexpected treatment effects in biomedical datasets, challenging existing paradigms and opening new research avenues. [Jha et al. 2019]\n",
            "- Incomplete datasets or the presence of latent variables can limit the capacity of causal inference frameworks to generate innovative hypotheses. [Shavit et al. 2023]\n",
            "- Integrating causal inference with deep generative models allows for the simultaneous capture of causal and non-causal patterns, broadening hypothesis generation while maintaining scientific rigor. [Beltagy et al. 2019]\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- VirSci, a large-scale LLM-based multi-agent system that simulates real-world scientific collaboration, achieved significant gains in generating original research ideas compared to single-agent and prior multi-agent baselines. [Su et al. [2024]]\n",
            "- Communication and coordination overhead in multi-agent systems can hinder scalability and reduce the system‚Äôs ability to address highly complex problems. [Zhou et al. [2024]]\n",
            "- Advanced negotiation protocols and enhanced knowledge-sharing mechanisms foster deeper interdisciplinary interactions, promoting the generation of more innovative and robust hypotheses. [Beltagy et al. [2019]]\n",
            "- In materials science, a MAS framework enables agents to explore chemical spaces and identify catalysts with unique properties, generating hypotheses that balance chemical feasibility with performance metrics and accelerating the discovery of high-performance materials. [Park et al. [2024]]\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "Automated microscopy systems in experimental biology have been observed to enable real-time cellular imaging and validation while reducing manual errors and increasing experimental throughput.\n",
            "https://arxiv.org/pdf/2505.04651\n",
            "- Simulating a computational biological model, rather than reading, elicited measurable changes in brain activity during biological reasoning.\n",
            "- Target-assisted iterative screening (TAIS) of the BIR3-cIAP1 domain revealed an unconventional IAP-binding motif.\n",
            "https://arxiv.org/pdf/2503.24047\n",
            "Prior LLM-based multi-modal agents commonly include a separate perceptron module to handle multi-modal inputs.\n",
            "https://arxiv.org/pdf/2503.24047\n",
            "- Fine-tuning a planner on an instruction-based dataset in drug discovery enables interactive molecule optimization by internalizing expert feedback, effectively yielding a functional drug discovery agent (Ye et al., 2023a).\n",
            "- Fine-tuning a planner on the ToolBench dataset improves its ability to invoke and interact with external APIs and to generate plans that effectively leverage external tools (Qin et al., 2023).\n",
            "- A multi-agent RL framework for large-eddy simulations can identify optimal agent strategies for wall-model discovery when performance is measured via scenario-related scalar reward functions (Bae and Koumoutsakos, 2022).\n",
            "https://arxiv.org/pdf/2503.24047\n",
            "- Lutz et al. applied an RL-based approach combined with Monte Carlo tree search for protein design and created complex protein nanomaterials with desired properties (Lutz et al., 2023).\n",
            "- Barata et al. demonstrated that incorporating human preferences into a diagnostic AI for skin cancer by adjusting rewards and penalties based on expert-generated tables tailored the system‚Äôs performance to real-world clinical insights (Barata et al., 2023).\n",
            "- Sauter et al. developed a meta-reinforcement learning algorithm for causal discovery that enabled agents to construct explicit causal graphs and, with rewards based on Structural Hamming Distance to the true graph, guided agents toward more accurate causal models through optimal interventions (Sauter et al., 2023).\n",
            "- Reinschmidt et al. applied RL to manage a magneto-optical trap in a cold atom experiment, optimizing atom cooling with a reward function accounting for atom number and average kinetic energy; the experiment underscored the robustness of RL-based planners to external disturbances (Reinschmidt et al., 2024).\n",
            "https://arxiv.org/pdf/2503.24047\n",
            "- Prompt-based planners in LLM-based scientific agents are highly sensitive to prompt quality, leading to inconsistent scientific outputs.\n",
            "- SFT-based planners require extensive, high-quality domain-specific datasets that are often costly to curate.\n",
            "- RL-based planners struggle with designing robust reward functions and managing computational costs critical for scientific exploration.\n",
            "- Process supervision-based planners demand complex and as-yet non-standardized feedback mechanisms.\n",
            "https://arxiv.org/pdf/2503.24047\n",
            "Early scientific agent systems such as Coscientist and ChemCrow used a single LLM-based planner to orchestrate all operations, whereas recent multi-agent planners like Google‚Äôs AI co-scientist distribute responsibilities among specialized agents (e.g., hypothesis generation, reflection, idea ranking/refinement, meta-review).\n",
            "https://arxiv.org/pdf/2503.24047\n",
            "Empirical results from the AI co-scientist framework show that a modular approach accelerates discovery by reducing hypothesis-generation timelines from weeks to days and improves the novelty and accuracy of research outputs.\n",
            "https://arxiv.org/pdf/2503.24047\n",
            "- A retrieval framework manages a pre-defined, scalable list of external data sources (e.g., OpenStreetMap, US Census), exemplifying a curated external data approach for task-specific scientific agents (Ning et al., 2025).\n",
            "- In the astronomical domain, mephisto employs a dynamically updated external knowledge base via a learning system that extracts and validates domain knowledge, indicating a continuously evolving external KB (Sun et al., 2024b).\n",
            "- Across examined approaches, scientific agents are enhanced when given access to diverse external knowledge bases, which vary by task (from broad literature and web resources to specialized knowledge graphs and curated datasets) and collectively expand the recency and breadth of information beyond intrinsic LLM knowledge.\n",
            "- External knowledge bases tend to ground agent reasoning in established scientific knowledge, promote validity and novelty through referencing existing works and data, and enable interaction with real-world tools and information sources.\n",
            "- Implementation strategies observed span from RAG-based retrieval of unstructured text to querying structured KGs/databases, API integrations, and web browsing, reflecting increasingly sophisticated knowledge integration to empower scientific agents.\n",
            "https://arxiv.org/pdf/2503.24047\n",
            "- In Matchat, fine-tuning Llama2-7B with structured materials knowledge data improved model performance in materials science, demonstrating the efficacy of incorporating domain-specific structured information (Chen et al., 2023).\n",
            "- ProLLaMA, which employs Low-Rank Adaptation, improved the efficiency of protein learning during fine-tuning, making specialized model training more resource-effective (Lv et al., 2024; Hu et al., 2022).\n",
            "- Tx-LLM, fine-tuned from PaLM-2, was trained on 709 datasets covering 66 tasks across the drug discovery pipeline (Chaves et al., 2024; Anil et al., 2023).\n",
            "https://arxiv.org/pdf/2503.24047\n",
            "- Integrating SymPy, SciPy, and CVXPY into natural-language reasoning frameworks (Tora) produced significant performance improvements for open-source LLMs across multiple mathematical reasoning benchmarks.\n",
            "- Adding a retrieval mechanism to ClimateGPT that accesses curated climatology literature enhanced response accuracy in climate science applications.\n",
            "https://arxiv.org/pdf/2503.24047\n",
            "- Physics-simulator-driven LLM workflows achieved validated results in constitutive law discovery and molecular design tasks. (Ma et al., 2024a)\n",
            "- Airfoil optimizations performed with MyCrunchGPT were validated by high-fidelity simulations, confirming the correctness of the LLM-guided designs. (Kumar et al., 2023)\n",
            "- Molecular docking simulations enabled generation of molecules with target-specific binding affinities, with docking scores providing empirical signals for refining synthesis. (Gao et al., 2024a)\n",
            "- Benchmark evaluations reveal that even state-of-the-art systems struggle to manage API dependencies and adapt to frequently updated external services. (Shen et al., 2025)\n",
            "https://arxiv.org/pdf/2503.24047\n",
            "No single agent system has yet achieved all the features listed in Table 4 comparing general-purpose and scientific agents, although systems are trending toward these features due to the scientific field‚Äôs demands for high logic, structured content, long-term retention, professionalism, low error tolerance, and reproducibility.\n",
            "https://arxiv.org/pdf/2503.24047\n",
            "- Geometry3K contains 3,002 K-12 mathematics items (Lu et al., 2021).\n",
            "- GeoEval contains 5,050 K-12 mathematics items (Zhang et al., 2024b).\n",
            "- VisScience contains 3,000 K-12 items spanning physics, chemistry, and mathematics (Jiang et al., 2024).\n",
            "- MathVista contains 6,141 items across K-12 and college mathematics (Lu et al., 2024b).\n",
            "- SciBench contains 869 college-level items in physics, chemistry, and mathematics (Wang et al., 2024d).\n",
            "- SciEval contains 18,000 college-level items in physics, chemistry, and biology (Sun et al., 2024a).\n",
            "- SuperGPQA contains 26,529 graduate-level items in general domains (Team et al., 2025).\n",
            "- HLE contains 3,000 expert-level items spanning humanity, science, and mathematics (Phan et al., 2025).\n",
            "https://arxiv.org/pdf/2503.24047\n",
            "On the Humanity‚Äôs Last Exam (HLE) benchmark, multiple frontier large language models achieve accuracy scores below 10%, indicating a substantial performance gap on difficult, closed-ended academic tasks.\n",
            "https://arxiv.org/pdf/2503.24047\n",
            "- Chemist-X automates reaction condition recommendations for chemical synthesis using RAG techniques and CAD tools and surpasses traditional synthesis AIs in performance (Chen et al.).\n",
            "- Coscientist autonomously planned, designed, and executed scientific experiments, demonstrating its capabilities through successful catalyzed chemical reactions while addressing safety concerns (Boiko et al., 2023).\n",
            "- ChemCrow, integrating 18 expert tools, autonomously executes complex chemical tasks and enhances performance in organic synthesis, drug discovery, and materials design (Bran et al., 2024).\n",
            "- HoneyComb achieves superior performance in multiple materials science tasks via a researcher-constructed knowledge base and a generated API library (Zhang et al., 2024a).\n",
            "- A-Lab integrates LLM models, robotics, and active learning with automated experimentation to optimize synthesis pathways for novel inorganic materials, accelerating materials discovery (Szymanski et al., 2023).\n",
            "https://arxiv.org/pdf/2503.24047\n",
            "- BioDiscoveryAgent autonomously designs genetic perturbation experiments and empirically improves prediction accuracy and efficiency, outperforming traditional methods in identifying genes linked to specific phenotypes (Roohani et al., 2024).\n",
            "- AgentMD, augmented with 2,164 clinical calculators, significantly improves risk prediction accuracy and clinical workflows by curating and applying relevant tools in healthcare analysis (Jin et al., 2024a).\n",
            "- AI co-scientist built on Gemini 2.0 demonstrates empirically validated effectiveness in pharmaceutical repurposing, target discovery, and antimicrobial resistance research through rigorous experimental verification (Gottweis et al., 2025).\n",
            "https://arxiv.org/pdf/2503.24047\n",
            "- The LPCOMDA framework uses an LLM-based planner to automate modulation design in power electronics and reports improved efficiency. (Liu et al., 2024b)\n",
            "- LLMPhy, which combines LLMs with physics engines, enhances optimization and accuracy in physical reasoning tasks. (Cherian et al., 2024)\n",
            "- MyCrunchGPT achieves automated NACA airfoil design and validation via CFD-LLM integration, accomplishing multi-cycle iterative optimization within significantly reduced timeframes. (Kumar et al., 2023)\n",
            "- MechAgents leverages multi-agent LLMs with finite element methods to solve mechanics problems with improved speed and precision. (Ni and Buehler, 2024)\n",
            "- AstroSage-Llama-3.1-8B, a domain-specialized assistant for astronomy, astrophysics, and cosmology, demonstrates remarkable proficiency across a wide range of questions. (de Haan et al., 2024)\n",
            "- Data Interpreter autonomously solves end-to-end data science problems and achieves significant performance improvements across various tasks. (Hong et al., 2024)\n",
            "- LLMs perform well on basic physics problems but struggle with complex simulations; integrating them with established computational packages enhances their capabilities. (no explicit citation given in the chunk)\n",
            "https://arxiv.org/pdf/2503.24047\n",
            "- LLMs can be manipulated to generate fabricated scientific arguments that falsely claim biases are beneficial, risking the misleading of researchers.\n",
            "- Adversaries can use LLMs to poison biomedical knowledge graphs, manipulating drug‚Äìdisease relations.\n",
            "- Empirical studies of responsible AI practices reveal gaps in ethical preparedness within research contexts.\n",
            "- Implementing structured internal logs and explanation frameworks increases transparency by making AI reasoning auditable and more explainable.\n",
            "- Even advanced models reproduce historical biases when not properly managed or mitigated.\n",
            "https://arxiv.org/pdf/2503.24047\n",
            "- The AI co-scientist has been demonstrated to propose promising drug repurposing candidates for acute myeloid leukemia.\n",
            "- The system identified epigenetic regulators as novel treatment targets for liver fibrosis.\n",
            "- The system helped uncover mechanisms of microbial evolution and antimicrobial resistance by recapitulating unpublished findings of novel gene transfer pathways in bacteria.\n",
            "Loading embedding model: allenai/scibert_scivocab_uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name allenai/scibert_scivocab_uncased. Creating a new one with mean pooling.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding 94 extractions...\n",
            "Model embedding dimension: 768\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cec026e7ae804e49b9de3ec0b259a499"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "393ab0f70c474a7abc15ef2faa2050db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93f85936f9a8475b9f6f11a9b2f12ac4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "490a5678dd4c41ca8b8d7b4b6fda1562"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b991fc1889645a7b2e9eaf4e5cec163"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1c9846b2b2d45aab66653674c0421ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Created embeddings matrix: (94, 768)\n",
            "Debug info:\n",
            "Embeddings shape: (94, 768)\n",
            "Embedding dimensions: 768\n",
            "\n",
            "Similarity stats:\n",
            "Mean similarity: 0.846\n",
            "Std similarity: 0.063\n",
            "Min similarity: 0.551\n",
            "Max similarity: 0.985\n",
            "\n",
            "KMeans with k=3:\n",
            "  Cluster 0: 10 observations\n",
            "  Cluster 1: 48 observations\n",
            "  Cluster 2: 36 observations\n",
            "\n",
            "KMeans with k=4:\n",
            "  Cluster 0: 9 observations\n",
            "  Cluster 1: 32 observations\n",
            "  Cluster 2: 18 observations\n",
            "  Cluster 3: 35 observations\n",
            "\n",
            "KMeans with k=5:\n",
            "  Cluster 0: 9 observations\n",
            "  Cluster 1: 32 observations\n",
            "  Cluster 2: 16 observations\n",
            "  Cluster 3: 36 observations\n",
            "  Cluster 4: 1 observations\n",
            "\n",
            "KMeans with k=8:\n",
            "  Cluster 0: 6 observations\n",
            "  Cluster 1: 20 observations\n",
            "  Cluster 2: 19 observations\n",
            "  Cluster 3: 1 observations\n",
            "  Cluster 4: 16 observations\n",
            "  Cluster 5: 18 observations\n",
            "  Cluster 6: 9 observations\n",
            "  Cluster 7: 5 observations\n",
            "\n",
            "KMeans with k=6:\n",
            "  Cluster 0: 7 observations\n",
            "  Cluster 1: 8 observations\n",
            "  Cluster 2: 14 observations\n",
            "  Cluster 3: 29 observations\n",
            "  Cluster 4: 1 observations\n",
            "  Cluster 5: 35 observations\n",
            "Clustering 94 extractions with KMeans (k=4)...\n",
            "\n",
            "üìä Clustering Results:\n",
            "   Clusters found: 4\n",
            "   Cluster 0: 9 extractions\n",
            "   Cluster 1: 32 extractions\n",
            "   Cluster 2: 18 extractions\n",
            "   Cluster 3: 35 extractions\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'cluster_id'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-556019723.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcluster_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m       \u001b[0;31m# Get all extractions for this cluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m       \u001b[0mcluster_extractions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent_for_embedding\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cluster_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcluster_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_extractions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'cluster_id'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(content_for_embedding)):\n",
        "    content_for_embedding[i]['cluster_id'] = k_means_clustering_labels[i]\n",
        "\n",
        "# Generating cluster summary\n",
        "\n",
        "cluster_summaries = {}\n",
        "for cluster_id in range(k):\n",
        "      # Get all extractions for this cluster\n",
        "      cluster_extractions = [ext for ext in content_for_embedding if ext['cluster_id'] == cluster_id]\n",
        "\n",
        "      if len(cluster_extractions) > 0:\n",
        "          summary = summarize_cluster_extractions(\n",
        "              cluster_extractions,\n",
        "              property_name=\"empirical_observations\",\n",
        "              property_description=property_dictionary['empirical_observations']['description'],\n",
        "              cluster_id=cluster_id\n",
        "          )\n",
        "          cluster_summaries[cluster_id] = summary\n",
        "          print(f\"Cluster {cluster_id}: {summary} ({len(cluster_extractions)} extractions)\")\n",
        "\n",
        "      print(\"\\n‚úÖ Cluster summaries generated!\")\n",
        "      print(\"\\nFinal summaries:\", cluster_summaries)\n",
        "\n",
        "\n",
        "# Generating 2d visualisation data using UMAP\n",
        "\n",
        "visualisation_data = create_visualization_data(embeddings, k_means_clustering_labels, content_for_embedding)\n",
        "\n",
        "# Generating visualisation with summary\n",
        "\n",
        "visualize_clusters_with_summaries(visualisation_data['embeddings_2d'], visualisation_data['cluster_labels'], visualisation_data['extractions'], cluster_summaries=cluster_summaries, property_name=\"Empirical Observations\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uRhyGnCgK4g_",
        "outputId": "83d8752a-8df6-461f-eef3-7abd728421c0"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. - The TOMATO-Chem benchmark contains 51 chemistry/materials papers: Polymer Chemistry 21, Organic Chemistry 22, Inorganic Chemistry 3, Analytical Chemistry 5; publication venues include Nature/Science 27, Nature subjournals 20, and other top journals 4. [Table 1; Table 2]\n",
            "- In inspiration retrieval (Q1), GPT-4o achieves high hit ratios even with aggressive down-selection: for corpus sizes 150/300/1000/3000, hit ratios are 92.8/76.8/61.4/NA% at top 20%, 96.7/83.7/60.8/NA% at top 4%, 96.4/88.9/69.0/46.7% at top 0.8%, and 95.8/86.9/70.6/52.0% at top 0.016%. [Table 3]\n",
            "- Smaller screening windows improve retrieval: with a screen window size of 60 selecting 3 in one round (5% of corpus) the hit ratio is 71.6%, whereas a window of 15 selecting 3 for two rounds (4% of corpus) yields 83.7%. [Table 4]\n",
            "- Model comparison for inspiration retrieval (corpus size 300) shows GPT-4o outperforming Llama-3.1 models: GPT-4o achieves 96.7% (top 20%), 83.7% (top 4%), 60.8% (top 0.8%); Llama-3.1-405B: 95.7%, 78.7%, 52.7%; Llama-3.1-70B: 83.0%, 59.5%, 43.5%; Llama-3.1-8B: 71.6%, 43.5%, 26.8%. [Table 5]\n",
            "- In hypothesis synthesis evaluation (Q2), with a background survey, GPT-4o Top Matched Score (MS) distribution across 51 tasks is: 28 with MS=5, 1 with MS=4, 19 with MS=3, 3 with MS=2, 0 with MS=1, 0 with MS=0; expert Top MS distribution is: 9 with MS=5, 12 with MS=4, 22 with MS=3, 6 with MS=2, 2 with MS=1, 0 with MS=0. Without a background survey, GPT-4o Top MS distribution is: 25 with MS=5, 2 with MS=4, 19 with MS=3, 5 with MS=2, 0 with MS=1, 0 with MS=0. [Table 7]\n",
            "- LLM-based ranking correlates with use of ground-truth inspirations (Q3): hypotheses that match 2 inspirations have average rank ratio 0.411 (n=302), 1 inspiration 0.474 (n=2458), and 0 inspirations 0.521 (n=4899). [Table 8]\n",
            "- LLM-based ranking also trends with content quality: hypotheses with GPT-4o-labeled MS of 4 have average rank ratio 0.439 (n=36), MS=3 has 0.488 (n=404), MS=2 has 0.501 (n=427), while those with no matched inspirations (MS=-1) have 0.503 (n=6451). [Table 9]\n",
            "- In copilot-like experiments (|I|=300, strict background), expert Top MS over 51 tasks is: 0 with MS=5, 2 with MS=4, 19 with MS=3, 16 with MS=2, 8 with MS=1, 6 with MS=0. [Table 11]\n",
            "- MOOSE-Chem outperforms baselines on hypothesis quality: Top MS/Average MS are 4.020/3.765 versus SciMON 2.549/2.464, MOOSE 2.281/2.686, and Qi et al. (2024) 2.882/2.356; removing multi-step and evolutionary units reduces performance (e.g., w/o multi-step: 2.564/2.730; w/o multi-step & EU: 2.863/2.578). [Table 10]\n",
            "- Prior studies report: (i) LLM-generated NLP/biochemical hypotheses lag scientific papers in novelty, depth, and utility (Wang et al., 2024b); (ii) LLMs can generate novel and valid social science hypotheses as judged by PhD students (Yang et al., 2024b); (iii) FunSearch finds solutions to specific conjectures but not new theorems (Romera-Paredes et al., 2024); (iv) catalyst discovery evaluations relying on rediscovery of commercial catalysts risk data contamination (Sprueill et al., 2023; 2024); (v) word embeddings from 3.3M materials abstracts recommended materials years pre-discovery (Tshitoyan et al., 2019); (vi) literature sentiment predicts emerging thermoelectric materials (Xie et al., 2024). [(Wang et al., 2024b); (Yang et al., 2024b); (Romera-Paredes et al., 2024); (Sprueill et al., 2023; 2024); (Kumar et al., 2024); (Tshitoyan et al., 2019); (Xie et al., 2024)]\n",
            "2. - Under Claude-3.5-Sonnet evaluation, MOOSE-Chem achieved the highest Top Matched Score (4.471) and a higher Average Matched Score (3.697) than SciMON (TopMS 3.824, AvgMS 3.559), MOOSE (TopMS 3.529, AvgMS 3.431), and Qi et al. (TopMS 3.902, AvgMS 3.092); removing multi-step (TopMS 4.216, AvgMS 3.592) and removing both multi-step & EU (TopMS 3.941, AvgMS 3.614) reduced performance relative to full MOOSE-Chem.\n",
            "- Under Gemini-1.5-Pro evaluation, MOOSE-Chem attained the highest Top Matched Score (3.686), while its Average Matched Score (2.443) was lower than SciMON (2.690) and the w/o multi-step variant (2.529); removing multi-step & EU further reduced TopMS to 2.902.\n",
            "- Counting high-quality hypotheses by branch shows: at MS ‚â• 5, only non-EU = 16, only EU branches = 19, only EU-recombination branch = 20; at MS ‚â• 4, only non-EU = 46, only EU branches = 54, only EU-recombination branch = 24; this indicates roughly one-third of high-quality hypotheses can be obtained without mutations and that recombination yields more high-quality hypotheses than the non-EU branch.\n",
            "- Significance feedback ablation (Claude-3.5-Sonnet) shows that omitting significance feedback improves Matched Score distributions (e.g., Average MS count at score 5 increases from 4 to 8; Top MS count at score 4 increases from 7 to 13), leading to better performance on the Matched Score metric.\n",
            "- When ranking ground truth hypotheses mixed with generated ones, average rank ratios were Overall = 0.65, Validness = 0.75, Novelty = 0.76, Significance = 0.73, Potential = 0.70, indicating ground truth hypotheses are not consistently ranked at the top.\n",
            "- Background options ablation for Q1 shows that including a background survey improves inspiration retrieval: with strict background + survey, Hit Ratios are 96.7% (top 20%), 83.7% (top 4%), 60.8% (top 0.8%); without strict background (with survey), 95.1%, 77.8%, 54.2%; with strict background without survey, 96.7%, 80.1%, 57.8%.\n",
            "3. - Existing benchmarks ScienceQA and GSM8K predominantly contain problems grounded in grade-level subjects (Lu et al., 2022; Cobbe et al., 2021).\n",
            "- The MATH benchmark introduces high-school level questions but primarily focuses on math problems (Hendrycks et al., 2021).\n",
            "- Benchmarks such as MMLU, AGIEval, and JEEBench, though spanning multiple disciplines, mainly require basic computations (addition, subtraction, multiplication, exponentiation), which do not adequately assess deep scientific reasoning (Hendrycks et al., 2020; Zhong et al., 2023; Arora et al., 2023).\n",
            "- Most of these benchmarks are restricted to textual problems and omit visual elements such as figures or diagrams (Lu et al., 2022; Cobbe et al., 2021; Hendrycks et al., 2021; Hendrycks et al., 2020; Zhong et al., 2023; Arora et al., 2023).\n",
            "- The Chain-of-Thought approach prompts LLMs to produce detailed, step-by-step solutions intended to stimulate deeper problem thinking (Huang et al., 2022; Wang et al., 2022; Wei et al., 2022; Zhou et al., 2022).\n",
            "- Strategies that enable LLMs to use external tools improve numerical computation capability (Lu et al., 2023b; Schick et al., 2023).\n",
            "4. - Existing LLM benchmarks for lower educational levels predominantly focus on basic arithmetic operations rather than advanced mathematical computations.\n",
            "- Most existing benchmarks are textual-only and omit visual elements such as graphs or diagrams.\n",
            "- In the SCIBENCH textbook dataset, the Fundamentals of Physics subset contains 142 problems, with 9.2% including detailed solutions and 43.0% including visual elements (Halliday et al., 2013).\n",
            "- In the SCIBENCH textbook dataset, the Statistical Thermodynamics subset contains 83 problems, with 20.5% including detailed solutions and 0.0% including visual elements (Engel & Reid, 2010).\n",
            "- In the SCIBENCH textbook dataset, the Classical Dynamics of Particles and Systems subset contains 66 problems, with 12.1% including detailed solutions and 4.5% including visual elements (Thornton & Marion, 2021).\n",
            "- In the SCIBENCH textbook dataset, the Calculus: Early Transcendentals subset contains 161 problems, with 19.3% including detailed solutions and 67.7% including visual elements (Stewart et al., 2012).\n",
            "- In the SCIBENCH textbook dataset, the Probability and Statistical Inference subset contains 93 problems, with 21.5% including detailed solutions and 1.1% including visual elements (Hogg et al., 1977).\n",
            "- In the SCIBENCH textbook dataset, the Elementary Differential Equations and Boundary Value Problems subset contains 55 problems, with 9.1% including detailed solutions and 0.0% including visual elements (Boyce et al., 2021).\n",
            "5. - On the SCIBENCH textbook dataset in the zero-shot setting, GPT-4-Turbo achieved an average accuracy of 40.99%, outperforming GPT-4 at 33.79% and Mistral-7B at 6.23%; GPT-4-Turbo outperformed Mistral-7B by 34.76 percentage points.\n",
            "- In few-shot Chain-of-Thought prompting on the textbook dataset, GPT-4-Turbo achieved 39.45% average accuracy versus GPT-4 at 30.36%.\n",
            "- LLaMA-2-70B improved from 2.41% average accuracy (zero-shot) to 8.40% (few-shot) on the textbook dataset.\n",
            "- Incorporating external tools improved GPT-4‚Äôs textbook performance from 30.36% (few-shot CoT) to 43.22%.\n",
            "- Despite superior zero-shot performance, GPT-4-Turbo underperformed GPT-4 in the few-shot setting when leveraging Python for numerical computation.\n",
            "- On the multimodal subset, proprietary GPT-4 augmented with image captions and OCR-detected text outperformed open-source LMMs; specifically, GPT-4 with Program-of-Thoughts achieved 13.8% accuracy versus 7.4% for the best open model LLaVA-LLaMA-2-13B.\n",
            "- On the closed exam subset, GPT-4 achieved an averaged score of 57.54%; in the Data Mining course, GPT-4 scored 64.44% on the midterm and 42.67% on the final, below average student scores of 80.18% and 72.71%, respectively.\n",
            "- In the automated skill-classification verification process for error analysis, approximately 20% of incorrectly classified skills were discarded after human scrutiny.\n",
            "6. - In zero-shot settings, Chain-of-Thought (CoT) prompting reduces calculation-error rates to 13.6% compared to 29.0% with the vanilla zero-shot baseline.\n",
            "- Zero-shot CoT increases errors in other skills: causal reasoning error rate rises to 32.2% and logical decomposition to 25.4%, versus 18.3% and 18.3% in zero-shot without CoT.\n",
            "- An observed case shows zero-shot without CoT produced the correct formula but failed in calculation steps, while CoT misinterpreted problem conditions and failed to use the correct formula.\n",
            "- Using external tools reduces calculation errors compared to few-shot CoT, decreasing from 14.5% to 6.2%, but weakens other skills, particularly code conversion (program generation for solutions).\n",
            "- Few-shot CoT is not universally beneficial: it reduces causal-reasoning errors by 12.8% but increases errors in other skills such as logical decomposition.\n",
            "7. On the SCIBENCH exam dataset under zero-shot settings (with and without chain-of-thought), GPT-4 achieved the highest scores in 6 of 7 exams‚Äî58/90, 44/75, 40/56, 50/100, 80/100, and 25/95‚Äîwhile Claude2 attained the top score in one exam with 41/75; LLaMA-2-7B scored substantially lower (e.g., 24/90, 14/75, 6/56, and 0/100 on some exams). Additionally, using Wolfram Language as an external tool led to a 46.9% code conversion error rate, predominantly due to syntax errors and reserved-symbol violations, indicating that external tool use can weaken code conversion skills.\n",
            "8. - In Figure S9, for the de Broglie wavelength problem, the ChatGPT solution using chain-of-thought was classified as error category 4 (Causal Reasoning) and produced 8.09 pm, whereas the non-chain-of-thought solution was classified as error case 10 (Calculation Skills) and produced 3.31 pm; the correct value is 243 pm.\n",
            "- In Figure S10, for the cardioid length problem, the model produced 32 instead of the correct 8, and the error reason was categorized as 10 (Calculation Skills).\n",
            "- In Figure S11, for Simpson‚Äôs Rule with n = 10, the model returned a Wolfram Language error message (‚ÄúObjects of unequal length...‚Äù), and the error category was 8 (Code Conversion Skills).\n",
            "- In Figure S12, for finding H√ºckel molecular orbitals for ethene, the model asserted visual representations were required and provided no solution; the error category was 3 (Spatial Perception).\n",
            "- In Figure S13, for the mean speed of N2 at 25¬∞C, the model computed 515 m s‚Åª¬π using the rms formula; the error category was 7 (Scientific Literacy); the correct value is 475 m s‚Åª¬π.\n",
            "- In Figure S14, for four-letter code words with all letters different, the model answered 456,976 (26‚Å¥) instead of the correct 358,800; the error category was 9 (Logical Reasoning).\n",
            "- In Figure S15, for ethene H√ºckel molecular orbitals, the model claimed two bonding and two antibonding orbitals and returned ‚ÄúNone‚Äù as final answer; the error category was 6 (Abstract Reasoning); the correct normalization constant is 0.70710678.\n",
            "- In Figure S16, for the weight of air in a room, the model computed 101,640 N using the ideal gas law; the error category was 2 (Identification of Assumptions); the correct value is 418 N.\n",
            "- In Figure S17, for the probability that the third spade appears on the sixth draw, the model answered 0.258 instead of the correct 0.064; the error category was 4 (Causal Reasoning).\n",
            "- In Figure S18, for the CO2 cylinder usage time, the model answered 24.1 days; the error category was 5 (Problem Deduction Skills); the correct duration is 52 days.\n",
            "- In Figure S19, for the gravitational acceleration difference between an astronaut‚Äôs feet and head in orbit, the model answered 1.0 √ó 10‚Åª‚Åµ m s‚Åª¬≤; the error category was 1 (Logical Decomposition and Analysis Skills); the correct difference is ‚àí4.37 √ó 10‚Åª‚Å∂ m s‚Åª¬≤.\n",
            "9. - Geometry3K contains 3,002 K-12 mathematics items (Lu et al., 2021).\n",
            "- GeoEval contains 5,050 K-12 mathematics items (Zhang et al., 2024b).\n",
            "- VisScience contains 3,000 K-12 items spanning physics, chemistry, and mathematics (Jiang et al., 2024).\n",
            "- MathVista contains 6,141 items across K-12 and college mathematics (Lu et al., 2024b).\n",
            "- SciBench contains 869 college-level items in physics, chemistry, and mathematics (Wang et al., 2024d).\n",
            "- SciEval contains 18,000 college-level items in physics, chemistry, and biology (Sun et al., 2024a).\n",
            "- SuperGPQA contains 26,529 graduate-level items in general domains (Team et al., 2025).\n",
            "- HLE contains 3,000 expert-level items spanning humanity, science, and mathematics (Phan et al., 2025).\n",
            "{'id': 'gen-1762617366-1Ivc7Jkop3efcvdhNae3', 'provider': 'Azure', 'model': 'openai/gpt-5', 'object': 'chat.completion', 'created': 1762617366, 'choices': [{'logprobs': None, 'finish_reason': 'stop', 'native_finish_reason': 'completed', 'index': 0, 'message': {'role': 'assistant', 'content': 'LLM scientific reasoning benchmarks', 'refusal': None, 'reasoning': '**Evaluating LLMs on science tasks**\\n\\nI\\'m thinking about the common practice of evaluating large language models (LLMs) on scientific tasks, datasets, and error categories. CoT effects come into play here too. I believe I should focus my output specifically on \"LLM scientific reasoning benchmarks.\" It feels like the right direction, and I want to ensure I\\'m honing in on the most relevant aspects. Outputting only that should suffice for clarity and relevance.', 'reasoning_details': [{'type': 'reasoning.summary', 'summary': '**Evaluating LLMs on science tasks**\\n\\nI\\'m thinking about the common practice of evaluating large language models (LLMs) on scientific tasks, datasets, and error categories. CoT effects come into play here too. I believe I should focus my output specifically on \"LLM scientific reasoning benchmarks.\" It feels like the right direction, and I want to ensure I\\'m honing in on the most relevant aspects. Outputting only that should suffice for clarity and relevance.', 'format': 'openai-responses-v1', 'index': 0}, {'type': 'reasoning.encrypted', 'data': 'gAAAAABpD2gbqj5Hw9ejr4s6DvDtSnIQNmBKD_PtL12euQIWwxnLxZZoD6DrenoQl5Df4GoZBuUwnF3dkqI32eiR129xl-IY1XALEtIaunuHM3GjMsStkZQ6PesKeIrYqyKwEvwnvHkM59xbqa8nHtaVY4bUYrKUHO6X5Rg04CHjX2A3bVvtpF9V2r3aCkyy7MxUbOodAfxYwE9sY2__DV76Zkvy0tFrsGmx1u3cui9UKCO0eumyDLeXEJBWnf_tC9HWKWSnrzwQ-03MKS917Qq4xycxxsNgOajc_f2VIvhEY0zupwg_ut_9shm81KIwQhu8sN_hdwzHi_VoFuBuKJijyrBamwGurfFT-WMFjgwzdA6ml50anDQ7VT7weIWlJRi1FbAcqxVWFbSmHbJtaDMVKn0PFys2uMy3ZkmYyqLb8VCDo-zjTCbRTy-K1JokWLLchWmyBFT5qk61jUPiMbcQ-3utTqOZSaSmhzsk_OMojQcb_V0fLVEqFZY-0CimX6NMOGvgqLx3_NujTwAU8xy3aWYjBmP_eSWnErMCEOkCPqxkHy-nDbuJAqK3QPEGpB8_L-_JTKqm89zdCUwjslYyqUjNMh_3i08f9dYfQ5XXvJqoMPCSdddVdx06Pxcwznxtp0lbMnIc1KjHsLw7_2BzzGgUc3kJri9zMA9hjaunE9V1vasLC_iboHm2ruRkypXZEGpVWpQwq0Nwxz_o5Fi-OnraNWhoPRgnBpn2jU6uKTxBHGc5K4d-yHwe51-GNA97YrFw0dpLVO7YZAWGDtqJKq3vBN2qO-946qFzEOeH0sYMtKdyTAVym88e4jTFzcFG59jpc0eJmDT1Ha7npiBmoylUt-QRiRVkuElDXWI2QbYPnxOX_PVGxiTNzSezXbGt4RIizMywJoOh96xVBnceC9iIlrRk0gjHAcB4vJOQpKbrOzoFVkKbGv4SVvL4Kj9DD00c3JqBxhPzk2TLnOJXZvkHn4aafHXQNsntJ0tLtwrzZKGqBLKMAlWdB_mczgdsz5v35z4ov5zlW3fwcOPWto9XvStnRiY-I2RI3E1V0nPXC5iP3JB_Z4NiDbl_lQS6rRQXEdKcKYKwCLHqeHGUS1v-CjHEh9jQYjkXtU56q6SQ8A9_rOCSd4m7B_P5wx9EM9mrYO-31rd0NcK6MO8VgxrA7XW6hc-JXkv3z-3ZDtPn1VIMXAWA86wzW102zBw6okhtkC8Mj7lTD7HWWDR0eeLimLFVJcRUG-_59Hql9v0DA7-_Uk-IjBuCDQCUG4U5_7u9P19GlYfcAuQH5M7Au9T5BPzmRy_E9TElvPXqE0QaJaZCuDsi6hvmXktNrar-2dsoNoxo2-mOz42-njmQ1STxZqQC_DM6CPEajYsoafp95tSby8N8UaIjua12L6dnYRNw2HK1tGEnl19q6o5N54uGeMs0Zhr6ANEanConh_7MZcMnz0RLJTUPZSZ3nKKXpVsWyByk_EJYsNStdNecWJIcfIw4TD57HJGVE0bHjGQRweQb2hlNldMT6mKXgVvwKNs4CxT0XAs54QZqv3S4iYGl79dVlfqDHioEmKZ2TuCeLt2Lwk-d4GqQacLQ9_eqk51-cIznSr3jc-V1QQ-3UsJ-1OSEiqhQG2OA4-SHQ0L82mHL7Z66g4twkXeyvOC0ZBv-yJdJ5ICCigw_zsPg35QEyJRxHiI7Ii_XYs1SSBupphsnzyXB5rhEMxYI8DyszoXeO5vpSo-bMDj22pQt9coxf5DHlroPU-v570uDbs_nIUj0pmJD_aUGhq1ysh3iFWC5fx36zHhagsTDwWKqdsx5AdzW4U6jqWtaYj_JMl9b59h5GRiAElSsbrmJXok1sfip4P9fEKU2kT27mtyDC_8y9MnbQ88Qa-bokzoc5rZR2fRydaLks6jPA2ONRzEaFbrJsBtWbqE6M2-I4CRBt4r6RJXBFZSI4E8aZdADAPn-4SrRQNPDaovlkr7v5dhZB6qU8FdXCUONMM9RDG9_EJuayM5b3sjwtzww02lh4-PlimvO8ElBHJ1AH4dMfsyYHHdXr8XLnuhGFXu9dNAJG800r5EXHyEjs8bM4axTzMH7pLqV20hzsi_IRBwzCnsbYVORgA3uqKPmPY_GzGKnd1b9z8kt_FJestzi4PcdaLWaZIlKqt1FmOzakgEla6bysHNxGPBC_Rhwh93-4XEZNCmN0hjNKoI8lF-3sqCBlc6Ct5BKG2LvuwURoofAlkB_46Rcmxav4s0Hg4tEE1YBxVZJRRs8dOH9A6cC6TmV6RxwURR3Q35Sq3Iuj5XyZ2WGdzChlpqcdEJrRV7NTSAbt6Fgcg-eFIso1lTHNE7fO7aICHKX5kk10gyjcsH0cYaK4tSlBPmcLZhrLLB7QIL_SV1OPgDjFJ1htmyWYOXP-ypMM43bOgM51YLngIS0ExitinXXX53FkpbjKnxAhG_Nuz11HZPz-A33yxRRWMycfH0qB3jp8JU=', 'id': 'rs_0ee2b96bf358058f01690f6818088c81978f7dc648824ae540', 'format': 'openai-responses-v1', 'index': 0}]}}], 'usage': {'prompt_tokens': 4210, 'completion_tokens': 267, 'total_tokens': 4477, 'completion_tokens_details': {'reasoning_tokens': 256}}}\n",
            "Cluster 0: LLM scientific reasoning benchmarks (9 extractions)\n",
            "\n",
            "‚úÖ Cluster summaries generated!\n",
            "\n",
            "Final summaries: {0: 'LLM scientific reasoning benchmarks'}\n",
            "1. - Early evidence indicates that large language models can propose scientific hypotheses in biomedicine in a zero-shot manner using only pre-trained knowledge [Qi et al., 2023].\n",
            "- Domain-specific foundation models such as BioBERT, pre-trained on biomedical texts, more accurately extract entities (genes, diseases, chemicals) and their relationships from research articles than general models [Lee et al., 2020].\n",
            "- The generative model BioGPT can extract and coherently summarize complex biological information from literature [Luo et al., 2022].\n",
            "- The Galactica model performs large-scale structuring and organization of scientific knowledge from papers, lecture notes, and textbooks, effectively carrying out knowledge extraction at massive scale [Taylor et al., 2022].\n",
            "- Retrieval-Augmented Generation produces factually grounded, traceable outputs (precise summaries and entity‚Äìrelationship pairs) by retrieving relevant documents, enabling the construction of verified, structured scientific knowledge bases [Lewis et al., 2020, Guu et al., 2020, Gao et al., 2023, Izacard et al., 2023, Fan et al., 2024, Garcia et al., 2024, Lopez et al., 2025, Feng et al., 2025a, Krotkov et al., 2025, Lee et al., 2025, Xie et al., 2024, Gao et al., 2025a].\n",
            "- Multimodal systems like ChemMiner extract chemical information from both text and diagrams within scientific papers [Chen et al., 2024a].\n",
            "- ChartAssistant can reverse-engineer scientific charts by converting visual data back into structured tables [Meng et al., 2024].\n",
            "- Multimodal models such as GPT-4o integrate information across text, tables, and diagrams to answer questions about complex biomedical documents [Hurst et al., 2024].\n",
            "- LLM-generated hypotheses are susceptible to being plausible but factually incorrect or to restating known facts [Xiong et al., 2025].\n",
            "2. - The evolutionary LLM-based code mutation process has produced entirely new algorithms that are more efficient than human-designed counterparts. [Nagda et al., 2025]\n",
            "- ShinkaEvolve improves sample efficiency in program evolution by combining exploration‚Äìexploitation-balanced parent selection, novelty-based rejection sampling, and bandit-based, task-dependent LLM prioritization. [Lange et al., 2025]\n",
            "- TOOLMAKER‚Äôs cross-domain evaluations demonstrate substantially higher reliability than prior software-engineering agents. [W√∂lflein et al., 2025]\n",
            "- Coscientist has demonstrated the ability to use bioinformatics software and control liquid handling robots to execute experiments. [Boiko et al., 2023]\n",
            "3. - SpatialAgent autonomously processed over 2 million cells across spatial transcriptomics and MERFISH pipelines and achieved parity or gains versus automated baselines while handling full projects with minimal human input [Wang et al., 2025a].\n",
            "- Team of AI-made Scientists (TAIS) analyzed gene expression datasets to identify disease-predictive genes and completed the gene discovery pipeline from raw data to insights without human intervention [Liu et al., 2024a].\n",
            "- CRISPR-GPT‚Äôs agent-generated gene-editing experiment designs were successfully translated into real-world wet-lab execution [Qu et al., 2025].\n",
            "- BioDiscoveryAgent outperformed trained Bayesian-optimization baselines by an average of 21% (and 46% on non-essential genes) across six datasets and doubled the hit-rate for multi-gene combinations over random [Roohani et al., 2024].\n",
            "- PerTurboAgent improved next-round gene panel quality over static or heuristic baselines in retrospective/simulation studies and shortened cycles in pooled genetic screens [Hao et al., 2025].\n",
            "- BioAgents achieved near-expert performance on conceptual genomics tasks in locally runnable, privacy-preserving multi-agent workflows with RAG [Mehandru et al., 2025].\n",
            "- scBaseCount expanded a harmonized single-cell compendium to over 230 million cells spanning 21 organisms and 72 tissues [Youngblut et al., 2025].\n",
            "- scAgent achieved state-of-the-art accuracy and generalization for universal cell-type annotation across 160 cell types in 35 tissues, with data-efficient extension to unseen types [Mao et al., 2025].\n",
            "- CASSIA improved low-quality cell-type annotations by retrieving external evidence and consolidating tool outputs into audit-ready documentation [Xie et al., 2024].\n",
            "- GeneAgent reduced hallucinations by cross-checking claims against expert-curated biological databases and produced auditable rationales for functional descriptions [Wang et al., 2025b].\n",
            "4. - SAMPLE experimentally identified GH1 hydrolase variants with at least 12 ¬∞C higher thermal stability than the starting sequences using autonomous design‚Äìbuild‚Äìtest cycles in a robotic lab. [Rapp et al., 2024]\n",
            "- The Virtual Lab designed 92 novel SARS-CoV-2 nanobodies and experimentally validated expression and binding across variants, with more than 90% expression among tested constructs. [Swanson et al., 2024]\n",
            "- ProtAgents generated de novo protein sequences that met specified vibrational-frequency profiles and structural constraints, validated via structure prediction and physics-aware analyses. [Ghafarollahi and Buehler, 2024a]\n",
            "- Sparks uncovered two design rules: Œ≤-biased peptides exceed Œ±-helical peptides in unfolding force for chain lengths beyond approximately 80 residues; and a chain-length/secondary-structure stability map exhibits a high-variance ‚Äúfrustration zone‚Äù for Œ±/Œ≤ folds. [Ghafarollahi and Buehler, 2025]\n",
            "- VibeGen produced de novo proteins whose all-atom molecular dynamics trajectories reproduced prescribed normal-mode amplitudes. [Ni and Buehler, 2025]\n",
            "- AutoProteinEngine achieved substantially higher accuracy than zero-shot approaches and manual fine-tuning on two real protein-engineering tasks. [Liu et al., 2024b]\n",
            "5. - LIDDiA generated molecules meeting pharmaceutical criteria across many targets and surfaced promising EGFR candidates. [Averly et al., 2025]\n",
            "- AgentMD automatically selected and executed from 2,164 curated clinical calculators (RiskCalcs) to return risk estimates with formula provenance, substantially outperforming strong prompting baselines on RiskQA. [Jin et al., 2024a]\n",
            "- MedAgents enabled multi-specialist deliberation in zero-shot settings to reach consensus diagnoses and plans across standard medical QA suites. [Tang et al., 2023]\n",
            "- ClinicalGPT is a domain LLM for clinical tasks; an EHR-integrated example, EHRAgent, auto-generated and executed code against EHR data to answer complex patient queries and compute scores. [Wang et al., 2023b]\n",
            "- BehaveAgent provided turn-key cross-species behavior analysis from raw video‚Äîincluding planning, tracking/pose estimation, sequence labeling, and report generation‚Äîwithout retraining across paradigms. [Aljovic et al., 2025]\n",
            "6. - Coscientist autonomously executed palladium-catalyzed cross-coupling reactions in under 4 minutes. [Boiko et al., 2023]\n",
            "- ChemCrow integrated 18 expert-designed tools and successfully performed autonomous planning and multi-tool analysis across organic synthesis, drug discovery, and materials science. [M. Bran et al., 2024]\n",
            "- Chemist-X achieved a fully automated, closed-loop system that proposes reaction conditions via retrieval-augmented generation and directs a robotic platform for wet-lab validation. [Chen et al., 2023a]\n",
            "- El Agente Q autonomously handled an entire computational chemistry workflow‚Äîfrom file preparation to cluster submission and result parsing‚Äîbased on a simple natural language prompt. [Zou et al., 2025]\n",
            "- LLM-RDF enabled an end-to-end synthesis workflow from literature search to product purification through a conversational six-agent framework for users without coding expertise. [Ruan et al., 2024]\n",
            "- FROGENT showed superior performance on multistep drug discovery benchmarks by integrating diverse biochemical databases and predictive models into a unified framework. [Pan et al., 2025]\n",
            "- LARC achieved a near-human-level success rate on diverse retrosynthesis tasks using an Agent-as-a-Judge mechanism to apply practical constraints. [Baker et al., 2025]\n",
            "- FMG achieved expert-level molecular design by adopting graph representations rendered as images, demonstrating benefits of multimodal inputs. [Sun et al., 2025]\n",
            "- MOOSE-Chem rediscovered core innovations from recent high-impact chemistry papers without prior knowledge of their content via automated hypothesis generation. [Yang et al., 2024]\n",
            "- The A-Lab synthesized 41 novel compounds from 58 targets over 17 days of continuous operation using an autonomous facility with three robotic arms, furnaces, and X-ray diffractometers, achieving a 71% success rate. [Szymanski et al., 2023]\n",
            "7. - MDAgent automated the MD workflow and reduced the total task time for thermodynamic calculations by over 40%. [Shi et al., 2025]\n",
            "- The Materials Laws Multi-Agent Framework used LLM agents for symbolic regression to derive a low-complexity, highly accurate formula for predicting glass-forming ability in metallic glasses. [Hu et al., 2024]\n",
            "- HoneyComb, grounded with a curated materials knowledge base and validated tools, achieved significantly higher accuracy in both reasoning and computation than general-purpose LLMs. [Zhang et al., 2024b]\n",
            "- The ‚ÄúChatGPT Research Group‚Äù system significantly accelerated the experimental optimization cycle for advanced materials synthesis via a collaborative multiagent setup. [Zheng et al., 2023]\n",
            "- LLMatDesign enabled effective discovery of materials with specific, user-defined properties in low-data regimes by adapting strategy through self-reflection on computational outcomes. [Jia et al., 2024]\n",
            "- MatAgent‚Äôs integrated human-in-the-loop multi-agent framework enhanced both the speed and reproducibility of materials research workflows. [Bazgir et al., 2025a]\n",
            "8. - The k-agents framework enabled a fully autonomous laboratory where LLM agents planned and executed experiments on superconducting quantum processors, producing entangled states with performance equivalent to human experts [Cao et al., 2024].\n",
            "- AtomAgents integrated knowledge retrieval with physics-based simulations to autonomously propose, simulate, and refine alloy compositions that met specified performance targets [Ghafarollahi and Buehler, 2024c].\n",
            "- Mephisto automated interpretation of multi-band galaxy observations by calling the CIGALE code, successfully proposing and testing hypotheses against observational data in an iterative loop [Sun et al., 2024].\n",
            "- QCopilot achieved a 100-fold speedup over manual procedures by autonomously performing modeling, optimization, and anomaly detection in atom cooling experiments [Sha et al., 2025].\n",
            "- OpenFOAMGPT successfully automated the entire OpenFOAM workflow from case setup to iterative correction, lowering the barrier to advanced CFD usage [Pandey et al., 2025].\n",
            "- MetaOpenFOAM used a multi-agent system to decompose complex natural language instructions and achieved strong performance across a diverse range of flow simulations [Chen et al., 2024b].\n",
            "9. - LP-COMDA automated the design of complex modulation strategies for power converters with a physics-informed planner, accelerating design time by over 30x and reducing errors by over 60% [Liu et al., 2024d].\n",
            "- The Autonomous GIS Agent and GIS Copilot automated end-to-end geospatial workflows‚Äîfrom data retrieval to generating maps and statistics‚Äîby translating natural language requests into executable programs, enabling advanced geospatial analysis without specialized programming [Ning et al., 2025] [Akinboyewa et al., 2025].\n",
            "- A domain-specific ReAct agent for gas turbines integrated expert knowledge with predefined tools to perform iterative gas path analysis, demonstrating feasibility of AI-driven diagnostics in power engineering [Song et al., 2024].\n",
            "10. - Across 392 comparison pairs, the agreement between expert evaluation and GPT-4o on the Matched Score yielded a hard consistency of 0.345 and a soft consistency of 0.542.\n",
            "- Across 48 comparison pairs, inter-expert agreement on the Matched Score yielded a hard consistency of 0.438 and a soft consistency of 0.854.\n",
            "- In practice, GPT-4o‚Äôs automatic evaluation scores on the Matched Score are typically 1‚Äì2 points higher than expert evaluation scores.\n",
            "- Findings reported by Bu et al. (2024) validate that a nitrogen-doped ruthenium electrode in the presence of D2O enables highly efficient, scalable, and versatile electrocatalytic reductive deuteration suitable for a wide range of substrates.\n",
            "- Wang et al. (2024a) report that integrating (Gdm)2SO4 into a PVA hydrogel with directional freezing achieves a flexible thermogalvanic armor with Carnot-relative efficiency exceeding 8% while maintaining high mechanical strength, with thermopower and mechanical robustness surpassing traditional quasi-solid thermocells.\n",
            "{'id': 'gen-1762617372-CJdT5F9RtH4w49hfLQke', 'provider': 'Azure', 'model': 'openai/gpt-5', 'object': 'chat.completion', 'created': 1762617372, 'choices': [{'logprobs': None, 'finish_reason': 'stop', 'native_finish_reason': 'completed', 'index': 0, 'message': {'role': 'assistant', 'content': 'LLM-powered autonomous scientific workflows', 'refusal': None, 'reasoning': '**Summarizing autonomous AI agents**\\n\\nI need to create a concise label that captures the common theme among the extractions in Cluster 1. They reference various AI agents and their roles in advancing scientific workflows across disciplines, such as biomedicine, code generation, and autonomous bioinformatics. It looks like the focus is on how AI-driven automation enhances scientific discovery. I‚Äôm thinking the label could be \"Autonomous AI Agents Advancing Scientific Discovery.\" This encapsulates the essence neatly!**Choosing a concise label**\\n\\nI‚Äôm considering various labels for the cluster related to autonomous AI in scientific discovery. ‚ÄúAutonomous AI-driven scientific discovery‚Äù is 4 words, while ‚ÄúLLM agents for scientific automation‚Äù and ‚ÄúAutonomous LLM agents in science‚Äù are both 5 words. There\\'s also ‚ÄúAI agents automating research workflows,‚Äù which fits the theme but adds up to 5 words. I‚Äôm leaning towards ‚ÄúAutonomous agents for scientific discovery,‚Äù which captures the essence nicely at 5 words. Avoiding hyphens could simplify things!', 'reasoning_details': [{'type': 'reasoning.summary', 'summary': '**Summarizing autonomous AI agents**\\n\\nI need to create a concise label that captures the common theme among the extractions in Cluster 1. They reference various AI agents and their roles in advancing scientific workflows across disciplines, such as biomedicine, code generation, and autonomous bioinformatics. It looks like the focus is on how AI-driven automation enhances scientific discovery. I‚Äôm thinking the label could be \"Autonomous AI Agents Advancing Scientific Discovery.\" This encapsulates the essence neatly!**Choosing a concise label**\\n\\nI‚Äôm considering various labels for the cluster related to autonomous AI in scientific discovery. ‚ÄúAutonomous AI-driven scientific discovery‚Äù is 4 words, while ‚ÄúLLM agents for scientific automation‚Äù and ‚ÄúAutonomous LLM agents in science‚Äù are both 5 words. There\\'s also ‚ÄúAI agents automating research workflows,‚Äù which fits the theme but adds up to 5 words. I‚Äôm leaning towards ‚ÄúAutonomous agents for scientific discovery,‚Äù which captures the essence nicely at 5 words. Avoiding hyphens could simplify things!', 'format': 'openai-responses-v1', 'index': 0}, {'type': 'reasoning.encrypted', 'data': 'gAAAAABpD2gjUb5HS49kzYdaTTxfLu3udYi-YPxjwA49bT_1iS9uxE2CA0b_iihahjKPDM3x_XZChiek4oAA9q9Ti0p-RDbad4flcuH8YA5gOMWqxbcX3Lj_z1dkBxGDWZBXclEBB_VYGxtJDHkXb52N92JERxE5iJv6jjg5SeZRelrjcvTZZ_aAlcxWkQ6tuJGOZeK6XTR4kgm-RIX3T0UexYt2eBRnm-UQapZbhGjn_R0oyCinQGwzqn5Y1algAaQMMOywuxOf7aNqM1B7QiAgaVorhQrUzHDcWJl8Tm8-9GfJEaU7jiTomfVIvKcpAEOKqxip-UvVWZQNTsb611F3bVx1EqIU956teYxmaLGOsokXhbEWO5gh5qNUXAU6_sFNVNjo5e23mf1jPRjhWrJAAXYOQKrgeBXGiVKB2DI0w1yrFesD6PyMTS9AsqgiCrl88oPqVO-LpIwKMKYn02AsmI4KERYUQ0rL-qURuxMavGELwjVyQ5vJ9I9sHktavtHJ9AM7iLJhSyGsGfB-0U3WyQepvH_DDV15Mng94q3VDS9QiKvjbEzRjbs2OEt5Zq2QhYYqORiIIbHzyLUXNQd__nqDNaFK5FzpqVXstwNgNMebEeFI3T4I8hEX1qRVUWWEcuMQf34TVZ4z1crGbxBalvZLSXK0WUtLwnrZsevJp44bvxlWAjO5yAPQJG_jtyZtoGa-JeumtgjLm4gTpDW9u6SLuvyfyYPcR-KtetaEsaiPidJ0Q4f4kHTosHemH8jVNTHZ3o02dMOSXHLH2SkaEA4WFF8EJIyxuPlWoCiQTzB0XVcPmsg_TEenz7P0Fpdm6M4xGh0qltO962I8X3iW4Kku31WeBxSS_QbUFjCga_uuoQ-rQ7vCD4TXhGwbd8wb_w93AwGvYXdV9HnvbDX4VmbF4MYHeXUK-fydqSyNESsFhc4my28WEV6d8pzxA6-hNjsJn3hfhvQtJQ7YON2ZXI6DCHKVsY72i7A3t-45KsJMi-8yMEhqnSC7YwSKhyMUQXY7feMKiN0VL071nMBzQU7rdE3r08HNakpQYtY1DuIt5rbojJEqg8IUzXNF5hqiHoabmEOJMHx7kFKCyPaKxO5pS0lnRyEOPMefx9YUCFDhfI6rWyhvbFC4pGAEjalafV1oE0O6KHqOa2oWZPneyQhZ48zbI1dchM0ZxNy8fyrIpKUxz6z67y01xGPcV1PWH82E-icvTYB59Jowkqfc5DNKJVL7Pjf4eeE-U6CRHlIJXKGlfdTaTKjMDVgk9Dcwud7TPcbIHSzpF8rhAlRpfc6h5WqID9q9kYajIvgpE7-FT30gxBctvA03lZCayENDQYVrefJGZpHny88MsNIL7-cxs_tjKyq8ZIxi-kCfZubfOX1ewHNpZCu1eBvMLXAzQ-3Kuc0HXVReCwKkOG_JVAFExfYFCgWVGuD-SGmNpRZDvAW3djnoAhw5QSVU13egZRNOe4IaEk7k-V3JTGRaSHeJoOu1mI5zwAEF6AjiIKFOywzkcdqdf49BR4_oYLrKf85bMTJV2UitC-ZeDsp298OBSwpsHSeB733s_fBPMi7nMBSZ594mv757aKRRqClrDU9qS4qCxBnEuKb-F5ZxeJj39n6i78ZU6JwpsHyv-OpffIwKLbmJdNhjVg7QPeifv-Q72QUu5graiXS8pFir-oqi9nGBnsSw7LKExULw47WCqpb4B-q8pIFyVcI7qhtzzX2W3_aKuAnicXM6mn0AkHWngqHlvepKtEZkaQeClyWuavZ7Q_6HNz8xTpchwjSI8zkJ-W6WBjwvl9JaqErWXTGz3Qs5NNdu6bn4dbLqp0zHiLljeR5QZ6Zh8Mol0hlYmJLyE6FnZgFqPZ94uFf-p4kaQfK7j5nYJ8HBe-TioY8QxY65VBw_Onbey_CxefXph8xpQVeXioxPZjXt-Vt58NPAtXQKKWLWNEbzmPFjASn33xGpqHmFNo3vDWwBj4_vuExwsNzPcwxU00YAsvib0UOfozbTtb9D6BTr7QKFsUanwVh4F9Y9BnMGJ9yKOkC3Fc0e0Te_-obEiyaratCuObNFijoCN7VoqZv8fMmzNXdXMwsghuoAMvYmUrGdca7FavEATSORsTH2tE9BEpf86dkmd7LIPQwGFwKW-SzYCwTgTBmehosgRHUn8pAY3TwvTCplmOkU-fiGL5eR87kF1ONZiIl8hb3V-A5VoZvvps_1Yt20WFWANIf3jviJdNtIX7dUNBavLy8RdwP1MQ_o0g1IlTySUIWsSyV3qELJLT44dN05PVDaBdN2ZoE2X7xP2vYdDHhloFbXiqmKLNqHRaQXCKvFNrG-lilRgB_rEAsdtAcmlkCqQKyPOJR1tAHKE8SxkRJfGTtce0SCVJYCYALSkXt6IXr5M7lneC6mcSP5y9e7e6x4Up9fMC13a_DqBvu_MjlYq4z_BCt5MTY2RDpk07KGg9f5qWxsa8pDO6MRfGu2rzgAqExl8ph_LAYdjJYPtLjoCSfKkjYxogEEuuE85mgU7kFytKjcZHlOb_xl8GP6DHXOemkRimjhyu7x0PagHUqaIkwOlGJlxFURujAi0zxyl8nHXlteRkcECZiLOI9VpeZK9C2ZQVt8d-E91cZuFuI6GGco4voVXUuXpx2PNJI-Z4ugOk_7f-MIrPmJALG7SjFmTb3YHH1vAWKSMmutrQbNWC-ejizQZ3u2WCIov93dswXPQPI20Ocs88Wm_a5M_XzrFvupW99ggnV3zSpOy5bM3Lfc9p3387lIacy35VUzWq67Hvvg7uQsWm16q7QTUazq5C17juwg0wzhUE6VISTVgenMkJXkBJjDs1qIFIoDKssdFkdbvJ3uC22xj5widiNbSlSbn4C-ChAs2fl0D_yLSErcRo_XZbA9cdswVy3E9Q4PmdFRKstP9pDbQ5y-D6rqY4P5Tw8SpewCGNZ-WkqiDU7QSkZ9rpcC2ToHiBZQ8XHcYa8dcZYnvaFdvx88FmaBKR_3-9fU9IO91v2lIUrw6nFWMwEVHumlMfZRAYMbz62xuNaygPfeNhQcr79o5N02Drx5fgmJkiGHsas0m7oc7aiByhkJi-x2UifrMmvadkid8XlHBsC3Y5JoZYkB0OLLbZ9vvFD0JiBp8Lg1CkDkj01h1rPLe5nKh99J1uQrCwFR7-ryk0fYVy7XIe_Q2Y6B9rJMNJ73lamdFLZVDeVpPspEJCYqNtx38L0UbzcBfzglMoF6ftf1MBnBt0uP5QQZexNov1DiwixnJMg5-E9L-V6WfXzeRM6RfOsxOCIEm2JPpUYffTsAtgIfDlIzWFXSEe4RPjNROVN-6xEnBTYc6G5EU6zL_2OPjuJhj-iduMo_pW1b-GwmekfffPy958k2yvrLeYva2bcPed6QaDlx6MOOh6O_dvvlYQnfJEZvJ6KJr5kn-gp-MqrpRXGvVrDK-jYNx8zeeAUhOKhx7k7ifRDmkPLa1G2ccpFNtgXZdkX0fEJQlsSCNMzQ-ClJORVydCiXK1cb4nF67x_4rR8NU7lr01LpRZVCrCKXHfcA155wjWj16GpZHIJ78nJyhunIBMtufhDqgF1EXC1bf3fom0C6hcWUuOpO-Qp8_kVTLXB7P3csTt-HVVhPoN_-3s2mDnOvJkhsz8oUnSDx89OAq6IANpV4LSZ7_y9OHJFEQLV3HP1aWGMF02053KFf94LtGE-rn0FmB0Sbb50GCbtUSo8kPojuXNgdAK2VGH0PT_P6dAqdQuxLMVIxjTRp2zliaL7pM_NWaNfhaTrRSUTXth-7tzng-cFNCKIa1yABAPsSSewCLJKzofvY6rbitQsxwWueiwCCcTeP1bilcp_zah4myhKJhjxJRvJr2cLeUvq7_IwvoNnDKwjUu8Swe-6WBQASgsQ_lNdIdQMJMDMdyIO2auv1JwuhaegxI-n1Ar5dM8C6eC8ssori8C1kOzp8C66ZScXn92uXsJSfUtQVQWDOtsXYcpwtzdyQC6W93-PSBqg-cJSVxa4b_wwreOGg06hyaEmjzLGJQnxMkwPbB3PMtPvpLkS_1PP7x4EcvvrvmG8_yO5rKLE3keFlLMdzOAjUtdRYbtjIJtzT9qovrKYPaf0KgoNVJyP5n6HQb0H8VBwzoolhLr_oKuDHFboaGn9GoAY80dT4gWev_afT-dLjbFiun-LwSDKipEW5W99WUP5Lf52UEopLSrJbhkEghVFj1A8b0i-gO1dOs1mdDHV7N2TXCwf_YK9tj0hp3S_Rwbrb_WE7BFY_Diwa5NkaDv_D7AbJ_0cdjg3I5dyVDBV2wYveIZ7OuuUOxzgJFrY9B7BCQslo_NNCGrTjzfUnZb6uxzEXD7DrtLtzS7iSzNkLtbfMU_I-kdBm9G4Q70-S792yEjuONYr8tdP9cwu_czqks8vP2ARKIQg8VEWqhhpMdsgg_wBuGfODbhi22epAgMwIq-ABslXvO-OtuVwObTUkF1e-zwb5RT7K15NIZiPQi3w2gO05zug1rl0hc60OR_k2y7NI7Jb4J1n8BZwXsx5uvg_FJWDaU4HxK7qxUuvP3kiEuZcIGvmeIP6bREmcDSbTXOQkywXk8pWbRkj6Y0G8v5K-pjgD6bXF01Ms051SfcpwdNAxZTz_KXRruw==', 'id': 'rs_05bc15002959009f01690f681d7acc81959e56feda84693e92', 'format': 'openai-responses-v1', 'index': 0}]}}], 'usage': {'prompt_tokens': 3005, 'completion_tokens': 588, 'total_tokens': 3593, 'completion_tokens_details': {'reasoning_tokens': 576}}}\n",
            "Cluster 1: LLM-powered autonomous scientific workflows (32 extractions)\n",
            "\n",
            "‚úÖ Cluster summaries generated!\n",
            "\n",
            "Final summaries: {0: 'LLM scientific reasoning benchmarks', 1: 'LLM-powered autonomous scientific workflows'}\n",
            "1. In an isolated physical system, the total entropy can never decrease, consistent with the Second Law of Thermodynamics.\n",
            "2. In the AstroAgents system, over 30% of the generated hypotheses were validated as scientifically plausible by expert reviewers.\n",
            "3. - POPPER‚Äôs automated validation framework achieves human-level accuracy at significantly greater speed by applying rigorous statistical checks. [Huang et al., 2025a]\n",
            "- The Logit-based Calibrated Prior technique enables high-accuracy ranking of hypotheses by novelty and relevance in real-world data using LLM-generated expectations. [Gong and Fernandez, 2025]\n",
            "4. The Virtual Lab multi-agent system generated 92 nanobody candidates and experimentally validated two that showed improved binding to SARS-CoV-2 variants.\n",
            "5. - DrugAgent achieved approximately 0.92 F1 on PAMPA absorption prediction in case studies.\n",
            "- CLADD improved task performance compared to general LLMs and classical deep-learning baselines on drug discovery tasks involving design, docking, and triage.\n",
            "- BioResearcher demonstrated measurable gains on complex research objectives without manual ‚Äúglue code.‚Äù\n",
            "6. - Supervised fine-tuning, as a form of imitation learning, limits LLMs‚Äô ability to generalize to new domains. \n",
            "- Reinforcement learning from verifiable rewards elicits generalizable reasoning abilities from base policy models. \n",
            "- In DeepSeek-R1, rewarding models solely on final-answer correctness led to learned complex reasoning behaviors, including self-verification and self-correction.\n",
            "7. Experiments on a 2024 chemistry and materials science benchmark using an LLM-based multi-agent framework (with models trained up to October 2023 in a copilot in-the-wild setting) showed the system could rediscover many published hypotheses with very high similarity to the ground-truth innovations.\n",
            "8. In the actual experiment, dual-wavelength catalysis and solvent mixing were not employed; acetone and acetonitrile were the two best-performing single solvents in the actual research.\n",
            "9. - In SCIBENCH benchmarking across representative open-source and proprietary LLMs with various prompting strategies, the best overall performance was 43.22%, indicating current LLMs fall short on collegiate-level scientific problem solving.\n",
            "- Analysis found no single prompting strategy significantly outperforms others; strategies that improve certain problem-solving skills can produce declines in other skills.\n",
            "- A detailed user study categorized LLM errors into ten distinct problem-solving abilities.\n",
            "- Cited prior result: GPT-4 scored 163 out of 170 on the GRE Quantitative Exam, corresponding to the 80th percentile.\n",
            "10. In a human evaluation of the model verifier‚Äôs error classifications, two annotators reviewed 151 samples across different settings and judged 123 to be correctly classified, yielding an accuracy of 81.45%.\n",
            "{'id': 'gen-1762617379-ODf8uWR4nAPRMwfmXatK', 'provider': 'Azure', 'model': 'openai/gpt-5', 'object': 'chat.completion', 'created': 1762617380, 'choices': [{'logprobs': None, 'finish_reason': 'stop', 'native_finish_reason': 'completed', 'index': 0, 'message': {'role': 'assistant', 'content': 'Evaluating LLM scientific agents', 'refusal': None, 'reasoning': '**Crafting Theme for Extractions**\\n\\nI need to create a short label, something concise‚Äîabout 4-5 words‚Äîto capture the common theme from these ten extractions in Cluster 2. Each extraction seems to relate to advances and validations within the realm of LLMs and their applications, particularly in scientific contexts. I‚Äôll focus on incorporating elements of validation, systematic discoveries, and advancements in hypothesis generation and testing related to LLMs. I\\'ll consider something like \"Validating Hypotheses in LLM Science.\"**Defining the Theme for Experiments**\\n\\nI‚Äôm looking at experiment details that focus on dual-wavelength catalysis, which didn\\'t use specific solvent mixing, but it seems like acetone and acetonitrile were the best solvents. There are also benchmarks showing LLMs performing at 43.22%, indicating some areas for improvement in prompting strategies. Human evaluations showed an 81.45% accuracy in classifying model verifier errors. Overall, the theme appears to focus on the evaluation of LLM-based scientific discoveries, so I think a good label would be \"AI Agents for Scientific Discovery: Validation and Benchmarks\".**Choosing a Label for Evaluation**\\n\\nI need to select a concise label, keeping it between 4-5 words. Some possible options include \"LLM Scientific Discovery Benchmarking,\" \"Validating LLM Scientific Hypotheses,\" and \"Evaluating LLM-Based Research Agents.\" \\n\\nI‚Äôm leaning toward \"Evaluating LLM Scientific Agents,\" which is 4 words and effectively captures the essence of the content. It reflects the evaluation and validation of LLMs across various scientific applications, ensuring it‚Äôs not too broad or too narrow. This title seems to fit well with the themes in the findings.', 'reasoning_details': [{'type': 'reasoning.summary', 'summary': '**Crafting Theme for Extractions**\\n\\nI need to create a short label, something concise‚Äîabout 4-5 words‚Äîto capture the common theme from these ten extractions in Cluster 2. Each extraction seems to relate to advances and validations within the realm of LLMs and their applications, particularly in scientific contexts. I‚Äôll focus on incorporating elements of validation, systematic discoveries, and advancements in hypothesis generation and testing related to LLMs. I\\'ll consider something like \"Validating Hypotheses in LLM Science.\"**Defining the Theme for Experiments**\\n\\nI‚Äôm looking at experiment details that focus on dual-wavelength catalysis, which didn\\'t use specific solvent mixing, but it seems like acetone and acetonitrile were the best solvents. There are also benchmarks showing LLMs performing at 43.22%, indicating some areas for improvement in prompting strategies. Human evaluations showed an 81.45% accuracy in classifying model verifier errors. Overall, the theme appears to focus on the evaluation of LLM-based scientific discoveries, so I think a good label would be \"AI Agents for Scientific Discovery: Validation and Benchmarks\".**Choosing a Label for Evaluation**\\n\\nI need to select a concise label, keeping it between 4-5 words. Some possible options include \"LLM Scientific Discovery Benchmarking,\" \"Validating LLM Scientific Hypotheses,\" and \"Evaluating LLM-Based Research Agents.\" \\n\\nI‚Äôm leaning toward \"Evaluating LLM Scientific Agents,\" which is 4 words and effectively captures the essence of the content. It reflects the evaluation and validation of LLMs across various scientific applications, ensuring it‚Äôs not too broad or too narrow. This title seems to fit well with the themes in the findings.', 'format': 'openai-responses-v1', 'index': 0}, {'type': 'reasoning.encrypted', 'data': 'gAAAAABpD2gt4lFW8VwqP4-2HZdc7oN4_K8VuGC5ev3uC_yakVPJ-1wOoG19r90tFN9d8hn2Jhni6wnGiffUoEJ484plCS1XgR2bFMhPUvfrU0HwUgfQEXckmIMUUTGAyWfyfoWUrLnUUnCgdmXvYC5QzSGitgV3Hkc1vuzGW-60vLLNEtzeAASrVL_9J4nj3Xkgv_fGRqhOwBCYMaxaW_0RQufTwR8_DsMZXLHbA2i7ZTqqCZLkY2BwhzbTO1bIBElh4V4RK9JVh2uZFJVv3xU5QsOSnhoPXnzAStDsDvg5aZ90fX1P-WPodQoSdxL0sT0liDNvVcorlsPgRWtJxWrHOVUl1V7yf-4SxRnRKiEQVcHPXBt9P7j1hsIe802FzjbOduC5oNXFycmGZrj7ncowd93zW8hHeuHg8wManUwaxHDCFmAtc3CNGEPTsXEMkG-RQkh9U19OudfCpJhwt5Jur91qEiry9OWAMhO_hsFAsMj3Bc_IJlzEwmOOGEOalhVPaQ_9D4gacQx8uSA4N7omxaq2fCIQawTbRsr967jyDeWMFLYriwX8s9YQKZcwuenmlqpE3DiDd21qtdiTFoYmy-_Yevp-odx_S_XmvB0OEpG5MY38sBuGJW-2yt2PDY1ED5j7KfVcnxeIpuWWj4s4BHaCh3kDHNaqqtaS__sN_Judw9hLoWBi7rp89tcjCJKXoz24qjLJOJ7NH2pQWCeWnLd0x1ScxOe5M0Gjt-FvO7sN2YkcUltlLukn9N0U3atIP_ru_tqwHEi0a-Xto8j1M87rLlKu1RrQfyjCdIqv275PALaV-EbD_DCcRx4w0_5B5CQxQdTeJAJO9NW8m4LU2VfuoPXAi2ZGPazJVKeXVYKaCzqSFxgFJK7abijbxvXReXWRrcaBj6hxtVFXZITmChbvcOEsc-3LHLfgnOKHDRzW_Bfo6EpJpaI-2vPUpgr8Zq3HKv52BThBASeSFb4dBtoJzDFEX3IC-LmTWcaWHkLiT6p3KQnBSel5-BtE1R1JPbDrK7rcMOwkT4Dz7iVgeysGtmm2nQ2faSsgkmGFmElC1jifrT7gbEB4UguPtuYuZyk0hjDWJtzTAVPvKDk8x7_1eElJoXVYkYB2pm8qe-hx6g2m701QJHS-n77ptSBaCFWRKdZX_49TBGxmED69MsSaEXo1MZAcnNCfJDLCmTqzRiIbczgyODgdrrTUvcTJbGFBO_sGR3kTfJW1Us-0dcLMOk3QGd4PaEaiIQDXqpwFekYRjK6LPXZQNAs8ItckOzzubvCC9zm3zRha5G8lIMvlLakPCQdLQaBlUN2j2-joVI-7NRwiwAbpnVh7U4oOV1H2ShhtHhxoxzpNrdxgx2YlISdORJ81D4mHFoSdKdsbv4x2j4MGKkwB6BHzg14xRK3VdiXJTzq382tLDYGPejNXMOz5kEye-xp1vfjxud-u_kYhjOZbDixd03C2JTG-CvpVvxbDm-jo8jDFnVeNR_jv-wOegLv3adeeqBgxqslX4r2bO5siQcle_gYgbBRmxvpaeeHSlV9xmKzOmV7ay_pmS-Vzr3K7VywSlsiaV9OiHRVaDDJBmDVkhDDM6Iq0UoLgCZBKU6ZN0pGxHYKCxyAzJzer8GAms_PqxXyL8CQVLoG83nW-dNJhuz5VsXaUXCy_so9dj0v85IObKLTu5zlCoBtjUlfe_wrJpnqANSdLdX_ewTeYA0O5e0e61lpudAQbJS_EyN2Kypvqvr4ypyTTZSIUl_eUetrZnx9DFeART6IaggtNS6TLlPtRA5JgjZxMplcl5t9b4Vl-BFR-IDgzyBCz9I09KWUHjCd_-KtU8D14dn8pmA621hxsfqipGuthKOg65q9U4kA4tUmMfoWOd73QT1kkLdhjIVKoNDqoRjR4vHiBwY1x4anMGRnU4pOmFtSM2-MtbZ5ImYoplcNQmFJPiACMd3-01Kq2unCt_xWAzXPb2qAjl9p9YtC2wAIxPqJrOIBlATnp4ZY_yv_7PrI14SK5TnWEfkj_MVrQHck-ZkRL_TL_GJ-nSU77CShoTI-ur4pH5r2FMxYyGPz0Nr4Dsudhj8B7vpKrC6kAtH1C_MLIbu4Doq8RQqM7DlsHCMoDNRdWBqOX3tQIyuCMw9v3wscar_n7iX61F6xsHNh1BkmPgj3wlrNWvRJ9nOeXKF0X9tt9k7GpMT9X1_Rh7G5_hnerdpTdxhDuc_9xfvw1ukfP5HRd55uzEdTAE7bAsaohiXMsO_DjctPd_UaKMdrJb4phTC57v8q39YSURGJ-G-7Y4Y3oMU3lcy5RcL55rA0QNUiMnCIaJVuANoxPbN6RGfBWwJ-RZho-szINNQ26WbgZoTbBHf0DsklyVsW1hF38zlU2-L1ztYpH0aAP2EAZqh7yd_jMEK0KFBfOi0sRmj2n_bNdhG4jF9xEVQ0AWlUp0F4dIIfLuJ1GyZA6iEMLEJ9BgnhRZTWZNLNTpz8grv6g7JOAb2KobDtOEKjvj3V3Z5jQXkQX6a36Ci3VC0YYXbqTLTDzdnh5ChkP7IrdqhxMZHydoIUOijD4zc2_plg6exV-vtmlhZu0Fl2THi8K5s6nzxiw93fCytUXms4MzgC-fl8qCOdHf5NctYn_dnze-JP8UUpR-ti0ix6xC6BVXf1oCJkLcjplfRue8h2b7ZWUZ2ikMvM03Ch1L56bylZf3PUXttBlOFAZ5bZo_lHWBVtOi3F4vpDBL_TQVJPghPN_5hgF4_FA_rdCN5ydTwtzJk1yYJs1XuVx5Avn4_sFbbcZ5jWyrOs_doIVduaMADdHJgV-mwEqb2zcp70lNCVTx8ySoQCj_O272JZSDv8qcVqiqnAIr2o0L4acqcz8xkNh6ZwomkJdxWEY64NqwvFwsTib9nb4BdLA1GJao9zvbU_TjwE4dIg8eEp9p8PskgjSvD6g7ywfNbRBieKzyBtjfVhB9CxDImI-YLqzjHh-2OjJrAKG-ZzYzEHucNqSs1I_JfYg8aEnBKuw0eA28gbUbwbCXitI4-kBo5MMbI_pXDnEm8lnvNUL6dsTxO89T26tdOI_c45BivrS4wk0LAjeokr6U_jGZvb1_5m-5tQbifGJrXJ6psHUgavH0U_dDKCBvwyx4veq9m1Xhi_K4V03Ty_PEm57vyPSVikqgtIoOmaey8OHk4nTaY8sk5KewRRQSXOlpBPlzuJt1hxR7E4nD3d8Gv_zV3zcfRf4VXWjARaYGRhjxzQtjMg3Kad6KCWdudi9KvRN9ellvRZi872F1YfVs_OFkiy8iYjcNmNGDZRzlgNfEI_q-XYSAsp9xAOPi94RLGW5aTkGjnGcboDjmjSJINoG-U3RC3eEpsnWKXsV2LeE99Sl9VF4uawI7l1bzw6TRpiwPsOx_LoI-w4p_Mc0ycESTInZ5IhPvXSWanYZgq-9ax-jrKuchpqV_vu2YC1ikFuNVajZgxJ60sD822PIwDkbXCKAHF-Y38sO1rtPUeenbjodr0S-G-obm-dT7Zi7IPFEclkf7_WuX1k-kjp-9kBDd0ZQ4LmqeuA21bjPQQjfpnZAlRu8fuH0D5k2NieLJ2U0oFTh0htZ6TJyUz59HJSWWK6HW57pdHBRdIXta6jim9bCgWorLJTgsau5GsEbyRiEwuF2JjO6BJVTMMGIOdQc-1n72-4XLW9swKTPy97yY1h9uhi_BoeXqsRcb4oEb0tm0utCR0ROuCjQPr1Saa-ZiRQNhtt-U646O5nJ7W00GXqtrR-MPTjfukRPyjaz8yC0avQNoLKNByuLsvAmfAl7q_PZWXmEKvn6O0jdM7f1utX_cMcA5KJdd_vGUbr81dSFktbuiNQx75fhEXoc8Kc7VhUFnRkHbTJTqE0n_5NUOGDY16hoGCB0YnFdJok0UUGslbRKMoUwP6z6c9sUHTrc3b2egdMIYX80obSkKCyFyLu5sImWxJYvvFDNOtdVpcH4cuwItmZ_ej4x7DEJGaq8538HrjG4ufiCyxorveDPVZAoa96_SQVLfFhpgnlKYwnCLH2C02IuO6vtwhyqzDydF1QxYocINay-OFNYq9dAzOl_SQ_aiEISgCH-xIc2PG4wrocBHUxrzvhHntXoXL05fAnGrjFpqkfjB2LB2oT_DNpF9EyUlG2ZBl1SnyH54QbQwLf4_0WrXDk77kRagTbuDOOg3pg_YY05hyORKg5V8JDwRVFNFsxunGdugPwpgTRkagPzhOLY0z-iSjZYUAyyYOsWPIeLDZYj89dH1oh-Ti67pSohlM8DllNc4nJzqOasXnFq4JosrmVwSKgDxPrr0z0ESwYZ3AZsQ1UzkXJuk9T4SBp8Tdb0LVQxf4dERWWTISSAhEhLVraA2ZSH5GmMhmFDmNu82QwAdcp7k3hWAkPwIPfuqeITEBp48a8ARLNvHXXlOLagJ-Wqt8EpDU3n5BV7yu7inNJTiDTE3szRt2Izut-AOV9JVwsbbOxUaMb2asqs0LP9wcKmFT0k_d5uW9DrEScvUaJZ0oKUsYdZ9QDhI3aDW6YFqm8OJTe0IsYx37TqjJp9FN6CdAUR4C6B5cmnF448tKNqXLDw9Rh7yBleHDEYRUP_3nbvFBk1wMyG6MdAvUCA-8Q-nnPDaa4fK_s-WmugWdlRlVLAhGpR3kPCuCKUT7sGkeahDk8eg_-4y2Wev5yMbdG_y0gqiUwf6Qzp-PK8yxyOXMSqmETW7wbYvdSGuciOu3g_PdUCSQM48uNAp3fvf6nRXowb-1r9uGrFtpe_-OqI3rxvKnHHc-UF3umiKA02LAiYRjsQo8vExX1_e_n3k4cC7T7TqysB54p83RjukgzEXaCaE4IWITPSbxELJ0pm04e31tKZgFPveGwJWqzqL9MtUXDxnqisJdePZNNZmMlJxIJlFZjBDeOlCWqPJbh8ZJeGto1C-tC3cbczKbNCmAGzwonziPELhr1Dh8QCWYiXJUWibtSbsGMk9luourT3gggQjwEOz2acqfc4_Zf7r8ydJqqIcj2RiPyt08ZAQIHepQmz7aKV1IwFJdEQJt18nZGUT6xAx0O-IwU9SHBWPXR2q6rhE0Abjg3J9slluKRPgBiLx34Y65Aq3gGMR4CRJV5tIXQZKsKBhk53aDtPveqc7xfB-QtFXbyEAcs-HpyZXffWV2vB-YIByYj0M4WsGT9K4Ns2eZt2T9gtv9pHkPUPRrbjY5gt-cH0lXAQ_pFQNduRPeJ0U4smLIC6kwRJ5qDKdgifpPB8aTN_aUiVu4u26Mbc1LDX4Peg-FOXgnAsklnAP7vYnNi6bZzH9ESMglMGpF752HqkqUPVPZw6NHrzDpzFalv-2dsG_8gCiPUMzHYmAg_r4l9j6zA7kWZJkc9xc5wYxLgh-qbGT-KtO7FBGuZ0Ayg2kr-AGgEOGSJCE67_eCD1ZxXSzjKPdN3M0aKtq54-CDp7sRc9YHdmyW79irrYfvL32Gl_wsH7lNY0WYKt846_XSAuquXT5sAJjsfjzBSXDV7aPoPNLPeqKwc9vw376tEVgJRZhuphD1sKEu0D8FsM35WsvF3p2nz1F2pXgX3h9zzUEIu3JDLChRAf', 'id': 'rs_00ef587da64d34c701690f68247eb48194895c2361840cbfc0', 'format': 'openai-responses-v1', 'index': 0}]}}], 'usage': {'prompt_tokens': 775, 'completion_tokens': 716, 'total_tokens': 1491, 'completion_tokens_details': {'reasoning_tokens': 704}}}\n",
            "Cluster 2: Evaluating LLM scientific agents (18 extractions)\n",
            "\n",
            "‚úÖ Cluster summaries generated!\n",
            "\n",
            "Final summaries: {0: 'LLM scientific reasoning benchmarks', 1: 'LLM-powered autonomous scientific workflows', 2: 'Evaluating LLM scientific agents'}\n",
            "1. A prior review systematically surveyed more than 260 scientific large language models spanning general science, mathematics, physics, chemistry, materials science, biology, medicine, and geoscience.\n",
            "2. - A framework combining LLMs with causal knowledge graphs analyzed over 43,000 psychology articles and generated well-being hypotheses whose novelty matched expert-level assessments, significantly outperforming LLM-only methods.\n",
            "- ResearchLink generated cross-domain hypotheses with high precision by integrating graph embeddings, path-based features, and bibliometric data.\n",
            "- KG-CoI grounded hypotheses using structured knowledge graphs, improving the accuracy of reasoning chains and reducing factual errors.\n",
            "3. - The Self-Refine framework, which enables LLMs to critique and iteratively refine their outputs, has shown significant improvements on reasoning tasks. [Madaan et al., 2023]\n",
            "- Iterative introspection significantly enhances code-generation accuracy in LLMs. [Chen et al., 2023b]\n",
            "- Self-correction methods alone are often insufficient due to inherent limitations in LLM self-assessment capabilities. [Huang et al., 2023]\n",
            "4. - ResearchAgent‚Äôs human-aligned reviewing agents simulate structured peer review and significantly enhance the feasibility and novelty of scientific research proposals.\n",
            "- MAPPS uses human scientists to verify generated workflows for materials discovery, leading to more robust results.\n",
            "- AI co-scientist integrates human expertise within an iterative ‚Äúgenerate, debate, and evolve‚Äù framework, ensuring robust scientific outcomes.\n",
            "- Across general research, RLHF guides models toward behaviors reflecting expert evaluations.\n",
            "- Domain-specific scientific agents are beginning to tackle tasks such as designing novel molecules, predicting protein structures, and discovering new materials with unprecedented speed.\n",
            "5. - Integrating code interpreters enhances agents‚Äô mathematical abilities, and RL further optimizes multi-turn reasoning via real-time code execution.\n",
            "- With search engine tools, RL helps agents generate more effective search queries to acquire external and up-to-date information.\n",
            "- Web browsing and GUI agent studies show agents operating in dynamic environments where states change in response to their actions, necessitating interactive decision-making.\n",
            "- RL has been used to train agents to coordinate multiple tools concurrently to solve harder tasks such as deep research.\n",
            "6. Historical evidence indicates that major scientific breakthroughs‚Äîsuch as penicillin, the cosmic microwave background radiation, and graphene‚Äîoriginated from chance or unexpected observations, underscoring the pivotal role of serendipity in discovery.\n",
            "7. - Yang et al. (2024b) found via expert evaluation that LLMs can generate hypotheses in social science that are both novel and sufficiently valid.\n",
            "- Si et al. (2024) showed in the NLP domain that LLMs generate research hypotheses that are more novel but slightly less valid than those from human researchers.\n",
            "- In this work, using a benchmark of 51 high-impact chemistry papers published after January 2024, the proposed MOOSE-Chem framework rediscovered many target hypotheses with high similarity to the ground truth and captured core innovations while using an LLM with a pre-2024 knowledge cutoff.\n",
            "- The framework achieved surprisingly high accuracy on the inspiration-retrieval subtask despite its out-of-distribution nature.\n",
            "8. - From extensive discussions with chemistry experts, the authors observed that the social-science assumption that a hypothesis can be divided into background and inspiration applies to a majority of chemistry hypotheses.\n",
            "- Prior cognitive science research has shown that creative ideas often result from the cohesive association of two seemingly unrelated pieces of knowledge.\n",
            "9. - Experiments using the constructed benchmark found that LLMs are highly capable of addressing the three fundamental questions posed for hypothesis rediscovery.\n",
            "- When tested with only a background and a corpus of up to 3,000 chemistry papers for inspiration selection, MOOSE-CHEM rediscovered many hypotheses with very high similarity to the ground-truth hypotheses, capturing the main innovations.\n",
            "- The study shows that an LLM-based framework can largely rediscover the main innovations of many chemistry hypotheses published in Nature and Science, despite controls to prevent data contamination (LLM training data cutoff in December 2023; target papers only available online in 2024).\n",
            "10. - On SCIBENCH, even with the strongest configuration (combining chain-of-thought prompting and external tools), the best model achieves average scores of 43.22% on the textual dataset, 13.8% on the multimodal dataset, and 51.57% on the closed exam dataset.\n",
            "- Experimental results indicate the dataset‚Äôs complexity and difficulty are sufficient to differentiate performance levels across different LLMs.\n",
            "- Analysis shows that chain-of-thought prompting significantly improves calculation ability but is less effective for other skill dimensions.\n",
            "- Prompts that use external tools can compromise other fundamental skills.\n",
            "- Few-shot learning does not universally improve scientific problem-solving skills.\n",
            "{'id': 'gen-1762617390-T5fO2l0WcKxHvF7yCzHz', 'provider': 'Azure', 'model': 'openai/gpt-5', 'object': 'chat.completion', 'created': 1762617390, 'choices': [{'logprobs': None, 'finish_reason': 'stop', 'native_finish_reason': 'completed', 'index': 0, 'message': {'role': 'assistant', 'content': 'LLM-augmented scientific hypothesis discovery', 'refusal': None, 'reasoning': '**Creating a label for themes**\\n\\nI need to come up with a short label that captures the common theme of these AI and LLM concepts related to scientific discovery. It seems the cluster includes things like tools, frameworks, RL, knowledge graphs, and hypothesis generation. The main theme could be \"LLM agents for scientific discovery,\" which fits within the 4-5 word limit. Alternatively, I could go with \"LLM-driven hypothesis generation and evaluation.\" Either way, it‚Äôs important to be concise.**Determining a cluster label**\\n\\nI\\'m focusing on generating a concise label that reflects the cluster‚Äôs themes, which are centered around tool use, RL, and hypothesis discovery using LLM frameworks. I‚Äôm considering \"LLM-augmented scientific hypothesis discovery,\" which is four words if I treat \"LLM-augmented\" as one term. Alternatively, I could go with \"AI agents for hypothesis discovery,\" which is five words. After weighing my options, I think \"LLM-augmented scientific hypothesis discovery\" fits best for its specificity.', 'reasoning_details': [{'type': 'reasoning.summary', 'summary': '**Creating a label for themes**\\n\\nI need to come up with a short label that captures the common theme of these AI and LLM concepts related to scientific discovery. It seems the cluster includes things like tools, frameworks, RL, knowledge graphs, and hypothesis generation. The main theme could be \"LLM agents for scientific discovery,\" which fits within the 4-5 word limit. Alternatively, I could go with \"LLM-driven hypothesis generation and evaluation.\" Either way, it‚Äôs important to be concise.**Determining a cluster label**\\n\\nI\\'m focusing on generating a concise label that reflects the cluster‚Äôs themes, which are centered around tool use, RL, and hypothesis discovery using LLM frameworks. I‚Äôm considering \"LLM-augmented scientific hypothesis discovery,\" which is four words if I treat \"LLM-augmented\" as one term. Alternatively, I could go with \"AI agents for hypothesis discovery,\" which is five words. After weighing my options, I think \"LLM-augmented scientific hypothesis discovery\" fits best for its specificity.', 'format': 'openai-responses-v1', 'index': 0}, {'type': 'reasoning.encrypted', 'data': 'gAAAAABpD2g0igK-GipEyElYBWFGGYwxMA0PZOyhWCMT-kmkSNmqqw8ly8Y8OttKkoOFosgCAvyg0Tu80Ixa5K3O6YgU5FuYFldF8dW5orzELqxLdXPcdobRJbmz64Be1e5SVa1BMSsbENZMebc5dropMZzNs_w6V8WttEip0lpU_FvEb43ZZcXJUvrQqjRzCepQsQEdLmcgwWBgtL4qSzSKsEGfNCw-v9t-UHQh_vBarXPMKnT-1eqiSHZFM1vCaBiYu2zHARS9Y-KOjbAQFakblnmHJ_90sZFo-mzMHLPcSBxTEiq7rB0ChcDav5AfQ96R6dU5_U9pCBU88qggiCqx8gaYpdHs3EaqkyCfy7OhLEUiKA86ZUrnCaUuj-OpsF_akpA_cH3bZNzrNwaQ1JnRitfyxxC_JiT0l23cPPdc4-_oWj0VbvBnfaX6od7xgofySpH3QnY0w_C5bbe0XyHazgAaUnA8HtCb-hMOId-4tDPy-xVXtGHEmzbROqIQ1vQeLLYaR3FK8mi1ohYkPT-2roKyGRJOo4ENMk0cc7QLY2j_pEXGcGW9vGxAQFbtUmz6F6_RxjxAzEGig4I1M3S87dmu5UtAAXFzqmqGGRt0HXYz5Gt5vgM5shIZWVwIXtf7B42b7Htae-dcGZFFpkgXALG4KCbzjXXa7VOBUXJVyoxnY5U7a7axZbNFZTlTs5ILzmZpMlIUUY5rbhEgdjpK8v8lDSE5pmjBwg7xJ6BSwJobzKU93aeOCDbSmVh3yMRS-voOkz6Hba-Nd8blwXS4yjdAtRGn3dBdp_ExxSzRpAtXWmAa61vNyNUGpli7GuSibXdF5U89uQSNQwfQXkyInAJQwULHutZ8I-yXQBKAPNoV2af7qVb-g_Hfsp2kv9fmZkhNjXiItdc2-vgdmOGKEDkS7gakT-vG8Kt-VueQVjqamsquO2xgusB-WK23DHb17DKtxlVBJ4GLkUwMVh28J_nMfHtfrv9wz6ueyl1b8CD1ZHzr63wVfah-Bp0wrBiC5QoSplVobIPfjUJCmtW9Sv-vEyOu2_YdrAw8wJIARvmW6m3xm-iNUaULyFYHPPTB0ceub9ja591LeruLe9aMj4nFf3bpxWV9TJU5cI32adX6dZ2d0IOrU-gqrd_UlR_BFfRTsutxnSo6OhDe6hTM-OEhgHudCcp_-lrmHMhNfQOBd__72qb6TyFP914O5q2HvD0FdGW4eXYaonsOVuzKzyWzjITQEMI6PKRTdfEYa1alroB3dPEwglxsbAUj4W3AncB5hGzhpe5A8R2opI62ErZzAUlCYVd2naff1htU8w9T1f0F68c7O2VoVul8svbkGvQMr_WS3P6ntlCqGKnikB1elbVgJdKYvvxt_1RRhceU5ZTWDH50CcyDlO7ZcVutawGmKTfZKvj8IpMyZCS1EuQ3vRU26EAshdJbFBIqI_dxJTo0w83aBUyxsPcFd7q9myAoiRw9pAoHLyaZ4z5gto9ADu76ncfELynPSYSdXTAY88xjm9yTUwwPjqT03C9SmRmcirYSS6DQjmFEhUbhlU5G-flXaka_xydR8UtoGXfO2mMftkkVVIJHmrRIy3ZAoMhkjBRONWWvscSfaN2YJ5-QkO6u_cQE3zwzW3fqlUCKq1jpJ5aMJWK-ZgLxbd0ZAlEtGtW0RMpRNh6M56CtXJPmtJUo2Lbo8h9sRRE94bupXNBNKr1W22lKX9163FbSgqXKUFOZjL26iPXzaXQ4VGsKNUUBkHokB1FWsh4vJk-Y7mMMRxKmyLFkgpZ6FgBbwa-qiv6MRbGJw5eaLwptNViFxbVuYXfn25_VZSGExSA88fdYQIN8ApCKsJfUkcjouIOhpPu5uFKPBv6hpP8IzIfDM1dgVpKCdWLmBMCHfTszW_Ep6Sku1BJ8f-Wg2hM37ICfgH4X7Y1DHK3mYKwaF10Sn94_hGlUCI0UfBcBMvZOiHqnWWpRqQ28fu0vCyzZhbqiS51srk_bvG-9YNE_9USVuK3ndd8EyjuKwhPZQcUB0ZVF0icGz8cA2lyU4Eq4tBPHzR83V17wXWHmybqMl1jCtgZ7h9Sg9SWyU5_S9ItS10OWeOsG7VtpGvAVpzZz7AH1IFpGbtyqEfJj4odOhad_3WGkVV9OcpADUT6THQ8HfIWhDI1ghFAOmx7mTq4SmXHstYnaEIE1GKiOv9UE_mvsOBxljLOtXm0lbfQZNZwsHzoU5-GBYBXC6DwzoqpqSotc7QHHfnLVe5IJgs_DSsNxCK7EvpeQTNsAhc9LTKD34J6TcnV-Do6Wr1OQly5ObZeYXgI-2P1U8v5TXnaj3oqCAUHFl57_3zx9KuvvUKmCOelJ70ZhGlvpDdKo-plJv7SHhmSK1dQfVVv4cn5v0q11FZSNHxgvuZEMGH5lTsKd6ctj8qTn0MfgYxEmNg-LG9YvGQYN4cMjrnfx12a2LB7dAv1Qe22pTEt_D0-EQATT-q-WueiqvZmKlCfQ1sTKeyZsW5zjnyHRjKHer-UuiN8qSW7T4sxgNbodjRMST3EO-7xWet7zcRmbH9snE_baUlfQjUTVToFIzte4JlJJ1t2DMCyBsyjm3f5lstEv3ivWKddvsdhGY0-zqwht4e8cljnC_-X7ACMhbdjzADMzjdQ8IYDog3NPhZ3yosCFTMKJiBpEOM1vhRFUap7auzcLEhZugpj-uC20FvhEiVT-m3ua-mKJJw93QG-MSwOYTM8rwGUfxjjIBrIF-XTHWBQLwBNhUM60Jsmo-FOh3BFqEvdbeS29Bc8DCHcQNPKr7wHSFBMp2B8-kIylPPX8mmrJ1nt07gRSDSwlDaBnOw2YMCmIyA0yNwCcTeXXDPmPY2XtRN5F6aptkz_YHyq9mdapDbWpoMf1OrOb--yDbjih9f0vAqS2QnJlv_898kN9_w_FxRRl66Fps7u6oj1o_K0QR0Ks7J26szvcO0AaiRt1FMiSckUSEa-gZtE5ApBcF3BRL3F2FslY3AAOz0-CUn9vBgh8zS4iPvVBzl4xE0Iapf8zB9oykbqOx8KeFYTdIP_84DTGzDptvnGSABOWZHMsv421-MNDS4zeV6kJhIUEdLPmwaZL4Qd5D-GKhhaI6d9VbsaPftcK20xN0gilCGYAgLEFxuLLeEU05hYMU98Sy4soydmSIuXb2BtATHl1X6U15-0sXdEALRlUdXuGyNHMz9Be3onqLgmMyUR89WQIMdl4CjtCIzZbyLJAol33SGXovKV-NR7YJBB3CR2edi0rKPoabK6jION44gfaZqR22xMi68p-TqGx8xu44-8pgm0H2LJ9fwp6Gp9ktLWfyrpJYtenJapDHewQtpjgHTsq4DlzGLASd3R2qfxJjgqFN83ydTw8G6SuGUhCOcLkKWGxkA9ur5lVs-zU0Cx3p-sgCEG1I09YwxD9-qWrjLK1UsbAsyVPkkkDqQgpsruG-i7CBihaEAjB9QQeljkzeH70_2G_Q5AXrcejD6a53fdVZbNDta5wkzMnqN_aeW9bJAnNCyTP0vSXbsuS3ioDExI01i5xaxe6vz_NRnlPzIjTd4N2Zs3DeE2cIsDKc3D2FOjOOHNxpa76jGRDrfknwBM6i_R4omt-4j67ndIywnkKMEXBiAwE31AZhKsoigFF2flY7DE3DEtuzB5UhFzEzCmCBn-XQiI_H8axL5AxJKqgSjgjSUup07k5mc8BdpAscVjJhMrwIQKMa_VQ6sGG6L5t-x2MRLsFEaUt_Dfms2xUOoT1ZaROuYwaxXIXMZCbV3xCwa4w8FPc74hhEPmQsyB6FS8w7VXoFftty1LauYWqvdcYi7NKloQpsKfG8yIlzPVbL2zhDhHZ', 'id': 'rs_09b082208dc3593c01690f682ec4b88197a50a139688640a1b', 'format': 'openai-responses-v1', 'index': 0}]}}], 'usage': {'prompt_tokens': 1184, 'completion_tokens': 462, 'total_tokens': 1646, 'completion_tokens_details': {'reasoning_tokens': 448}}}\n",
            "Cluster 3: LLM-augmented scientific hypothesis discovery (35 extractions)\n",
            "\n",
            "‚úÖ Cluster summaries generated!\n",
            "\n",
            "Final summaries: {0: 'LLM scientific reasoning benchmarks', 1: 'LLM-powered autonomous scientific workflows', 2: 'Evaluating LLM scientific agents', 3: 'LLM-augmented scientific hypothesis discovery'}\n",
            "Projecting to 2D with UMAP...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/umap/umap_.py:1952: UserWarning:\n",
            "\n",
            "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ UMAP complete! Shape: (94, 2)\n",
            "   Visualizing 4 clusters\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"6e75cac1-c0dc-4e28-96b2-126b8b873729\" class=\"plotly-graph-div\" style=\"height:800px; width:1200px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"6e75cac1-c0dc-4e28-96b2-126b8b873729\")) {                    Plotly.newPlot(                        \"6e75cac1-c0dc-4e28-96b2-126b8b873729\",                        [{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(141,211,199)\",\"line\":{\"color\":\"white\",\"width\":1},\"size\":12},\"mode\":\"markers\",\"name\":\"LLM scientific reasoning benchmarks (n=9)\",\"text\":[\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2410.07076...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- The TOMATO-Chem benchmark contains 51 chemistry\\u002fmaterials papers: Polymer Chemistry 21, Organic Chemistry 22, Inorganic Chemistry 3, Analytical Chemistry 5; publication venues include Nature\\u002fScience...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2410.07076...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Under Claude-3.5-Sonnet evaluation, MOOSE-Chem achieved the highest Top Matched Score (4.471) and a higher Average Matched Score (3.697) than SciMON (TopMS 3.824, AvgMS 3.559), MOOSE (TopMS 3.529, A...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2307.10635...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Existing benchmarks ScienceQA and GSM8K predominantly contain problems grounded in grade-level subjects (Lu et al., 2022; Cobbe et al., 2021).\\n- The MATH benchmark introduces high-school level quest...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2307.10635...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Existing LLM benchmarks for lower educational levels predominantly focus on basic arithmetic operations rather than advanced mathematical computations.\\n- Most existing benchmarks are textual-only an...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2307.10635...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- On the SCIBENCH textbook dataset in the zero-shot setting, GPT-4-Turbo achieved an average accuracy of 40.99%, outperforming GPT-4 at 33.79% and Mistral-7B at 6.23%; GPT-4-Turbo outperformed Mistral...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2307.10635...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- In zero-shot settings, Chain-of-Thought (CoT) prompting reduces calculation-error rates to 13.6% compared to 29.0% with the vanilla zero-shot baseline.\\n- Zero-shot CoT increases errors in other skil...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2307.10635...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eOn the SCIBENCH exam dataset under zero-shot settings (with and without chain-of-thought), GPT-4 achieved the highest scores in 6 of 7 exams‚Äî58\\u002f90, 44\\u002f75, 40\\u002f56, 50\\u002f100, 80\\u002f100, and 25\\u002f95‚Äîwhile Claude...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2307.10635...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- In Figure S9, for the de Broglie wavelength problem, the ChatGPT solution using chain-of-thought was classified as error category 4 (Causal Reasoning) and produced 8.09 pm, whereas the non-chain-of-...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Geometry3K contains 3,002 K-12 mathematics items (Lu et al., 2021).\\n- GeoEval contains 5,050 K-12 mathematics items (Zhang et al., 2024b).\\n- VisScience contains 3,000 K-12 items spanning physics, ch...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\"],\"x\":[8.050034,8.254594,6.3672776,6.0833488,8.376924,8.7389345,9.131671,8.549357,6.1215606],\"y\":[10.26234,10.32924,7.0741143,7.2769094,10.455448,10.300976,9.987775,10.368286,7.2708235],\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(255,255,179)\",\"line\":{\"color\":\"white\",\"width\":1},\"size\":12},\"mode\":\"markers\",\"name\":\"LLM-powered autonomous scientific workflows (n=32)\",\"text\":[\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Early evidence indicates that large language models can propose scientific hypotheses in biomedicine in a zero-shot manner using only pre-trained knowledge [Qi et al., 2023].\\n- Domain-specific found...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- The evolutionary LLM-based code mutation process has produced entirely new algorithms that are more efficient than human-designed counterparts. [Nagda et al., 2025]\\n- ShinkaEvolve improves sample ef...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- SpatialAgent autonomously processed over 2 million cells across spatial transcriptomics and MERFISH pipelines and achieved parity or gains versus automated baselines while handling full projects wit...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- SAMPLE experimentally identified GH1 hydrolase variants with at least 12 ¬∞C higher thermal stability than the starting sequences using autonomous design‚Äìbuild‚Äìtest cycles in a robotic lab. [Rapp et ...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- LIDDiA generated molecules meeting pharmaceutical criteria across many targets and surfaced promising EGFR candidates. [Averly et al., 2025]\\n- AgentMD automatically selected and executed from 2,164 ...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Coscientist autonomously executed palladium-catalyzed cross-coupling reactions in under 4 minutes. [Boiko et al., 2023]\\n- ChemCrow integrated 18 expert-designed tools and successfully performed auto...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- MDAgent automated the MD workflow and reduced the total task time for thermodynamic calculations by over 40%. [Shi et al., 2025]\\n- The Materials Laws Multi-Agent Framework used LLM agents for symbol...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- The k-agents framework enabled a fully autonomous laboratory where LLM agents planned and executed experiments on superconducting quantum processors, producing entangled states with performance equi...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- LP-COMDA automated the design of complex modulation strategies for power converters with a physics-informed planner, accelerating design time by over 30x and reducing errors by over 60% [Liu et al.,...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2410.07076...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Across 392 comparison pairs, the agreement between expert evaluation and GPT-4o on the Matched Score yielded a hard consistency of 0.345 and a soft consistency of 0.542.\\n- Across 48 comparison pairs...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2307.10635...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- In the Physical Chemistry example problem requiring use of the Planck distribution to compare energy outputs at 450 nm and 700 nm at 298 K, GPT-4 with Chain-of-Thought prompting generated the correc...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- LLM-powered agentic AI systems automate routine scientific tasks such as data analysis, hypothesis formulation, and literature synthesis, enabling researchers to focus on more complex work (Paul et ...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- LLMs trained on historically grounded corpora exhibit regression to the mean in idea generation, prioritizing statistically likely continuations, producing variants of well-known ideas, and rarely p...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Tools such as MOLIERE for biomedical hypothesis validation and SciAgents for structured scientific discovery have demonstrated the efficacy of constrained knowledge generation (Sybrandt et al. [2018...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- PubMed Abstracts contains over 34 million abstracts. [2025]\\n- MeSH comprises 27,883 descriptors for categorizing biomedical content. [2025]\\n- ChEMBL includes over 2 million bioactive compounds. [202...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- DrugBank contains over 14,000 drugs and 6,000 protein targets.\\n- AI2 Science Questions contains over 10,000 multiple-choice science questions.\\n- Materials Project includes over 133,500 materials.\\n- ...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Integrating ocean salinity data with atmospheric pressure measurements has yielded previously unattainable insights into climate dynamics (Wang et al. [2023]).\\n- VirtualPlant integrates genomic, tra...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Dynamic retrieval in RAG enables extraction of up-to-date information from diverse sources, enriching hypotheses with the latest knowledge Beltagy et al. [2019].\\n- Carefully engineered prompts guide...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- SimHypoth employed neural network models to uncover non-linear relationships and generated hypotheses that included correlations between demographic factors and societal trends (Lin and Lucas [2023]...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Simulation tools provide scalable virtual environments that can effectively pre-test scientific hypotheses. Qi et al. [2024], Schumann et al. [2024].\\n- Simulation-based pre-testing methods have prov...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Open-domain hypothesis generation is more challenging to validate and assess for relevance, despite enabling novel interdisciplinary insights (observed in prior work) [Sybrandt et al. [2018], Wang e...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- The PubMed Abstracts database contains over 34 million abstracts.\\n- MeSH comprises 27,883 descriptors.\\n- ChEMBL contains over 2 million bioactive compounds.\\n- The GENIA Corpus includes over 2,000 ab...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- DrugBank contains over 14,000 drugs and 6,000 protein targets for drug-target interaction studies.\\n- The AI2 Science Questions dataset contains over 10,000 multiple-choice science questions.\\n- The M...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- The AHTech Electrolyte Additive dataset contains high-throughput screening measurements for 180 small-molecule electrolyte additives tested in aqueous zinc metal batteries, with each additive charac...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- LLMs often lack formal mechanisms for explanation, causality, and logical rigor, so their outputs require downstream validation via simulation, symbolic reasoning, or expert review [Wang et al. [202...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Integration of multiple ontologies has unified knowledge across physics, biology, and chemistry, facilitating interdisciplinary research. Wang et al. [2023]\\n- Ontology mapping aligns diverse ontolog...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Lutz et al. applied an RL-based approach combined with Monte Carlo tree search for protein design and created complex protein nanomaterials with desired properties (Lutz et al., 2023).\\n- Barata et a...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- A retrieval framework manages a pre-defined, scalable list of external data sources (e.g., OpenStreetMap, US Census), exemplifying a curated external data approach for task-specific scientific agent...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- In Matchat, fine-tuning Llama2-7B with structured materials knowledge data improved model performance in materials science, demonstrating the efficacy of incorporating domain-specific structured inf...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Physics-simulator-driven LLM workflows achieved validated results in constitutive law discovery and molecular design tasks. (Ma et al., 2024a)\\n- Airfoil optimizations performed with MyCrunchGPT were...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Chemist-X automates reaction condition recommendations for chemical synthesis using RAG techniques and CAD tools and surpasses traditional synthesis AIs in performance (Chen et al.).\\n- Coscientist a...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- The LPCOMDA framework uses an LLM-based planner to automate modulation design in power electronics and reports improved efficiency. (Liu et al., 2024b)\\n- LLMPhy, which combines LLMs with physics eng...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\"],\"x\":[6.6010456,8.11839,6.872961,7.579826,7.126723,7.067069,7.713854,7.348933,7.6004224,8.177533,8.829388,7.1063433,6.664435,7.7304974,5.740021,5.8122816,7.6165323,6.865538,7.6662025,6.950822,7.050494,5.593669,5.6841297,5.8470674,7.4223332,7.378244,8.0372305,10.085143,8.511514,8.299729,7.7342596,7.459406],\"y\":[7.007849,8.375283,8.580124,8.902064,8.4830675,8.314111,8.467489,8.616042,8.361322,9.353125,9.812449,6.830321,6.715338,7.265117,8.000699,7.954917,6.611565,6.4468307,6.4064393,6.52482,7.0110793,8.165694,8.05448,7.7097497,7.157467,6.8582335,8.639611,8.055374,8.341609,8.1247225,8.143647,8.540612],\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(190,186,218)\",\"line\":{\"color\":\"white\",\"width\":1},\"size\":12},\"mode\":\"markers\",\"name\":\"Evaluating LLM scientific agents (n=18)\",\"text\":[\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eIn an isolated physical system, the total entropy can never decrease, consistent with the Second Law of Thermodynamics....\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eIn the AstroAgents system, over 30% of the generated hypotheses were validated as scientifically plausible by expert reviewers....\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- POPPER‚Äôs automated validation framework achieves human-level accuracy at significantly greater speed by applying rigorous statistical checks. [Huang et al., 2025a]\\n- The Logit-based Calibrated Prior...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eThe Virtual Lab multi-agent system generated 92 nanobody candidates and experimentally validated two that showed improved binding to SARS-CoV-2 variants....\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- DrugAgent achieved approximately 0.92 F1 on PAMPA absorption prediction in case studies.\\n- CLADD improved task performance compared to general LLMs and classical deep-learning baselines on drug disc...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Supervised fine-tuning, as a form of imitation learning, limits LLMs‚Äô ability to generalize to new domains. \\n- Reinforcement learning from verifiable rewards elicits generalizable reasoning abilitie...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2410.07076...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eExperiments on a 2024 chemistry and materials science benchmark using an LLM-based multi-agent framework (with models trained up to October 2023 in a copilot in-the-wild setting) showed the system cou...\\u003cbr\\u003e\\u003cbr\\u003eCitations: False\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2410.07076...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eIn the actual experiment, dual-wavelength catalysis and solvent mixing were not employed; acetone and acetonitrile were the two best-performing single solvents in the actual research....\\u003cbr\\u003e\\u003cbr\\u003eCitations: False\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2307.10635...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- In SCIBENCH benchmarking across representative open-source and proprietary LLMs with various prompting strategies, the best overall performance was 43.22%, indicating current LLMs fall short on coll...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2307.10635...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eIn a human evaluation of the model verifier‚Äôs error classifications, two annotators reviewed 151 samples across different settings and judged 123 to be correctly classified, yielding an accuracy of 81...\\u003cbr\\u003e\\u003cbr\\u003eCitations: False\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eVirSci, a large-scale LLM-based multi-agent system, achieved significant gains in generating original research ideas compared to single-agent and prior multi-agent baselines....\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eTools such as BACON and KEKADA have empirically rediscovered known scientific laws and relationships when applied to structured datasets....\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Simulating a computational biological model, rather than reading, elicited measurable changes in brain activity during biological reasoning.\\n- Target-assisted iterative screening (TAIS) of the BIR3-...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003ePrior LLM-based multi-modal agents commonly include a separate perceptron module to handle multi-modal inputs....\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eEarly scientific agent systems such as Coscientist and ChemCrow used a single LLM-based planner to orchestrate all operations, whereas recent multi-agent planners like Google‚Äôs AI co-scientist distrib...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Integrating SymPy, SciPy, and CVXPY into natural-language reasoning frameworks (Tora) produced significant performance improvements for open-source LLMs across multiple mathematical reasoning benchm...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eNo single agent system has yet achieved all the features listed in Table 4 comparing general-purpose and scientific agents, although systems are trending toward these features due to the scientific fi...\\u003cbr\\u003e\\u003cbr\\u003eCitations: False\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eOn the Humanity‚Äôs Last Exam (HLE) benchmark, multiple frontier large language models achieve accuracy scores below 10%, indicating a substantial performance gap on difficult, closed-ended academic tas...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\"],\"x\":[11.214287,10.088959,8.855928,9.316676,8.94537,9.249339,9.467289,11.151216,9.140408,9.432174,9.380431,10.200976,9.314828,9.535712,10.006935,9.650101,11.172652,9.342317],\"y\":[7.5078897,8.912177,8.475814,6.512803,8.681102,8.265698,9.096835,8.313127,9.814331,9.350528,8.607131,6.7841573,6.7555733,8.499323,8.204312,7.352738,7.8580694,9.553295],\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(251,128,114)\",\"line\":{\"color\":\"white\",\"width\":1},\"size\":12},\"mode\":\"markers\",\"name\":\"LLM-augmented scientific hypothesis discovery (n=35)\",\"text\":[\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eA prior review systematically surveyed more than 260 scientific large language models spanning general science, mathematics, physics, chemistry, materials science, biology, medicine, and geoscience....\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- A framework combining LLMs with causal knowledge graphs analyzed over 43,000 psychology articles and generated well-being hypotheses whose novelty matched expert-level assessments, significantly out...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- The Self-Refine framework, which enables LLMs to critique and iteratively refine their outputs, has shown significant improvements on reasoning tasks. [Madaan et al., 2023]\\n- Iterative introspection...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- ResearchAgent‚Äôs human-aligned reviewing agents simulate structured peer review and significantly enhance the feasibility and novelty of scientific research proposals.\\n- MAPPS uses human scientists t...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Integrating code interpreters enhances agents‚Äô mathematical abilities, and RL further optimizes multi-turn reasoning via real-time code execution.\\n- With search engine tools, RL helps agents generat...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eHistorical evidence indicates that major scientific breakthroughs‚Äîsuch as penicillin, the cosmic microwave background radiation, and graphene‚Äîoriginated from chance or unexpected observations, undersc...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2410.07076...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Yang et al. (2024b) found via expert evaluation that LLMs can generate hypotheses in social science that are both novel and sufficiently valid.\\n- Si et al. (2024) showed in the NLP domain that LLMs ...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2410.07076...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- From extensive discussions with chemistry experts, the authors observed that the social-science assumption that a hypothesis can be divided into background and inspiration applies to a majority of c...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2410.07076...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Experiments using the constructed benchmark found that LLMs are highly capable of addressing the three fundamental questions posed for hypothesis rediscovery.\\n- When tested with only a background an...\\u003cbr\\u003e\\u003cbr\\u003eCitations: False\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2307.10635...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- On SCIBENCH, even with the strongest configuration (combining chain-of-thought prompting and external tools), the best model achieves average scores of 43.22% on the textual dataset, 13.8% on the mu...\\u003cbr\\u003e\\u003cbr\\u003eCitations: False\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Early symbolic and heuristic AI systems were able to simulate elements of scientific reasoning (e.g., rediscovering physical laws and inferring causal structures), but their scalability and adaptabi...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eAlphaFold has revolutionized protein structure prediction, resolving key bottlenecks in drug discovery and expediting therapeutic innovation (Jumper et al. [2021]). Crispr-GPT streamlines the design o...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eLarge language models have been observed to generate outputs that appear fluent and scientifically plausible yet are factually incorrect or unsupported by evidence (AI hallucination)....\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Prior studies report that LLMs can automate bioinformatics workflows when paired with structured repositories such as cBioPortal. \\n- The AHTech platform integrates automated electrochemical experime...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- The MOLIERE system, leveraging PubMed-scale repositories, identified novel gene‚Äìdisease associations that conventional analyses often failed to detect.\\n- The SciAgents framework demonstrated that in...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Knowledge graphs uncovered correlations between oceanic and atmospheric variables that enabled modeling of ecological phenomena.\\n- In biomedicine, knowledge graphs and ontology-based reasoning mappe...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Analyses using causal inference frameworks have identified unexpected treatment effects in biomedical datasets, challenging existing paradigms and opening new research avenues (Jha et al. [2019]).\\n-...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Early symbolic and heuristic discovery systems could simulate elements of scientific reasoning (e.g., rediscovering physical laws and inferring causal structures), but their scalability and adaptabi...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- AlphaFold has revolutionized protein structure prediction, resolving key bottlenecks in drug discovery and expediting therapeutic innovation. [Jumper et al. [2021]]\\n- Crispr-GPT streamlines the desi...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eLarge language models have been observed to generate hallucinated outputs‚Äîfluent and scientifically plausible text that is factually incorrect or unsupported‚Äîand, without transparent reasoning or sour...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Contrastive learning techniques enable LLMs to refine hypotheses by balancing novelty and plausibility, reducing the generation of implausible or irrelevant ideas.\\n- Human-in-the-loop validation all...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eMOLIERE, leveraging knowledge graphs built from PubMed, identified novel gene‚Äìdisease associations that often elude conventional analysis....\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Prompting strategies and architectural mechanisms can enable LLMs to emulate knowledge graph behavior by performing multi-hop reasoning and generating structured triples directly from natural langua...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Integrating genomic, transcriptomic, and phenotypic data in agriculture has uncovered genetic pathways that enhance crop resistance to environmental stressors, supporting sustainable farming practic...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- DrugRL has accelerated the design of molecular structures in drug discovery, reducing costs and timelines by iteratively refining candidate molecules.\\n- RL-Discovery has optimized catalysts and iden...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- SimHypoth used neural-network models to uncover non-linear relationships and correlations between demographic factors and societal trends in social-science contexts. \\n- IdeaFlow‚Äôs simulations led to...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Causal inference frameworks have identified unexpected treatment effects in biomedical datasets, challenging existing paradigms and opening new research avenues. [Jha et al. 2019]\\n- Incomplete datas...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- VirSci, a large-scale LLM-based multi-agent system that simulates real-world scientific collaboration, achieved significant gains in generating original research ideas compared to single-agent and p...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eAutomated microscopy systems in experimental biology have been observed to enable real-time cellular imaging and validation while reducing manual errors and increasing experimental throughput....\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Fine-tuning a planner on an instruction-based dataset in drug discovery enables interactive molecule optimization by internalizing expert feedback, effectively yielding a functional drug discovery a...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Prompt-based planners in LLM-based scientific agents are highly sensitive to prompt quality, leading to inconsistent scientific outputs.\\n- SFT-based planners require extensive, high-quality domain-s...\\u003cbr\\u003e\\u003cbr\\u003eCitations: False\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eEmpirical results from the AI co-scientist framework show that a modular approach accelerates discovery by reducing hypothesis-generation timelines from weeks to days and improves the novelty and accu...\\u003cbr\\u003e\\u003cbr\\u003eCitations: False\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- BioDiscoveryAgent autonomously designs genetic perturbation experiments and empirically improves prediction accuracy and efficiency, outperforming traditional methods in identifying genes linked to ...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- LLMs can be manipulated to generate fabricated scientific arguments that falsely claim biases are beneficial, risking the misleading of researchers.\\n- Adversaries can use LLMs to poison biomedical k...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- The AI co-scientist has been demonstrated to propose promising drug repurposing candidates for acute myeloid leukemia.\\n- The system identified epigenetic regulators as novel treatment targets for li...\\u003cbr\\u003e\\u003cbr\\u003eCitations: False\"],\"x\":[10.032083,9.51245,8.145588,10.449802,9.752714,11.036943,9.725646,10.667425,9.85053,9.323387,10.353055,8.782,10.661829,9.956267,9.648474,8.817107,8.100287,10.234971,8.605924,10.940766,10.442855,9.800564,8.375861,8.270781,8.712662,8.465253,8.275565,8.306415,10.107497,8.85436,9.563039,10.45786,8.967702,10.645726,9.393424],\"y\":[7.3896184,7.7645187,8.009746,7.7090373,8.200504,7.352508,9.132802,7.38254,8.881085,10.174747,7.618618,7.1869125,8.166899,7.457149,6.911704,6.5008883,7.002924,7.971515,7.454671,7.978919,8.266492,6.7437778,7.14855,6.5370917,6.59285,6.3014207,7.2531667,7.6251664,7.056709,8.236906,7.934054,6.895413,7.6098056,7.8734126,6.724142],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"annotations\":[{\"bgcolor\":\"rgba(255,255,255,0.8)\",\"bordercolor\":\"rgb(141,211,199)\",\"borderwidth\":2,\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"LLM scientific reasoning benchmarks\",\"x\":7.741522,\"y\":9.258434},{\"bgcolor\":\"rgba(255,255,255,0.8)\",\"bordercolor\":\"rgb(255,255,179)\",\"borderwidth\":2,\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"LLM-powered autonomous scientific workflows\",\"x\":7.3216257,\"y\":7.869727},{\"bgcolor\":\"rgba(255,255,255,0.8)\",\"bordercolor\":\"rgb(190,186,218)\",\"borderwidth\":2,\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"Evaluating LLM scientific agents\",\"x\":9.748089,\"y\":8.252495},{\"bgcolor\":\"rgba(255,255,255,0.8)\",\"bordercolor\":\"rgb(251,128,114)\",\"borderwidth\":2,\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"LLM-augmented scientific hypothesis discovery\",\"x\":9.521052,\"y\":7.5727506}],\"font\":{\"size\":12},\"title\":{\"text\":\"Empirical Observations by Cluster Type\"},\"xaxis\":{\"title\":{\"text\":\"UMAP 1\"}},\"yaxis\":{\"title\":{\"text\":\"UMAP 2\"}},\"width\":1200,\"height\":800,\"hovermode\":\"closest\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('6e75cac1-c0dc-4e28-96b2-126b8b873729');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"a6942614-68f4-4eed-8fc9-f264c008c092\" class=\"plotly-graph-div\" style=\"height:800px; width:1200px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a6942614-68f4-4eed-8fc9-f264c008c092\")) {                    Plotly.newPlot(                        \"a6942614-68f4-4eed-8fc9-f264c008c092\",                        [{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(141,211,199)\",\"line\":{\"color\":\"white\",\"width\":1},\"size\":12},\"mode\":\"markers\",\"name\":\"LLM scientific reasoning benchmarks (n=9)\",\"text\":[\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2410.07076...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- The TOMATO-Chem benchmark contains 51 chemistry\\u002fmaterials papers: Polymer Chemistry 21, Organic Chemistry 22, Inorganic Chemistry 3, Analytical Chemistry 5; publication venues include Nature\\u002fScience...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2410.07076...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Under Claude-3.5-Sonnet evaluation, MOOSE-Chem achieved the highest Top Matched Score (4.471) and a higher Average Matched Score (3.697) than SciMON (TopMS 3.824, AvgMS 3.559), MOOSE (TopMS 3.529, A...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2307.10635...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Existing benchmarks ScienceQA and GSM8K predominantly contain problems grounded in grade-level subjects (Lu et al., 2022; Cobbe et al., 2021).\\n- The MATH benchmark introduces high-school level quest...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2307.10635...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Existing LLM benchmarks for lower educational levels predominantly focus on basic arithmetic operations rather than advanced mathematical computations.\\n- Most existing benchmarks are textual-only an...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2307.10635...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- On the SCIBENCH textbook dataset in the zero-shot setting, GPT-4-Turbo achieved an average accuracy of 40.99%, outperforming GPT-4 at 33.79% and Mistral-7B at 6.23%; GPT-4-Turbo outperformed Mistral...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2307.10635...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- In zero-shot settings, Chain-of-Thought (CoT) prompting reduces calculation-error rates to 13.6% compared to 29.0% with the vanilla zero-shot baseline.\\n- Zero-shot CoT increases errors in other skil...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2307.10635...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eOn the SCIBENCH exam dataset under zero-shot settings (with and without chain-of-thought), GPT-4 achieved the highest scores in 6 of 7 exams‚Äî58\\u002f90, 44\\u002f75, 40\\u002f56, 50\\u002f100, 80\\u002f100, and 25\\u002f95‚Äîwhile Claude...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2307.10635...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- In Figure S9, for the de Broglie wavelength problem, the ChatGPT solution using chain-of-thought was classified as error category 4 (Causal Reasoning) and produced 8.09 pm, whereas the non-chain-of-...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Geometry3K contains 3,002 K-12 mathematics items (Lu et al., 2021).\\n- GeoEval contains 5,050 K-12 mathematics items (Zhang et al., 2024b).\\n- VisScience contains 3,000 K-12 items spanning physics, ch...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\"],\"x\":[8.050034,8.254594,6.3672776,6.0833488,8.376924,8.7389345,9.131671,8.549357,6.1215606],\"y\":[10.26234,10.32924,7.0741143,7.2769094,10.455448,10.300976,9.987775,10.368286,7.2708235],\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(255,255,179)\",\"line\":{\"color\":\"white\",\"width\":1},\"size\":12},\"mode\":\"markers\",\"name\":\"LLM-powered autonomous scientific workflows (n=32)\",\"text\":[\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Early evidence indicates that large language models can propose scientific hypotheses in biomedicine in a zero-shot manner using only pre-trained knowledge [Qi et al., 2023].\\n- Domain-specific found...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- The evolutionary LLM-based code mutation process has produced entirely new algorithms that are more efficient than human-designed counterparts. [Nagda et al., 2025]\\n- ShinkaEvolve improves sample ef...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- SpatialAgent autonomously processed over 2 million cells across spatial transcriptomics and MERFISH pipelines and achieved parity or gains versus automated baselines while handling full projects wit...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- SAMPLE experimentally identified GH1 hydrolase variants with at least 12 ¬∞C higher thermal stability than the starting sequences using autonomous design‚Äìbuild‚Äìtest cycles in a robotic lab. [Rapp et ...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- LIDDiA generated molecules meeting pharmaceutical criteria across many targets and surfaced promising EGFR candidates. [Averly et al., 2025]\\n- AgentMD automatically selected and executed from 2,164 ...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Coscientist autonomously executed palladium-catalyzed cross-coupling reactions in under 4 minutes. [Boiko et al., 2023]\\n- ChemCrow integrated 18 expert-designed tools and successfully performed auto...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- MDAgent automated the MD workflow and reduced the total task time for thermodynamic calculations by over 40%. [Shi et al., 2025]\\n- The Materials Laws Multi-Agent Framework used LLM agents for symbol...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- The k-agents framework enabled a fully autonomous laboratory where LLM agents planned and executed experiments on superconducting quantum processors, producing entangled states with performance equi...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- LP-COMDA automated the design of complex modulation strategies for power converters with a physics-informed planner, accelerating design time by over 30x and reducing errors by over 60% [Liu et al.,...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2410.07076...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Across 392 comparison pairs, the agreement between expert evaluation and GPT-4o on the Matched Score yielded a hard consistency of 0.345 and a soft consistency of 0.542.\\n- Across 48 comparison pairs...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2307.10635...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- In the Physical Chemistry example problem requiring use of the Planck distribution to compare energy outputs at 450 nm and 700 nm at 298 K, GPT-4 with Chain-of-Thought prompting generated the correc...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- LLM-powered agentic AI systems automate routine scientific tasks such as data analysis, hypothesis formulation, and literature synthesis, enabling researchers to focus on more complex work (Paul et ...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- LLMs trained on historically grounded corpora exhibit regression to the mean in idea generation, prioritizing statistically likely continuations, producing variants of well-known ideas, and rarely p...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Tools such as MOLIERE for biomedical hypothesis validation and SciAgents for structured scientific discovery have demonstrated the efficacy of constrained knowledge generation (Sybrandt et al. [2018...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- PubMed Abstracts contains over 34 million abstracts. [2025]\\n- MeSH comprises 27,883 descriptors for categorizing biomedical content. [2025]\\n- ChEMBL includes over 2 million bioactive compounds. [202...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- DrugBank contains over 14,000 drugs and 6,000 protein targets.\\n- AI2 Science Questions contains over 10,000 multiple-choice science questions.\\n- Materials Project includes over 133,500 materials.\\n- ...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Integrating ocean salinity data with atmospheric pressure measurements has yielded previously unattainable insights into climate dynamics (Wang et al. [2023]).\\n- VirtualPlant integrates genomic, tra...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Dynamic retrieval in RAG enables extraction of up-to-date information from diverse sources, enriching hypotheses with the latest knowledge Beltagy et al. [2019].\\n- Carefully engineered prompts guide...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- SimHypoth employed neural network models to uncover non-linear relationships and generated hypotheses that included correlations between demographic factors and societal trends (Lin and Lucas [2023]...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Simulation tools provide scalable virtual environments that can effectively pre-test scientific hypotheses. Qi et al. [2024], Schumann et al. [2024].\\n- Simulation-based pre-testing methods have prov...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Open-domain hypothesis generation is more challenging to validate and assess for relevance, despite enabling novel interdisciplinary insights (observed in prior work) [Sybrandt et al. [2018], Wang e...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- The PubMed Abstracts database contains over 34 million abstracts.\\n- MeSH comprises 27,883 descriptors.\\n- ChEMBL contains over 2 million bioactive compounds.\\n- The GENIA Corpus includes over 2,000 ab...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- DrugBank contains over 14,000 drugs and 6,000 protein targets for drug-target interaction studies.\\n- The AI2 Science Questions dataset contains over 10,000 multiple-choice science questions.\\n- The M...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- The AHTech Electrolyte Additive dataset contains high-throughput screening measurements for 180 small-molecule electrolyte additives tested in aqueous zinc metal batteries, with each additive charac...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- LLMs often lack formal mechanisms for explanation, causality, and logical rigor, so their outputs require downstream validation via simulation, symbolic reasoning, or expert review [Wang et al. [202...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Integration of multiple ontologies has unified knowledge across physics, biology, and chemistry, facilitating interdisciplinary research. Wang et al. [2023]\\n- Ontology mapping aligns diverse ontolog...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Lutz et al. applied an RL-based approach combined with Monte Carlo tree search for protein design and created complex protein nanomaterials with desired properties (Lutz et al., 2023).\\n- Barata et a...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- A retrieval framework manages a pre-defined, scalable list of external data sources (e.g., OpenStreetMap, US Census), exemplifying a curated external data approach for task-specific scientific agent...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- In Matchat, fine-tuning Llama2-7B with structured materials knowledge data improved model performance in materials science, demonstrating the efficacy of incorporating domain-specific structured inf...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Physics-simulator-driven LLM workflows achieved validated results in constitutive law discovery and molecular design tasks. (Ma et al., 2024a)\\n- Airfoil optimizations performed with MyCrunchGPT were...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Chemist-X automates reaction condition recommendations for chemical synthesis using RAG techniques and CAD tools and surpasses traditional synthesis AIs in performance (Chen et al.).\\n- Coscientist a...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- The LPCOMDA framework uses an LLM-based planner to automate modulation design in power electronics and reports improved efficiency. (Liu et al., 2024b)\\n- LLMPhy, which combines LLMs with physics eng...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\"],\"x\":[6.6010456,8.11839,6.872961,7.579826,7.126723,7.067069,7.713854,7.348933,7.6004224,8.177533,8.829388,7.1063433,6.664435,7.7304974,5.740021,5.8122816,7.6165323,6.865538,7.6662025,6.950822,7.050494,5.593669,5.6841297,5.8470674,7.4223332,7.378244,8.0372305,10.085143,8.511514,8.299729,7.7342596,7.459406],\"y\":[7.007849,8.375283,8.580124,8.902064,8.4830675,8.314111,8.467489,8.616042,8.361322,9.353125,9.812449,6.830321,6.715338,7.265117,8.000699,7.954917,6.611565,6.4468307,6.4064393,6.52482,7.0110793,8.165694,8.05448,7.7097497,7.157467,6.8582335,8.639611,8.055374,8.341609,8.1247225,8.143647,8.540612],\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(190,186,218)\",\"line\":{\"color\":\"white\",\"width\":1},\"size\":12},\"mode\":\"markers\",\"name\":\"Evaluating LLM scientific agents (n=18)\",\"text\":[\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eIn an isolated physical system, the total entropy can never decrease, consistent with the Second Law of Thermodynamics....\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eIn the AstroAgents system, over 30% of the generated hypotheses were validated as scientifically plausible by expert reviewers....\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- POPPER‚Äôs automated validation framework achieves human-level accuracy at significantly greater speed by applying rigorous statistical checks. [Huang et al., 2025a]\\n- The Logit-based Calibrated Prior...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eThe Virtual Lab multi-agent system generated 92 nanobody candidates and experimentally validated two that showed improved binding to SARS-CoV-2 variants....\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- DrugAgent achieved approximately 0.92 F1 on PAMPA absorption prediction in case studies.\\n- CLADD improved task performance compared to general LLMs and classical deep-learning baselines on drug disc...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Supervised fine-tuning, as a form of imitation learning, limits LLMs‚Äô ability to generalize to new domains. \\n- Reinforcement learning from verifiable rewards elicits generalizable reasoning abilitie...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2410.07076...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eExperiments on a 2024 chemistry and materials science benchmark using an LLM-based multi-agent framework (with models trained up to October 2023 in a copilot in-the-wild setting) showed the system cou...\\u003cbr\\u003e\\u003cbr\\u003eCitations: False\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2410.07076...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eIn the actual experiment, dual-wavelength catalysis and solvent mixing were not employed; acetone and acetonitrile were the two best-performing single solvents in the actual research....\\u003cbr\\u003e\\u003cbr\\u003eCitations: False\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2307.10635...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- In SCIBENCH benchmarking across representative open-source and proprietary LLMs with various prompting strategies, the best overall performance was 43.22%, indicating current LLMs fall short on coll...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2307.10635...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eIn a human evaluation of the model verifier‚Äôs error classifications, two annotators reviewed 151 samples across different settings and judged 123 to be correctly classified, yielding an accuracy of 81...\\u003cbr\\u003e\\u003cbr\\u003eCitations: False\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eVirSci, a large-scale LLM-based multi-agent system, achieved significant gains in generating original research ideas compared to single-agent and prior multi-agent baselines....\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eTools such as BACON and KEKADA have empirically rediscovered known scientific laws and relationships when applied to structured datasets....\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Simulating a computational biological model, rather than reading, elicited measurable changes in brain activity during biological reasoning.\\n- Target-assisted iterative screening (TAIS) of the BIR3-...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003ePrior LLM-based multi-modal agents commonly include a separate perceptron module to handle multi-modal inputs....\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eEarly scientific agent systems such as Coscientist and ChemCrow used a single LLM-based planner to orchestrate all operations, whereas recent multi-agent planners like Google‚Äôs AI co-scientist distrib...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Integrating SymPy, SciPy, and CVXPY into natural-language reasoning frameworks (Tora) produced significant performance improvements for open-source LLMs across multiple mathematical reasoning benchm...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eNo single agent system has yet achieved all the features listed in Table 4 comparing general-purpose and scientific agents, although systems are trending toward these features due to the scientific fi...\\u003cbr\\u003e\\u003cbr\\u003eCitations: False\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eOn the Humanity‚Äôs Last Exam (HLE) benchmark, multiple frontier large language models achieve accuracy scores below 10%, indicating a substantial performance gap on difficult, closed-ended academic tas...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\"],\"x\":[11.214287,10.088959,8.855928,9.316676,8.94537,9.249339,9.467289,11.151216,9.140408,9.432174,9.380431,10.200976,9.314828,9.535712,10.006935,9.650101,11.172652,9.342317],\"y\":[7.5078897,8.912177,8.475814,6.512803,8.681102,8.265698,9.096835,8.313127,9.814331,9.350528,8.607131,6.7841573,6.7555733,8.499323,8.204312,7.352738,7.8580694,9.553295],\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(251,128,114)\",\"line\":{\"color\":\"white\",\"width\":1},\"size\":12},\"mode\":\"markers\",\"name\":\"LLM-augmented scientific hypothesis discovery (n=35)\",\"text\":[\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eA prior review systematically surveyed more than 260 scientific large language models spanning general science, mathematics, physics, chemistry, materials science, biology, medicine, and geoscience....\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- A framework combining LLMs with causal knowledge graphs analyzed over 43,000 psychology articles and generated well-being hypotheses whose novelty matched expert-level assessments, significantly out...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- The Self-Refine framework, which enables LLMs to critique and iteratively refine their outputs, has shown significant improvements on reasoning tasks. [Madaan et al., 2023]\\n- Iterative introspection...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- ResearchAgent‚Äôs human-aligned reviewing agents simulate structured peer review and significantly enhance the feasibility and novelty of scientific research proposals.\\n- MAPPS uses human scientists t...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Integrating code interpreters enhances agents‚Äô mathematical abilities, and RL further optimizes multi-turn reasoning via real-time code execution.\\n- With search engine tools, RL helps agents generat...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2510.09901...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eHistorical evidence indicates that major scientific breakthroughs‚Äîsuch as penicillin, the cosmic microwave background radiation, and graphene‚Äîoriginated from chance or unexpected observations, undersc...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2410.07076...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Yang et al. (2024b) found via expert evaluation that LLMs can generate hypotheses in social science that are both novel and sufficiently valid.\\n- Si et al. (2024) showed in the NLP domain that LLMs ...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2410.07076...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- From extensive discussions with chemistry experts, the authors observed that the social-science assumption that a hypothesis can be divided into background and inspiration applies to a majority of c...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2410.07076...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Experiments using the constructed benchmark found that LLMs are highly capable of addressing the three fundamental questions posed for hypothesis rediscovery.\\n- When tested with only a background an...\\u003cbr\\u003e\\u003cbr\\u003eCitations: False\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2307.10635...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- On SCIBENCH, even with the strongest configuration (combining chain-of-thought prompting and external tools), the best model achieves average scores of 43.22% on the textual dataset, 13.8% on the mu...\\u003cbr\\u003e\\u003cbr\\u003eCitations: False\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Early symbolic and heuristic AI systems were able to simulate elements of scientific reasoning (e.g., rediscovering physical laws and inferring causal structures), but their scalability and adaptabi...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eAlphaFold has revolutionized protein structure prediction, resolving key bottlenecks in drug discovery and expediting therapeutic innovation (Jumper et al. [2021]). Crispr-GPT streamlines the design o...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eLarge language models have been observed to generate outputs that appear fluent and scientifically plausible yet are factually incorrect or unsupported by evidence (AI hallucination)....\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Prior studies report that LLMs can automate bioinformatics workflows when paired with structured repositories such as cBioPortal. \\n- The AHTech platform integrates automated electrochemical experime...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- The MOLIERE system, leveraging PubMed-scale repositories, identified novel gene‚Äìdisease associations that conventional analyses often failed to detect.\\n- The SciAgents framework demonstrated that in...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Knowledge graphs uncovered correlations between oceanic and atmospheric variables that enabled modeling of ecological phenomena.\\n- In biomedicine, knowledge graphs and ontology-based reasoning mappe...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Analyses using causal inference frameworks have identified unexpected treatment effects in biomedical datasets, challenging existing paradigms and opening new research avenues (Jha et al. [2019]).\\n-...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Early symbolic and heuristic discovery systems could simulate elements of scientific reasoning (e.g., rediscovering physical laws and inferring causal structures), but their scalability and adaptabi...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- AlphaFold has revolutionized protein structure prediction, resolving key bottlenecks in drug discovery and expediting therapeutic innovation. [Jumper et al. [2021]]\\n- Crispr-GPT streamlines the desi...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eLarge language models have been observed to generate hallucinated outputs‚Äîfluent and scientifically plausible text that is factually incorrect or unsupported‚Äîand, without transparent reasoning or sour...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Contrastive learning techniques enable LLMs to refine hypotheses by balancing novelty and plausibility, reducing the generation of implausible or irrelevant ideas.\\n- Human-in-the-loop validation all...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eMOLIERE, leveraging knowledge graphs built from PubMed, identified novel gene‚Äìdisease associations that often elude conventional analysis....\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Prompting strategies and architectural mechanisms can enable LLMs to emulate knowledge graph behavior by performing multi-hop reasoning and generating structured triples directly from natural langua...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Integrating genomic, transcriptomic, and phenotypic data in agriculture has uncovered genetic pathways that enhance crop resistance to environmental stressors, supporting sustainable farming practic...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- DrugRL has accelerated the design of molecular structures in drug discovery, reducing costs and timelines by iteratively refining candidate molecules.\\n- RL-Discovery has optimized catalysts and iden...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- SimHypoth used neural-network models to uncover non-linear relationships and correlations between demographic factors and societal trends in social-science contexts. \\n- IdeaFlow‚Äôs simulations led to...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Causal inference frameworks have identified unexpected treatment effects in biomedical datasets, challenging existing paradigms and opening new research avenues. [Jha et al. 2019]\\n- Incomplete datas...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- VirSci, a large-scale LLM-based multi-agent system that simulates real-world scientific collaboration, achieved significant gains in generating original research ideas compared to single-agent and p...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2505.04651...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eAutomated microscopy systems in experimental biology have been observed to enable real-time cellular imaging and validation while reducing manual errors and increasing experimental throughput....\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Fine-tuning a planner on an instruction-based dataset in drug discovery enables interactive molecule optimization by internalizing expert feedback, effectively yielding a functional drug discovery a...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- Prompt-based planners in LLM-based scientific agents are highly sensitive to prompt quality, leading to inconsistent scientific outputs.\\n- SFT-based planners require extensive, high-quality domain-s...\\u003cbr\\u003e\\u003cbr\\u003eCitations: False\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eEmpirical results from the AI co-scientist framework show that a modular approach accelerates discovery by reducing hypothesis-generation timelines from weeks to days and improves the novelty and accu...\\u003cbr\\u003e\\u003cbr\\u003eCitations: False\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- BioDiscoveryAgent autonomously designs genetic perturbation experiments and empirically improves prediction accuracy and efficiency, outperforming traditional methods in identifying genes linked to ...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- LLMs can be manipulated to generate fabricated scientific arguments that falsely claim biases are beneficial, risking the misleading of researchers.\\n- Adversaries can use LLMs to poison biomedical k...\\u003cbr\\u003e\\u003cbr\\u003eCitations: True\",\"\\u003cb\\u003ehttps:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2503.24047...\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e- The AI co-scientist has been demonstrated to propose promising drug repurposing candidates for acute myeloid leukemia.\\n- The system identified epigenetic regulators as novel treatment targets for li...\\u003cbr\\u003e\\u003cbr\\u003eCitations: False\"],\"x\":[10.032083,9.51245,8.145588,10.449802,9.752714,11.036943,9.725646,10.667425,9.85053,9.323387,10.353055,8.782,10.661829,9.956267,9.648474,8.817107,8.100287,10.234971,8.605924,10.940766,10.442855,9.800564,8.375861,8.270781,8.712662,8.465253,8.275565,8.306415,10.107497,8.85436,9.563039,10.45786,8.967702,10.645726,9.393424],\"y\":[7.3896184,7.7645187,8.009746,7.7090373,8.200504,7.352508,9.132802,7.38254,8.881085,10.174747,7.618618,7.1869125,8.166899,7.457149,6.911704,6.5008883,7.002924,7.971515,7.454671,7.978919,8.266492,6.7437778,7.14855,6.5370917,6.59285,6.3014207,7.2531667,7.6251664,7.056709,8.236906,7.934054,6.895413,7.6098056,7.8734126,6.724142],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"annotations\":[{\"bgcolor\":\"rgba(255,255,255,0.8)\",\"bordercolor\":\"rgb(141,211,199)\",\"borderwidth\":2,\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"LLM scientific reasoning benchmarks\",\"x\":7.741522,\"y\":9.258434},{\"bgcolor\":\"rgba(255,255,255,0.8)\",\"bordercolor\":\"rgb(255,255,179)\",\"borderwidth\":2,\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"LLM-powered autonomous scientific workflows\",\"x\":7.3216257,\"y\":7.869727},{\"bgcolor\":\"rgba(255,255,255,0.8)\",\"bordercolor\":\"rgb(190,186,218)\",\"borderwidth\":2,\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"Evaluating LLM scientific agents\",\"x\":9.748089,\"y\":8.252495},{\"bgcolor\":\"rgba(255,255,255,0.8)\",\"bordercolor\":\"rgb(251,128,114)\",\"borderwidth\":2,\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"LLM-augmented scientific hypothesis discovery\",\"x\":9.521052,\"y\":7.5727506}],\"font\":{\"size\":12},\"title\":{\"text\":\"Empirical Observations by Cluster Type\"},\"xaxis\":{\"title\":{\"text\":\"UMAP 1\"}},\"yaxis\":{\"title\":{\"text\":\"UMAP 2\"}},\"width\":1200,\"height\":800,\"hovermode\":\"closest\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('a6942614-68f4-4eed-8fc9-f264c008c092');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Labelling Each Chunk\n",
        "for item in paper_list_for_download:\n",
        "  status = label_all_chunks_per_paper(item, \"observational_statements\", property_dictionary['observational_statements']['description'])\n",
        "  if status:\n",
        "    print(f\"‚úÖ Paper {item['paper_title']} labelled.\")\n",
        "\n",
        "\n",
        "# Checking Labelling Output\n",
        "for item in paper_list_for_download:\n",
        "  print(item.keys())\n",
        "  print(item['labeled_chunks_all'][0].keys())\n",
        "  print(item['labeled_chunks_all'][0]['label_output'])\n",
        "\n",
        "# Preparing Output for Embedding\n",
        "content_for_embedding = prepare_chunks_for_embedding(paper_list_for_download, property_label=\"observational_statements\", labeled_chunks_key='labeled_chunks_all_observational')\n",
        "print(len(content_for_embedding))\n",
        "print(type(content_for_embedding[0]))\n",
        "print(content_for_embedding[0].keys())\n",
        "\n",
        "# Checking Content for Embedding\n",
        "for item in content_for_embedding:\n",
        "  print(item['paper_url'])\n",
        "  print(item['extracted_content'])\n",
        "\n",
        "# Creating Embeddings\n",
        "embeddings = create_embeddings(content_for_embedding, model_name='allenai/scibert_scivocab_uncased', save_path='label_all_embeddings_observational')\n",
        "\n",
        "# First, let's check what we're working with\n",
        "print(\"Debug info:\")\n",
        "print(f\"Embeddings shape: {embeddings.shape}\")\n",
        "print(f\"Embedding dimensions: {embeddings.shape[1]}\")\n",
        "\n",
        "# Let's look at the similarity distribution\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Calculate similarities\n",
        "similarities = cosine_similarity(embeddings)\n",
        "# Get upper triangle (excluding diagonal)\n",
        "upper_tri = np.triu(similarities, k=1)\n",
        "flat_sims = upper_tri[upper_tri > 0]\n",
        "\n",
        "print(f\"\\nSimilarity stats:\")\n",
        "print(f\"Mean similarity: {flat_sims.mean():.3f}\")\n",
        "print(f\"Std similarity: {flat_sims.std():.3f}\")\n",
        "print(f\"Min similarity: {flat_sims.min():.3f}\")\n",
        "print(f\"Max similarity: {flat_sims.max():.3f}\")\n",
        "\n",
        "# Try simpler clustering - KMeans first to see if there ARE patterns\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Try different k values\n",
        "for k in [3, 4, 5, 8, 6]:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans_labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "    print(f\"\\nKMeans with k={k}:\")\n",
        "    for i in range(k):\n",
        "        count = list(kmeans_labels).count(i)\n",
        "        print(f\"  Cluster {i}: {count} observations\")\n",
        "\n",
        "for i in range(len(content_for_embedding)):\n",
        "    content_for_embedding[i]['cluster_id'] = k_means_clustering_labels[i]\n",
        "\n",
        "# Doing final clustering with k=4\n",
        "k=4\n",
        "k_means_clustering_labels, clusterer = cluster_extractions_kmeans(embeddings, k=k)\n",
        "\n",
        "\n",
        "# Generating cluster summary\n",
        "\n",
        "cluster_summaries = {}\n",
        "for cluster_id in range(k):\n",
        "      # Get all extractions for this cluster\n",
        "      cluster_extractions = [ext for ext in content_for_embedding if ext['cluster_id'] == cluster_id]\n",
        "\n",
        "      if len(cluster_extractions) > 0:\n",
        "          summary = summarize_cluster_extractions(\n",
        "              cluster_extractions,\n",
        "              property_name=\"observational_statements\",\n",
        "              property_description=property_dictionary['observational_statements']['description'],\n",
        "              cluster_id=cluster_id\n",
        "          )\n",
        "          cluster_summaries[cluster_id] = summary\n",
        "          print(f\"Cluster {cluster_id}: {summary} ({len(cluster_extractions)} extractions)\")\n",
        "\n",
        "      print(\"\\n‚úÖ Cluster summaries generated!\")\n",
        "      print(\"\\nFinal summaries:\", cluster_summaries)\n",
        "\n",
        "\n",
        "# Generating 2d visualisation data using UMAP\n",
        "\n",
        "visualisation_data = create_visualization_data(embeddings, k_means_clustering_labels, content_for_embedding)\n",
        "\n",
        "# Generating visualisation with summary\n",
        "\n",
        "visualize_clusters_with_summaries(visualisation_data['embeddings_2d'], visualisation_data['cluster_labels'], visualisation_data['extractions'], cluster_summaries=cluster_summaries, property_name=\"observational_statements\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQQa1GzIHSvj",
        "outputId": "9f01a0b0-1e48-4540-d19a-e21314fa7c66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labelling chunks for paper: paper_1, https://arxiv.org/pdf/2510.09901\n",
            "Total chunks: 36\n",
            "============================================================\n",
            "\n",
            "Starting labelling for: 36\n",
            "Labelling chunk 1/36\n",
            "AI request for chunk 1/36 completed.\n",
            "AI response for chunk 1/36 parsed.\n",
            "AI request for chunk 1/36 appended.\n",
            "Labelling chunk 2/36\n",
            "AI request for chunk 2/36 completed.\n",
            "AI response for chunk 2/36 parsed.\n",
            "AI request for chunk 2/36 appended.\n",
            "Labelling chunk 3/36\n",
            "AI request for chunk 3/36 completed.\n",
            "AI response for chunk 3/36 parsed.\n",
            "AI request for chunk 3/36 appended.\n",
            "Labelling chunk 4/36\n",
            "AI request for chunk 4/36 completed.\n",
            "AI response for chunk 4/36 parsed.\n",
            "AI request for chunk 4/36 appended.\n",
            "Labelling chunk 5/36\n",
            "AI request for chunk 5/36 completed.\n",
            "AI response for chunk 5/36 parsed.\n",
            "AI request for chunk 5/36 appended.\n",
            "Labelling chunk 6/36\n",
            "AI request for chunk 6/36 completed.\n",
            "AI response for chunk 6/36 parsed.\n",
            "AI request for chunk 6/36 appended.\n",
            "Labelling chunk 7/36\n",
            "AI request for chunk 7/36 completed.\n",
            "AI response for chunk 7/36 parsed.\n",
            "AI request for chunk 7/36 appended.\n",
            "Labelling chunk 8/36\n",
            "AI request for chunk 8/36 completed.\n",
            "AI response for chunk 8/36 parsed.\n",
            "AI request for chunk 8/36 appended.\n",
            "Labelling chunk 9/36\n",
            "AI request for chunk 9/36 completed.\n",
            "AI response for chunk 9/36 parsed.\n",
            "AI request for chunk 9/36 appended.\n",
            "Labelling chunk 10/36\n",
            "AI request for chunk 10/36 completed.\n",
            "AI response for chunk 10/36 parsed.\n",
            "AI request for chunk 10/36 appended.\n",
            "Labelling chunk 11/36\n",
            "AI request for chunk 11/36 completed.\n",
            "AI response for chunk 11/36 parsed.\n",
            "AI request for chunk 11/36 appended.\n",
            "Labelling chunk 12/36\n",
            "AI request for chunk 12/36 completed.\n",
            "AI response for chunk 12/36 parsed.\n",
            "AI request for chunk 12/36 appended.\n",
            "Labelling chunk 13/36\n",
            "AI request for chunk 13/36 completed.\n",
            "AI response for chunk 13/36 parsed.\n",
            "AI request for chunk 13/36 appended.\n",
            "Labelling chunk 14/36\n",
            "AI request for chunk 14/36 completed.\n",
            "AI response for chunk 14/36 parsed.\n",
            "AI request for chunk 14/36 appended.\n",
            "Labelling chunk 15/36\n",
            "AI request for chunk 15/36 completed.\n",
            "AI response for chunk 15/36 parsed.\n",
            "AI request for chunk 15/36 appended.\n",
            "Labelling chunk 16/36\n",
            "AI request for chunk 16/36 completed.\n",
            "AI response for chunk 16/36 parsed.\n",
            "AI request for chunk 16/36 appended.\n",
            "Labelling chunk 17/36\n",
            "AI request for chunk 17/36 completed.\n",
            "AI response for chunk 17/36 parsed.\n",
            "AI request for chunk 17/36 appended.\n",
            "Labelling chunk 18/36\n",
            "AI request for chunk 18/36 completed.\n",
            "AI response for chunk 18/36 parsed.\n",
            "AI request for chunk 18/36 appended.\n",
            "Labelling chunk 19/36\n",
            "AI request for chunk 19/36 completed.\n",
            "AI response for chunk 19/36 parsed.\n",
            "AI request for chunk 19/36 appended.\n",
            "Labelling chunk 20/36\n",
            "AI request for chunk 20/36 completed.\n",
            "AI response for chunk 20/36 parsed.\n",
            "AI request for chunk 20/36 appended.\n",
            "Labelling chunk 21/36\n",
            "AI request for chunk 21/36 completed.\n",
            "AI response for chunk 21/36 parsed.\n",
            "AI request for chunk 21/36 appended.\n",
            "Labelling chunk 22/36\n",
            "AI request for chunk 22/36 completed.\n",
            "AI response for chunk 22/36 parsed.\n",
            "AI request for chunk 22/36 appended.\n",
            "Labelling chunk 23/36\n",
            "AI request for chunk 23/36 completed.\n",
            "AI response for chunk 23/36 parsed.\n",
            "AI request for chunk 23/36 appended.\n",
            "Labelling chunk 24/36\n",
            "AI request for chunk 24/36 completed.\n",
            "AI response for chunk 24/36 parsed.\n",
            "AI request for chunk 24/36 appended.\n",
            "Labelling chunk 25/36\n",
            "AI request for chunk 25/36 completed.\n",
            "AI response for chunk 25/36 parsed.\n",
            "AI request for chunk 25/36 appended.\n",
            "Labelling chunk 26/36\n",
            "AI request for chunk 26/36 completed.\n",
            "AI response for chunk 26/36 parsed.\n",
            "AI request for chunk 26/36 appended.\n",
            "Labelling chunk 27/36\n",
            "AI request for chunk 27/36 completed.\n",
            "AI response for chunk 27/36 parsed.\n",
            "AI request for chunk 27/36 appended.\n",
            "Labelling chunk 28/36\n",
            "AI request for chunk 28/36 completed.\n",
            "AI response for chunk 28/36 parsed.\n",
            "AI request for chunk 28/36 appended.\n",
            "Labelling chunk 29/36\n",
            "AI request for chunk 29/36 completed.\n",
            "AI response for chunk 29/36 parsed.\n",
            "AI request for chunk 29/36 appended.\n",
            "Labelling chunk 30/36\n",
            "AI request for chunk 30/36 completed.\n",
            "AI response for chunk 30/36 parsed.\n",
            "AI request for chunk 30/36 appended.\n",
            "Labelling chunk 31/36\n",
            "AI request for chunk 31/36 completed.\n",
            "AI response for chunk 31/36 parsed.\n",
            "AI request for chunk 31/36 appended.\n",
            "Labelling chunk 32/36\n",
            "AI request for chunk 32/36 completed.\n",
            "AI response for chunk 32/36 parsed.\n",
            "AI request for chunk 32/36 appended.\n",
            "Labelling chunk 33/36\n",
            "AI request for chunk 33/36 completed.\n",
            "AI response for chunk 33/36 parsed.\n",
            "AI request for chunk 33/36 appended.\n",
            "Labelling chunk 34/36\n",
            "AI request for chunk 34/36 completed.\n",
            "AI response for chunk 34/36 parsed.\n",
            "AI request for chunk 34/36 appended.\n",
            "Labelling chunk 35/36\n",
            "AI request for chunk 35/36 completed.\n",
            "AI response for chunk 35/36 parsed.\n",
            "AI request for chunk 35/36 appended.\n",
            "Labelling chunk 36/36\n",
            "AI request for chunk 36/36 completed.\n",
            "AI response for chunk 36/36 parsed.\n",
            "AI request for chunk 36/36 appended.\n",
            "AI request for labelling None chunks complete. Appended.\n",
            "‚úÖ Paper paper_1 labelled.\n",
            "Labelling chunks for paper: paper_2, https://arxiv.org/pdf/2410.07076\n",
            "Total chunks: 13\n",
            "============================================================\n",
            "\n",
            "Starting labelling for: 13\n",
            "Labelling chunk 1/13\n",
            "AI request for chunk 1/13 completed.\n",
            "AI response for chunk 1/13 parsed.\n",
            "AI request for chunk 1/13 appended.\n",
            "Labelling chunk 2/13\n",
            "AI request for chunk 2/13 completed.\n",
            "AI response for chunk 2/13 parsed.\n",
            "AI request for chunk 2/13 appended.\n",
            "Labelling chunk 3/13\n",
            "AI request for chunk 3/13 completed.\n",
            "AI response for chunk 3/13 parsed.\n",
            "AI request for chunk 3/13 appended.\n",
            "Labelling chunk 4/13\n",
            "AI request for chunk 4/13 completed.\n",
            "AI response for chunk 4/13 parsed.\n",
            "AI request for chunk 4/13 appended.\n",
            "Labelling chunk 5/13\n",
            "AI request for chunk 5/13 completed.\n",
            "AI response for chunk 5/13 parsed.\n",
            "AI request for chunk 5/13 appended.\n",
            "Labelling chunk 6/13\n",
            "AI request for chunk 6/13 completed.\n",
            "AI response for chunk 6/13 parsed.\n",
            "AI request for chunk 6/13 appended.\n",
            "Labelling chunk 7/13\n",
            "AI request for chunk 7/13 completed.\n",
            "AI response for chunk 7/13 parsed.\n",
            "AI request for chunk 7/13 appended.\n",
            "Labelling chunk 8/13\n",
            "AI request for chunk 8/13 completed.\n",
            "AI response for chunk 8/13 parsed.\n",
            "AI request for chunk 8/13 appended.\n",
            "Labelling chunk 9/13\n",
            "AI request for chunk 9/13 completed.\n",
            "AI response for chunk 9/13 parsed.\n",
            "AI request for chunk 9/13 appended.\n",
            "Labelling chunk 10/13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Labelling Each Chunk\n",
        "for item in paper_list_for_download:\n",
        "  status = label_all_chunks_per_paper(item, \"theoretical_statements\", property_dictionary['theoretical_statements']['description'])\n",
        "  if status:\n",
        "    print(f\"‚úÖ Paper {item['paper_title']} labelled.\")\n",
        "\n",
        "\n",
        "# Checking Labelling Output\n",
        "for item in paper_list_for_download:\n",
        "  print(item.keys())\n",
        "  print(item['labeled_chunks_all'][0].keys())\n",
        "  print(item['labeled_chunks_all'][0]['label_output'])\n",
        "\n",
        "# Preparing Output for Embedding\n",
        "content_for_embedding = prepare_chunks_for_embedding(paper_list_for_download, property_label=\"theoretical_statements\", labeled_chunks_key='labeled_chunks_all_theoretical')\n",
        "print(len(content_for_embedding))\n",
        "print(type(content_for_embedding[0]))\n",
        "print(content_for_embedding[0].keys())\n",
        "\n",
        "# Checking Content for Embedding\n",
        "for item in content_for_embedding:\n",
        "  print(item['paper_url'])\n",
        "  print(item['extracted_content'])\n",
        "\n",
        "# Creating Embeddings\n",
        "embeddings = create_embeddings(content_for_embedding, model_name='allenai/scibert_scivocab_uncased', save_path='label_all_embeddings_theoretical')\n",
        "\n",
        "# First, let's check what we're working with\n",
        "print(\"Debug info:\")\n",
        "print(f\"Embeddings shape: {embeddings.shape}\")\n",
        "print(f\"Embedding dimensions: {embeddings.shape[1]}\")\n",
        "\n",
        "# Let's look at the similarity distribution\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Calculate similarities\n",
        "similarities = cosine_similarity(embeddings)\n",
        "# Get upper triangle (excluding diagonal)\n",
        "upper_tri = np.triu(similarities, k=1)\n",
        "flat_sims = upper_tri[upper_tri > 0]\n",
        "\n",
        "print(f\"\\nSimilarity stats:\")\n",
        "print(f\"Mean similarity: {flat_sims.mean():.3f}\")\n",
        "print(f\"Std similarity: {flat_sims.std():.3f}\")\n",
        "print(f\"Min similarity: {flat_sims.min():.3f}\")\n",
        "print(f\"Max similarity: {flat_sims.max():.3f}\")\n",
        "\n",
        "# Try simpler clustering - KMeans first to see if there ARE patterns\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Try different k values\n",
        "for k in [3, 4, 5, 8, 6]:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans_labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "    print(f\"\\nKMeans with k={k}:\")\n",
        "    for i in range(k):\n",
        "        count = list(kmeans_labels).count(i)\n",
        "        print(f\"  Cluster {i}: {count} observations\")\n",
        "\n",
        "\n",
        "# Doing final clustering with k=4\n",
        "k=4\n",
        "k_means_clustering_labels, clusterer = cluster_extractions_kmeans(embeddings, k=k)\n",
        "\n",
        "\n",
        "# Generating cluster summary\n",
        "\n",
        "cluster_summaries = {}\n",
        "for cluster_id in range(k):\n",
        "      # Get all extractions for this cluster\n",
        "      cluster_extractions = [ext for ext in content_for_embedding if ext['cluster_id'] == cluster_id]\n",
        "\n",
        "      if len(cluster_extractions) > 0:\n",
        "          summary = summarize_cluster_extractions(\n",
        "              cluster_extractions,\n",
        "              property_name=\"theoretical_statements\",\n",
        "              property_description=property_dictionary['theoretical_statements']['description'],\n",
        "              cluster_id=cluster_id\n",
        "          )\n",
        "          cluster_summaries[cluster_id] = summary\n",
        "          print(f\"Cluster {cluster_id}: {summary} ({len(cluster_extractions)} extractions)\")\n",
        "\n",
        "      print(\"\\n‚úÖ Cluster summaries generated!\")\n",
        "      print(\"\\nFinal summaries:\", cluster_summaries)\n",
        "\n",
        "\n",
        "# Generating 2d visualisation data using UMAP\n",
        "\n",
        "visualisation_data = create_visualization_data(embeddings, k_means_clustering_labels, content_for_embedding)\n",
        "\n",
        "# Generating visualisation with summary\n",
        "\n",
        "visualize_clusters_with_summaries(visualisation_data['embeddings_2d'], visualisation_data['cluster_labels'], visualisation_data['extractions'], cluster_summaries=cluster_summaries, property_name=\"theoretical_statements\")"
      ],
      "metadata": {
        "id": "L7Jr2xAMIb7L"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cec026e7ae804e49b9de3ec0b259a499": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3347e5b54efe4871915b0dca3d13133f",
              "IPY_MODEL_b033aafdddf44012af1aeef196bfc748",
              "IPY_MODEL_b5df3d46a0d54059b4ccf16c1027b45e"
            ],
            "layout": "IPY_MODEL_c053f97cfa574181830ce73d925a3281"
          }
        },
        "3347e5b54efe4871915b0dca3d13133f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b4673833f3b47c5b91f6ca7d7b1a105",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a1abfae7e2464509868a0904a2d0107c",
            "value": "Batches:‚Äá100%"
          }
        },
        "b033aafdddf44012af1aeef196bfc748": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b83632db8bc48c5b151873b13113da6",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ad6e469fbc34466498a611c4376490fd",
            "value": 1
          }
        },
        "b5df3d46a0d54059b4ccf16c1027b45e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b49045c4ca524b4da8aff291ec4db2a1",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_095a7c77e1a1438981177879650b5fee",
            "value": "‚Äá1/1‚Äá[00:24&lt;00:00,‚Äá24.71s/it]"
          }
        },
        "c053f97cfa574181830ce73d925a3281": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b4673833f3b47c5b91f6ca7d7b1a105": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1abfae7e2464509868a0904a2d0107c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b83632db8bc48c5b151873b13113da6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad6e469fbc34466498a611c4376490fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b49045c4ca524b4da8aff291ec4db2a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "095a7c77e1a1438981177879650b5fee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "393ab0f70c474a7abc15ef2faa2050db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c88c424e6764c269431877033dd7cf5",
              "IPY_MODEL_4660fa80bdb94166a842fd1518ca7161",
              "IPY_MODEL_aa550f5c82d04e8288b59e5eebc2fe5e"
            ],
            "layout": "IPY_MODEL_2e1fcebf6e144cd3b2d7d94c2a07e12d"
          }
        },
        "4c88c424e6764c269431877033dd7cf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62be1755e416413d93a4a3feb35749e1",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a06a0385e0714c8e92a0f4cf8e9688d9",
            "value": "Batches:‚Äá100%"
          }
        },
        "4660fa80bdb94166a842fd1518ca7161": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7df272ebcde942928d6c92deb626c528",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f8b446a74d474622b5ee98e3bbeb6a93",
            "value": 1
          }
        },
        "aa550f5c82d04e8288b59e5eebc2fe5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d05a01ca10f4802bdca6855e73e712c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_adc1401c38e34a649ad31faf90080b80",
            "value": "‚Äá1/1‚Äá[00:35&lt;00:00,‚Äá35.85s/it]"
          }
        },
        "2e1fcebf6e144cd3b2d7d94c2a07e12d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62be1755e416413d93a4a3feb35749e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a06a0385e0714c8e92a0f4cf8e9688d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7df272ebcde942928d6c92deb626c528": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8b446a74d474622b5ee98e3bbeb6a93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d05a01ca10f4802bdca6855e73e712c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adc1401c38e34a649ad31faf90080b80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93f85936f9a8475b9f6f11a9b2f12ac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e920c236d48a4bc39e566d0d2d6d67bf",
              "IPY_MODEL_9dca721589ad4d869ad971b8b38aa1fd",
              "IPY_MODEL_6124f717bee345dc8808e1083d2ee518"
            ],
            "layout": "IPY_MODEL_d49da52b0b3142cc9b0203c215b6710f"
          }
        },
        "e920c236d48a4bc39e566d0d2d6d67bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffbab51695c54d6e83229c6bf0968764",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_83781d3ae9804c7c9c5b0b5b65968fcb",
            "value": "Batches:‚Äá100%"
          }
        },
        "9dca721589ad4d869ad971b8b38aa1fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_709bfcff404c4323892ec4eeb77506bf",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ebd3879d5ba244c8911347c93bba0193",
            "value": 1
          }
        },
        "6124f717bee345dc8808e1083d2ee518": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10a9316d5d3b4ba7a5a161f624c0befa",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_09fd6104d3c740919460f5cb88f2fca5",
            "value": "‚Äá1/1‚Äá[00:27&lt;00:00,‚Äá27.59s/it]"
          }
        },
        "d49da52b0b3142cc9b0203c215b6710f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffbab51695c54d6e83229c6bf0968764": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83781d3ae9804c7c9c5b0b5b65968fcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "709bfcff404c4323892ec4eeb77506bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebd3879d5ba244c8911347c93bba0193": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "10a9316d5d3b4ba7a5a161f624c0befa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09fd6104d3c740919460f5cb88f2fca5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "490a5678dd4c41ca8b8d7b4b6fda1562": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_afa14ab48cfa4c4cb3e294b6981a87a0",
              "IPY_MODEL_54bb90079d1f42739b1c34da424d04f3",
              "IPY_MODEL_017d666c95544e3cbe1e14ba590c684f"
            ],
            "layout": "IPY_MODEL_13e251cc88ff4a38a28231926dbeaa4a"
          }
        },
        "afa14ab48cfa4c4cb3e294b6981a87a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53dd6eb8cc67421fb94538f193bd6bf7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_82f56a83a772493c88e7f3ee2d40db1e",
            "value": "Batches:‚Äá100%"
          }
        },
        "54bb90079d1f42739b1c34da424d04f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ce589b0184c4029a00b525c491d00f1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3808e7a798394a31a59291a4a5a8ba2a",
            "value": 1
          }
        },
        "017d666c95544e3cbe1e14ba590c684f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d09c68324354afebd38954ffa7e8bea",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4330ec8c6b4c4bee903805241d7466ef",
            "value": "‚Äá1/1‚Äá[00:28&lt;00:00,‚Äá28.67s/it]"
          }
        },
        "13e251cc88ff4a38a28231926dbeaa4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53dd6eb8cc67421fb94538f193bd6bf7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82f56a83a772493c88e7f3ee2d40db1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ce589b0184c4029a00b525c491d00f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3808e7a798394a31a59291a4a5a8ba2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d09c68324354afebd38954ffa7e8bea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4330ec8c6b4c4bee903805241d7466ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b991fc1889645a7b2e9eaf4e5cec163": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2212cf6e40764f03b34373ff346584bd",
              "IPY_MODEL_35a3be50cfa54842bf6374b0fc769236",
              "IPY_MODEL_766ee24808cd42eda8fa0695f57ff0c5"
            ],
            "layout": "IPY_MODEL_a6306d9e41e24be59ad9e4dffc094594"
          }
        },
        "2212cf6e40764f03b34373ff346584bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b8aa50e003446ab9d7a325990074055",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4d5221209103404f89549a3e3906e088",
            "value": "Batches:‚Äá100%"
          }
        },
        "35a3be50cfa54842bf6374b0fc769236": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c2e70b24ad74f6e9aa6e747fd5bbe96",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90874638ee3e486389ee68d53d57346f",
            "value": 1
          }
        },
        "766ee24808cd42eda8fa0695f57ff0c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90ae052fb21441279b15bdf9db2d4e29",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_424eecb711b249ab904d78f8652cdeec",
            "value": "‚Äá1/1‚Äá[00:12&lt;00:00,‚Äá12.71s/it]"
          }
        },
        "a6306d9e41e24be59ad9e4dffc094594": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b8aa50e003446ab9d7a325990074055": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d5221209103404f89549a3e3906e088": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c2e70b24ad74f6e9aa6e747fd5bbe96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90874638ee3e486389ee68d53d57346f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "90ae052fb21441279b15bdf9db2d4e29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "424eecb711b249ab904d78f8652cdeec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1c9846b2b2d45aab66653674c0421ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3679742eea1442428647ac650f2e565d",
              "IPY_MODEL_2c3ad49515794dd5b8a514a35fd0ad9b",
              "IPY_MODEL_37befc77db40419ab5f73b4f40d20737"
            ],
            "layout": "IPY_MODEL_46773632372b47b990d327f6a96abf69"
          }
        },
        "3679742eea1442428647ac650f2e565d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f07f7d0de1e46e0aeb59b925b7ee9f3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4d9803d64aa94efe8a80e48ea53245e8",
            "value": "Batches:‚Äá100%"
          }
        },
        "2c3ad49515794dd5b8a514a35fd0ad9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d16e7be449e7422682d96d1ee628517e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_93388392a8c8474782610305f46b1482",
            "value": 1
          }
        },
        "37befc77db40419ab5f73b4f40d20737": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68614738649b4da9bf5915c21a7016f5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_09a2a5887b8444acab90dfba4b25d67c",
            "value": "‚Äá1/1‚Äá[00:11&lt;00:00,‚Äá11.70s/it]"
          }
        },
        "46773632372b47b990d327f6a96abf69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f07f7d0de1e46e0aeb59b925b7ee9f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d9803d64aa94efe8a80e48ea53245e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d16e7be449e7422682d96d1ee628517e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93388392a8c8474782610305f46b1482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "68614738649b4da9bf5915c21a7016f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09a2a5887b8444acab90dfba4b25d67c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}